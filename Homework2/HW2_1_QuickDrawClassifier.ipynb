{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinUnnikrishnan/DeepLearning7150/blob/main/Homework2/HW2_1_QuickDrawClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ff28b42"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework-2/blob/main/HW2.1-QuickDrawClassifier.ipynb)\n",
        "\n",
        "\n",
        "# Assignment 2.1: Neural Network QuickDraw Classification"
      ],
      "id": "0ff28b42"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ef1d8e6"
      },
      "source": [
        "In this CS7150 assignment, we will develop a neural network with three fully-connected layers to perform classification, and test it out on a subset of the QuickDraw dataset. This notebook acts as a tutorial to get you started on writing Pytorch code to create Deep Learning models. \n",
        "\n",
        "**Your task**: Go through the entire notebook and fill out all the conceptual and technical questions that are indicated within the \"Exercise\" header. "
      ],
      "id": "0ef1d8e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c687004"
      },
      "source": [
        "# Setup Code\n",
        "\n",
        "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the autoreload extension. This allows us to edit .py source files (if there are any), and re-import them into the notebook for a seamless editing and debugging experience."
      ],
      "id": "2c687004"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d2cb283"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "5d2cb283"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "075f340c"
      },
      "source": [
        "## Device Setup"
      ],
      "id": "075f340c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33798935"
      },
      "source": [
        "We want to be able to train our model on a GPU to accelerate our computation. Let’s check to see if torch.cuda is available, else we continue to use the CPU."
      ],
      "id": "33798935"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a41951",
        "outputId": "425ace02-dc2a-4c43-86d0-e04eeeb9ca85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "id": "c9a41951"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f44c253a"
      },
      "source": [
        "## Google Colab Setup"
      ],
      "id": "f44c253a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d886b1"
      },
      "source": [
        "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "\n",
        "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
      ],
      "id": "24d886b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2338ff3",
        "outputId": "b71effa2-db88-4e6d-9229-944aa9b34f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "in_colab = False\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    in_colab = True\n",
        "except:\n",
        "    pass"
      ],
      "id": "f2338ff3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3c6c81"
      },
      "source": [
        "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
        "\n",
        "```\n",
        "['HW2.1-QuickDrawClassifier.ipynb', 'HW2.2-CIFAR10Classifier.ipynb']\n",
        "```"
      ],
      "id": "0e3c6c81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f1e917c",
        "outputId": "c068a3f7-df42-4306-e7a2-411ebc9a800f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Copy of Skydio Turtle Challenge - [ your name here ]', 'HW1-PytorchDemo.ipynb', 'Copy of Demonstration Code.ipynb', 'GAN_Working_MNIST_TensorFlow.ipynb', 'Copy of HW1_Pytorch.ipynb', 'Copy of ZigZagScenario.ipynb', 'Copy of HW2.2_DenoisingAutoencoder.ipynb', 'HW2.0_Backpropagation.ipynb', 'Untitled0.ipynb', 'Copy of HW2.1-QuickDrawClassifier.ipynb', 'Copy of Copy of HW2.1-QuickDrawClassifier.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks'\n",
        "if in_colab:\n",
        "    # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
        "    # Example: If you create a CS7150 folder and put all the files under HW2 folder, then 'CS7150/HW2'\n",
        "    # GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS7150/HW2'\n",
        "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks'\n",
        "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "    print(os.listdir(GOOGLE_DRIVE_PATH))"
      ],
      "id": "8f1e917c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6db073"
      },
      "source": [
        "# Loading TinyQuickDraw Data"
      ],
      "id": "2d6db073"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af842fd2"
      },
      "source": [
        "The QuickDraw dataset is a recent popular dataset that consists of human-drawn images that were drawn by users playing the [Quick, Draw!](https://quickdraw.withgoogle.com/) game. The entire dataset consists of about 50 million drawings across 345 categories. For this exercise, we chose a subset of images, TinyQuickDraw, across 20 categories, namely: Apple, Bat, Broccoli, Carrot, Cookie, Donut, Horse, Knee, Leaf, Lobster, Mushroom, Pizza, Rain, River, Sandwich, Shark, Strawberry, T-Shirt, Van, and Watermelon. Hence, our data will be classified into one of these 20 classes.\n",
        "\n",
        "You should have a local copy of your data in the directory of where this notebook is stored. Since this is a custom dataset where images with png extensions are stored, we use the [ImageFolder](http://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) package to load our data."
      ],
      "id": "af842fd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "ca18b354f0974e3f9455744e0007f07a",
            "1785f8f3f8444dbda75b7007891ad113",
            "09842abe94dd4b208ad3770950e5170f",
            "5b29eff153d841f3b3814bfe410cffe9",
            "826f6e617627401faeae7001417d00a0",
            "087374c2f407486d8cb62ec6fd4531a6",
            "3901cd5a42dd4a86a5372dff14095380",
            "c1a6c02212854b9bbef2feb58ba6f269",
            "5cf0dc3a20cc4f12b89e00f13d5ed55e",
            "7668ad40d6924e4aa43b269f1fc35b1e",
            "314b4052325344d1af87888f5eb37e98"
          ]
        },
        "id": "0c9c37c7",
        "outputId": "17f4cd15-c340-4bf5-d5be-3e50d6049340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://cs7150.baulab.info/2022-Fall/data/tiny_quick_draw.zip to tiny_quick_draw/tiny_quick_draw.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2965111 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca18b354f0974e3f9455744e0007f07a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting tiny_quick_draw/tiny_quick_draw.zip to tiny_quick_draw\n",
            "Labels and their assigned label numbers:  {'Apple': 0, 'Bat': 1, 'Broccoli': 2, 'Carrot': 3, 'Cookie': 4, 'Donut': 5, 'Horse': 6, 'Knee': 7, 'Leaf': 8, 'Lobster': 9, 'Mushroom': 10, 'Pizza': 11, 'Rain': 12, 'River': 13, 'Sandwich': 14, 'Shark': 15, 'Strawberry': 16, 'T-Shirt': 17, 'Van': 18, 'Watermelon': 19}\n",
            "Number of samples in training data: 500\n",
            "Number of samples in test data: 500\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "import torch.utils.data as data_utils\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "# setting directory paths for training and testing data\n",
        "url = \"https://cs7150.baulab.info/2022-Fall/data/tiny_quick_draw.zip\"\n",
        "if not os.path.isdir('tiny_quick_draw'):\n",
        "    download_and_extract_archive(url, 'tiny_quick_draw')\n",
        "train_data_dir = \"tiny_quick_draw/train\"\n",
        "test_data_dir = \"tiny_quick_draw/test\"\n",
        "train_data = ImageFolder(train_data_dir, transform=Compose([ToTensor()]))\n",
        "# storing and printing what each labels represent\n",
        "class_to_idx = train_data.class_to_idx\n",
        "print(\"Labels and their assigned label numbers: \", class_to_idx)\n",
        "test_data = ImageFolder(test_data_dir, transform=Compose([ToTensor()]))\n",
        "# check the length of dataset\n",
        "print(f'Number of samples in training data: {len(train_data)}')\n",
        "print(f'Number of samples in test data: {len(test_data)}')"
      ],
      "id": "0c9c37c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "507b2b64"
      },
      "source": [
        "# Displaying Loaded Dataset"
      ],
      "id": "507b2b64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "22b496c9",
        "outputId": "55850620-5d57-475d-eba8-b58cead227b2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx_to_class = {}\n",
        "# creating a label number to label dict for easy representation in graphs\n",
        "for label, idx in class_to_idx.items():\n",
        "    idx_to_class[idx] = label\n",
        "    \n",
        "fig = plt.figure()\n",
        "\n",
        "# showing 6 different images of the Apple class\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Class Label: {}\".format(idx_to_class[train_data[i][1]]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "id": "22b496c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b11341"
      },
      "source": [
        "# Building a Neural Network"
      ],
      "id": "f4b11341"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468b60ba"
      },
      "source": [
        "Neural networks consist of modules that perform data operations. The torch.nn namespace provides all the building blocks we would need to build our own neural network. In the following sections, we’ll build a neural network to classify images in the TinyQuickDraw dataset."
      ],
      "id": "468b60ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a180f9c7"
      },
      "source": [
        "## Defining a class"
      ],
      "id": "a180f9c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "003e4e25"
      },
      "source": [
        "We define our neural network by subclassing nn.Module, and initializing the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method. In the following cells, we show how we construct our neural network and illustrate the different components in it."
      ],
      "id": "003e4e25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd2c5289"
      },
      "outputs": [],
      "source": [
        "class TinyQuickDrawClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyQuickDrawClassifier, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.class_size = 20\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*28*28, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, self.class_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "id": "bd2c5289"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d758ebc"
      },
      "source": [
        "Let's create an instance of QuickDrawClassifier and move it to the device"
      ],
      "id": "3d758ebc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c872dd03"
      },
      "outputs": [],
      "source": [
        "model = TinyQuickDrawClassifier().to(device)"
      ],
      "id": "c872dd03"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8d94e98"
      },
      "source": [
        "### Model Parameters"
      ],
      "id": "f8d94e98"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40e8d3f3"
      },
      "source": [
        "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and a preview of its values."
      ],
      "id": "40e8d3f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4207cf93",
        "outputId": "9443327c-2adb-42c7-a712-fa98836d5a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: TinyQuickDrawClassifier(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=2352, out_features=1000, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1000, out_features=20, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([1000, 2352]) | Values : tensor([[ 0.0099, -0.0176, -0.0096,  ..., -0.0107,  0.0058, -0.0066],\n",
            "        [ 0.0150, -0.0121,  0.0160,  ..., -0.0174,  0.0199, -0.0043]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([1000]) | Values : tensor([-0.0002, -0.0025], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([20, 1000]) | Values : tensor([[-0.0310,  0.0048, -0.0291,  ...,  0.0315,  0.0213,  0.0128],\n",
            "        [-0.0311, -0.0204,  0.0140,  ...,  0.0081,  0.0016,  0.0223]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([20]) | Values : tensor([-0.0286, -0.0313], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "id": "4207cf93"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "640da335"
      },
      "source": [
        "Let’s break down the layers we created in the QuickDrawClassifier model. To illustrate it, we will take a sample image and see what happens to it as we pass it through the network."
      ],
      "id": "640da335"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869873c3"
      },
      "source": [
        "### Sample Image"
      ],
      "id": "869873c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "b499eafb",
        "outputId": "74e5d9d9-adde-4f71-dd38-2bfb51d13bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the image shown: torch.Size([1, 3, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "input_image = train_data[1][0]\n",
        "input_image = input_image[None,:,:,:]\n",
        "print(\"Size of the image shown:\", input_image.size())\n",
        "plt.imshow(input_image[0][0], cmap='gray', interpolation='none')\n",
        "plt.title(\"Class Label: {}\".format(idx_to_class[train_data[1][1]]))\n",
        "plt.show()"
      ],
      "id": "b499eafb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a59337"
      },
      "source": [
        "### nn.Flatten"
      ],
      "id": "59a59337"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b197f145"
      },
      "source": [
        "In __init__, we defined flatten to be [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html). It is used in the forward function. Its purpose is to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained)."
      ],
      "id": "b197f145"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9f02b084",
        "outputId": "e1cdb95d-0c6b-4dfd-894d-11c40cf7e681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2352])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe7d5d91790>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 10224x7200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())\n",
        "fig = plt.figure()\n",
        "fig.set_figwidth(142)\n",
        "fig.set_figheight(100)\n",
        "plt.imshow(flat_image, cmap='gray') #Note: Zoom in a lot to see the image being flatten to a vector"
      ],
      "id": "9f02b084"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ff664a"
      },
      "source": [
        "### nn.Linear"
      ],
      "id": "89ff664a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3910b3a"
      },
      "source": [
        "[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
      ],
      "id": "f3910b3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ff6c10",
        "outputId": "0d957433-36dd-4ae6-e157-a0dea22a25e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=3*28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "id": "82ff6c10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f290b4fd"
      },
      "source": [
        "### nn.ReLU"
      ],
      "id": "f290b4fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e89d81ca"
      },
      "source": [
        "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena. In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there’s other activations to introduce non-linearity in our model."
      ],
      "id": "e89d81ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41371ad",
        "outputId": "76849fa1-bb8e-4ca3-c107-903cbe12c46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.1057,  0.0668,  0.2621, -0.5927,  0.3658,  0.7562,  0.1000, -0.2912,\n",
            "         -1.3275, -0.2982,  0.0443,  0.2387,  0.3715, -0.5811,  0.6200,  0.0755,\n",
            "          0.7789,  0.0892,  0.3350,  0.7007]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.1057, 0.0668, 0.2621, 0.0000, 0.3658, 0.7562, 0.1000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0443, 0.2387, 0.3715, 0.0000, 0.6200, 0.0755, 0.7789, 0.0892,\n",
            "         0.3350, 0.7007]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "id": "a41371ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2b5017a"
      },
      "source": [
        "### nn.Sequential"
      ],
      "id": "a2b5017a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ec09cd0"
      },
      "source": [
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
      ],
      "id": "1ec09cd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a58e485d"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 20)\n",
        ")\n",
        "input_image = train_data[1][0]\n",
        "logits = seq_modules(input_image[None,:,:,:])"
      ],
      "id": "a58e485d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f9f25c"
      },
      "source": [
        "### nn.Softmax and Predicting class"
      ],
      "id": "30f9f25c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab940a31"
      },
      "source": [
        "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. The 'dim' parameter indicates the dimension along which the values must sum to 1.\n",
        "\n",
        "*Note: Since the model hasn't been trained yet (includes randomized weights and bias values), it is very likely that the predicted image label is different from the actual one.*"
      ],
      "id": "ab940a31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "b0cab7fb",
        "outputId": "ea503be1-af53-47d8-c0a9-771da8173b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Shark\n",
            "Actual class: Apple\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summing of probabilities from each class for an image is equal to 1.0000000223517418\n"
          ]
        }
      ],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "pred_probab_detached = pred_probab.detach().numpy()[0]\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {idx_to_class[y_pred.item()]}\")\n",
        "y_actual = train_data[1][1]\n",
        "print(f\"Actual class: {idx_to_class[y_actual]}\")\n",
        "\n",
        "plt.bar([i for i in range(len(pred_probab_detached))], pred_probab_detached, color ='maroon',\n",
        "        width = 0.4)\n",
        " \n",
        "plt.xlabel(\"Class Labels\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.title(\"Probability of an image being predicted as one of the class labels\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Summing of probabilities from each class for an image is equal to\", sum(pred_probab_detached))"
      ],
      "id": "b0cab7fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "720bfbd5"
      },
      "source": [
        "# Training a Neural Network"
      ],
      "id": "720bfbd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e209a3a"
      },
      "source": [
        "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph. In the following cell, we set the parameters within the model function to activate this feature."
      ],
      "id": "7e209a3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa8601e5",
        "outputId": "39e1fcd9-c4c7-45c6-af63-a464a461f54d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyQuickDrawClassifier(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=2352, out_features=1000, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=1000, out_features=20, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.requires_grad_(True)"
      ],
      "id": "fa8601e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a925f9d9"
      },
      "source": [
        "Before defining our training loop, let's define our hyperparameters, loss and optimizer functions."
      ],
      "id": "a925f9d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95dd6c1d"
      },
      "source": [
        "## Hyperparameters"
      ],
      "id": "95dd6c1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cffa834c"
      },
      "source": [
        "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        "\n",
        "- Number of Epochs - the number times to iterate over the dataset\n",
        "\n",
        "- Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
        "\n",
        "- Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ],
      "id": "cffa834c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a80fd6ff"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.02\n",
        "batch_size = 500\n",
        "epochs = 300"
      ],
      "id": "a80fd6ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8622f798"
      },
      "source": [
        "## Loss function"
      ],
      "id": "8622f798"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb140687"
      },
      "source": [
        "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value. Common loss functions include [nn.MSELoss (Mean Square Error)](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) and nn.CrossEntropyLoss. "
      ],
      "id": "eb140687"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a309c28f"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "id": "a309c28f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c70f21dc"
      },
      "source": [
        "## Optimizer"
      ],
      "id": "c70f21dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be187fdb"
      },
      "source": [
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this case, we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different [optimizers](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter."
      ],
      "id": "be187fdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d4dbf68"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.8)"
      ],
      "id": "7d4dbf68"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d6282d"
      },
      "source": [
        "## Train Loop"
      ],
      "id": "d2d6282d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70932b79"
      },
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "\n",
        "- Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "\n",
        "- Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "\n",
        "- Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
      ],
      "id": "70932b79"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7b9f4a0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "id": "a7b9f4a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1a06d08"
      },
      "source": [
        "In the above cell, we used a Dataloader to create batches for training and testing data. For each batch of size indicated in the batch_size hyperparameter, we perform backprop and update the model parameters' weights and biases.\n",
        "\n",
        "In the following cell, we define our train_loop."
      ],
      "id": "a1a06d08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6c4ecda"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, print_log=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    training_acc = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.to(device))\n",
        "        loss = loss_fn(pred, y.to(device))\n",
        "        correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (print_log==True) and (batch % 100 == 0):\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"\"\"Training loop: loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\"\"\")\n",
        "    correct /= size\n",
        "    training_acc = 100*correct\n",
        "    if (print_log==True):\n",
        "        print(f\"\"\"Training Accuracy: {training_acc:>0.1f}%\"\"\")\n",
        "    return training_acc"
      ],
      "id": "c6c4ecda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f788c657"
      },
      "source": [
        "## Test Loop"
      ],
      "id": "f788c657"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1069ff6"
      },
      "source": [
        "In the test loop, we iterate over the test dataset to check if model performance is improving."
      ],
      "id": "c1069ff6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e0b264d"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn, print_log=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred, y.to(device)).item()\n",
        "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    testing_acc = 100*correct\n",
        "    if (print_log==True):\n",
        "        print(f\"Testing loop: \\n Accuracy: {testing_acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return testing_acc, test_loss"
      ],
      "id": "6e0b264d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23b66834"
      },
      "source": [
        "## Running the loops"
      ],
      "id": "23b66834"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5499d58d"
      },
      "source": [
        "We run our loops for a certain number of times, which is indicated in the 'epoch' hyperparameter that we defined earlier. In the following cell, we run both our training and testing loop to see how our training and testing accuracies change over time."
      ],
      "id": "5499d58d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37805
        },
        "id": "d01bf15a",
        "outputId": "e6e0db07-7462-4ce0-b901-56576ec9caa4",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 3.023751  [    0/  500]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 6.2%, Avg loss: 2.999137 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.998937  [    0/  500]\n",
            "Training Accuracy: 6.0%\n",
            "Testing loop: \n",
            " Accuracy: 10.4%, Avg loss: 2.978129 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.971754  [    0/  500]\n",
            "Training Accuracy: 9.0%\n",
            "Testing loop: \n",
            " Accuracy: 7.0%, Avg loss: 2.964107 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.950931  [    0/  500]\n",
            "Training Accuracy: 10.2%\n",
            "Testing loop: \n",
            " Accuracy: 6.8%, Avg loss: 2.951036 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.930699  [    0/  500]\n",
            "Training Accuracy: 9.0%\n",
            "Testing loop: \n",
            " Accuracy: 7.6%, Avg loss: 2.932261 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.904244  [    0/  500]\n",
            "Training Accuracy: 10.8%\n",
            "Testing loop: \n",
            " Accuracy: 13.8%, Avg loss: 2.909678 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.873697  [    0/  500]\n",
            "Training Accuracy: 20.0%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.888343 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.844358  [    0/  500]\n",
            "Training Accuracy: 40.4%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.868965 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.816643  [    0/  500]\n",
            "Training Accuracy: 34.6%\n",
            "Testing loop: \n",
            " Accuracy: 27.4%, Avg loss: 2.848860 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.787641  [    0/  500]\n",
            "Training Accuracy: 37.0%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.826459 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.755845  [    0/  500]\n",
            "Training Accuracy: 43.6%\n",
            "Testing loop: \n",
            " Accuracy: 36.0%, Avg loss: 2.802429 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.722424  [    0/  500]\n",
            "Training Accuracy: 53.8%\n",
            "Testing loop: \n",
            " Accuracy: 37.6%, Avg loss: 2.777958 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.688420  [    0/  500]\n",
            "Training Accuracy: 57.6%\n",
            "Testing loop: \n",
            " Accuracy: 39.6%, Avg loss: 2.753132 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.653732  [    0/  500]\n",
            "Training Accuracy: 57.0%\n",
            "Testing loop: \n",
            " Accuracy: 40.4%, Avg loss: 2.727358 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.617723  [    0/  500]\n",
            "Training Accuracy: 58.2%\n",
            "Testing loop: \n",
            " Accuracy: 41.6%, Avg loss: 2.700454 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.580155  [    0/  500]\n",
            "Training Accuracy: 57.8%\n",
            "Testing loop: \n",
            " Accuracy: 41.2%, Avg loss: 2.672984 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.541814  [    0/  500]\n",
            "Training Accuracy: 58.8%\n",
            "Testing loop: \n",
            " Accuracy: 40.0%, Avg loss: 2.645199 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.502875  [    0/  500]\n",
            "Training Accuracy: 60.0%\n",
            "Testing loop: \n",
            " Accuracy: 40.0%, Avg loss: 2.616998 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.463347  [    0/  500]\n",
            "Training Accuracy: 60.2%\n",
            "Testing loop: \n",
            " Accuracy: 41.6%, Avg loss: 2.588206 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 2.422977  [    0/  500]\n",
            "Training Accuracy: 61.0%\n",
            "Testing loop: \n",
            " Accuracy: 42.2%, Avg loss: 2.558913 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 2.381925  [    0/  500]\n",
            "Training Accuracy: 62.4%\n",
            "Testing loop: \n",
            " Accuracy: 42.6%, Avg loss: 2.529373 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 2.340452  [    0/  500]\n",
            "Training Accuracy: 62.0%\n",
            "Testing loop: \n",
            " Accuracy: 42.0%, Avg loss: 2.499779 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 2.298770  [    0/  500]\n",
            "Training Accuracy: 61.6%\n",
            "Testing loop: \n",
            " Accuracy: 42.4%, Avg loss: 2.470218 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 2.256858  [    0/  500]\n",
            "Training Accuracy: 61.8%\n",
            "Testing loop: \n",
            " Accuracy: 42.4%, Avg loss: 2.440768 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 2.214935  [    0/  500]\n",
            "Training Accuracy: 62.6%\n",
            "Testing loop: \n",
            " Accuracy: 42.8%, Avg loss: 2.411595 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 2.173144  [    0/  500]\n",
            "Training Accuracy: 63.4%\n",
            "Testing loop: \n",
            " Accuracy: 43.0%, Avg loss: 2.382725 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 2.131588  [    0/  500]\n",
            "Training Accuracy: 63.8%\n",
            "Testing loop: \n",
            " Accuracy: 44.2%, Avg loss: 2.354232 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 2.090384  [    0/  500]\n",
            "Training Accuracy: 64.0%\n",
            "Testing loop: \n",
            " Accuracy: 44.4%, Avg loss: 2.326286 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 2.049664  [    0/  500]\n",
            "Training Accuracy: 64.4%\n",
            "Testing loop: \n",
            " Accuracy: 44.4%, Avg loss: 2.298964 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 2.009583  [    0/  500]\n",
            "Training Accuracy: 65.0%\n",
            "Testing loop: \n",
            " Accuracy: 44.8%, Avg loss: 2.272347 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 1.970196  [    0/  500]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 44.8%, Avg loss: 2.246506 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 1.931639  [    0/  500]\n",
            "Training Accuracy: 65.8%\n",
            "Testing loop: \n",
            " Accuracy: 45.0%, Avg loss: 2.221481 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 1.893962  [    0/  500]\n",
            "Training Accuracy: 66.2%\n",
            "Testing loop: \n",
            " Accuracy: 45.4%, Avg loss: 2.197307 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 1.857245  [    0/  500]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 45.6%, Avg loss: 2.174045 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 1.821534  [    0/  500]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 45.6%, Avg loss: 2.151708 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 1.786870  [    0/  500]\n",
            "Training Accuracy: 67.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.130268 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 1.753245  [    0/  500]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.109689 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 1.720668  [    0/  500]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.090029 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 1.689114  [    0/  500]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 2.071323 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 1.658584  [    0/  500]\n",
            "Training Accuracy: 68.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 2.053523 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 1.629074  [    0/  500]\n",
            "Training Accuracy: 69.0%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 2.036589 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 1.600549  [    0/  500]\n",
            "Training Accuracy: 69.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 2.020490 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 1.572978  [    0/  500]\n",
            "Training Accuracy: 69.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 2.005176 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 1.546330  [    0/  500]\n",
            "Training Accuracy: 70.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.990633 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 1.520565  [    0/  500]\n",
            "Training Accuracy: 71.0%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 1.976834 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 1.495639  [    0/  500]\n",
            "Training Accuracy: 71.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.2%, Avg loss: 1.963724 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 1.471511  [    0/  500]\n",
            "Training Accuracy: 72.2%\n",
            "Testing loop: \n",
            " Accuracy: 48.2%, Avg loss: 1.951258 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 1.448147  [    0/  500]\n",
            "Training Accuracy: 72.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.939431 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 1.425526  [    0/  500]\n",
            "Training Accuracy: 72.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.928228 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 1.403628  [    0/  500]\n",
            "Training Accuracy: 72.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.917614 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 1.382385  [    0/  500]\n",
            "Training Accuracy: 72.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.907562 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 1.361768  [    0/  500]\n",
            "Training Accuracy: 72.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.898014 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 1.341747  [    0/  500]\n",
            "Training Accuracy: 72.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.888977 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 1.322290  [    0/  500]\n",
            "Training Accuracy: 73.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.880440 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 1.303380  [    0/  500]\n",
            "Training Accuracy: 73.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.872369 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 1.284988  [    0/  500]\n",
            "Training Accuracy: 74.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.864706 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 1.267080  [    0/  500]\n",
            "Training Accuracy: 74.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.857451 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 1.249633  [    0/  500]\n",
            "Training Accuracy: 74.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.850564 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 1.232616  [    0/  500]\n",
            "Training Accuracy: 74.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.844018 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 1.216023  [    0/  500]\n",
            "Training Accuracy: 75.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.837833 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 1.199828  [    0/  500]\n",
            "Training Accuracy: 75.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.831978 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 1.184002  [    0/  500]\n",
            "Training Accuracy: 75.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.826459 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 1.168527  [    0/  500]\n",
            "Training Accuracy: 75.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.821247 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 1.153386  [    0/  500]\n",
            "Training Accuracy: 75.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.816325 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 1.138561  [    0/  500]\n",
            "Training Accuracy: 75.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.811641 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 1.124040  [    0/  500]\n",
            "Training Accuracy: 76.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.807238 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 1.109819  [    0/  500]\n",
            "Training Accuracy: 76.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.803119 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 1.095878  [    0/  500]\n",
            "Training Accuracy: 77.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.799266 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 1.082202  [    0/  500]\n",
            "Training Accuracy: 77.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.795611 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 1.068793  [    0/  500]\n",
            "Training Accuracy: 77.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.792109 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 1.055639  [    0/  500]\n",
            "Training Accuracy: 77.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.788830 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 1.042716  [    0/  500]\n",
            "Training Accuracy: 77.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.785753 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 1.030019  [    0/  500]\n",
            "Training Accuracy: 77.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.782895 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 1.017545  [    0/  500]\n",
            "Training Accuracy: 78.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.780224 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 1.005283  [    0/  500]\n",
            "Training Accuracy: 78.6%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.777685 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 0.993223  [    0/  500]\n",
            "Training Accuracy: 79.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.775250 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 0.981353  [    0/  500]\n",
            "Training Accuracy: 79.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.773019 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 0.969666  [    0/  500]\n",
            "Training Accuracy: 79.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.770998 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 0.958158  [    0/  500]\n",
            "Training Accuracy: 79.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.769155 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 0.946832  [    0/  500]\n",
            "Training Accuracy: 79.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.767386 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 0.935677  [    0/  500]\n",
            "Training Accuracy: 79.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.765762 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 0.924679  [    0/  500]\n",
            "Training Accuracy: 79.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.764268 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 0.913840  [    0/  500]\n",
            "Training Accuracy: 80.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.762890 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 0.903154  [    0/  500]\n",
            "Training Accuracy: 80.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.761599 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 0.892622  [    0/  500]\n",
            "Training Accuracy: 81.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.760407 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 0.882237  [    0/  500]\n",
            "Training Accuracy: 81.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.759314 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 0.871995  [    0/  500]\n",
            "Training Accuracy: 81.6%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.758401 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 0.861899  [    0/  500]\n",
            "Training Accuracy: 82.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.757609 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 0.851932  [    0/  500]\n",
            "Training Accuracy: 82.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.756846 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 0.842093  [    0/  500]\n",
            "Training Accuracy: 82.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.756149 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 0.832375  [    0/  500]\n",
            "Training Accuracy: 82.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.755576 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 0.822787  [    0/  500]\n",
            "Training Accuracy: 83.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.755149 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 0.813321  [    0/  500]\n",
            "Training Accuracy: 83.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.754790 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 0.803973  [    0/  500]\n",
            "Training Accuracy: 83.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.754477 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 0.794744  [    0/  500]\n",
            "Training Accuracy: 83.6%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.754233 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 0.785635  [    0/  500]\n",
            "Training Accuracy: 83.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.754064 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 0.776640  [    0/  500]\n",
            "Training Accuracy: 83.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.754013 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 0.767754  [    0/  500]\n",
            "Training Accuracy: 84.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.754023 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 0.758969  [    0/  500]\n",
            "Training Accuracy: 85.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.754133 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 0.750292  [    0/  500]\n",
            "Training Accuracy: 85.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.754270 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 0.741721  [    0/  500]\n",
            "Training Accuracy: 85.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.754447 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 0.733254  [    0/  500]\n",
            "Training Accuracy: 85.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.754722 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 0.724887  [    0/  500]\n",
            "Training Accuracy: 85.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.755102 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 0.716612  [    0/  500]\n",
            "Training Accuracy: 86.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.755469 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 0.708431  [    0/  500]\n",
            "Training Accuracy: 86.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.755822 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 0.700350  [    0/  500]\n",
            "Training Accuracy: 86.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.756289 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 0.692370  [    0/  500]\n",
            "Training Accuracy: 87.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.756871 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 0.684484  [    0/  500]\n",
            "Training Accuracy: 87.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.757460 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 0.676684  [    0/  500]\n",
            "Training Accuracy: 88.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.758102 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 0.668978  [    0/  500]\n",
            "Training Accuracy: 88.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.758855 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 0.661363  [    0/  500]\n",
            "Training Accuracy: 88.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.759579 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 0.653837  [    0/  500]\n",
            "Training Accuracy: 88.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.760309 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 0.646397  [    0/  500]\n",
            "Training Accuracy: 89.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.761149 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 0.639044  [    0/  500]\n",
            "Training Accuracy: 89.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.762096 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 0.631775  [    0/  500]\n",
            "Training Accuracy: 89.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.763048 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 0.624588  [    0/  500]\n",
            "Training Accuracy: 89.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.763970 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 0.617483  [    0/  500]\n",
            "Training Accuracy: 89.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.764960 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 0.610457  [    0/  500]\n",
            "Training Accuracy: 89.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.766034 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 0.603512  [    0/  500]\n",
            "Training Accuracy: 89.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.767142 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 0.596645  [    0/  500]\n",
            "Training Accuracy: 90.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.768283 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 0.589856  [    0/  500]\n",
            "Training Accuracy: 90.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.769493 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 0.583146  [    0/  500]\n",
            "Training Accuracy: 90.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.770672 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 0.576507  [    0/  500]\n",
            "Training Accuracy: 90.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.771875 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 0.569951  [    0/  500]\n",
            "Training Accuracy: 91.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.773127 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 0.563461  [    0/  500]\n",
            "Training Accuracy: 91.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.774464 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 0.557051  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.775833 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 0.550712  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.777199 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 0.544443  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.778594 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 0.538245  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.779999 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 0.532119  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.781449 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 0.526053  [    0/  500]\n",
            "Training Accuracy: 91.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.782948 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 0.520068  [    0/  500]\n",
            "Training Accuracy: 91.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.784388 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 0.514142  [    0/  500]\n",
            "Training Accuracy: 91.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.785930 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 0.508288  [    0/  500]\n",
            "Training Accuracy: 91.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.787590 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 0.502502  [    0/  500]\n",
            "Training Accuracy: 91.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.789154 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 0.496784  [    0/  500]\n",
            "Training Accuracy: 92.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.790726 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 0.491129  [    0/  500]\n",
            "Training Accuracy: 92.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.792349 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 0.485542  [    0/  500]\n",
            "Training Accuracy: 92.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.794005 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 0.480019  [    0/  500]\n",
            "Training Accuracy: 93.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.795690 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 0.474559  [    0/  500]\n",
            "Training Accuracy: 93.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.797355 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 0.469161  [    0/  500]\n",
            "Training Accuracy: 93.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.799008 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 0.463829  [    0/  500]\n",
            "Training Accuracy: 93.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.800805 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 0.458558  [    0/  500]\n",
            "Training Accuracy: 93.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.802601 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 0.453352  [    0/  500]\n",
            "Training Accuracy: 93.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.804419 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 0.448208  [    0/  500]\n",
            "Training Accuracy: 93.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.806159 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 0.443124  [    0/  500]\n",
            "Training Accuracy: 94.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.807907 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 0.438100  [    0/  500]\n",
            "Training Accuracy: 94.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.809735 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 0.433136  [    0/  500]\n",
            "Training Accuracy: 94.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.811622 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 0.428229  [    0/  500]\n",
            "Training Accuracy: 94.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.813500 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 0.423386  [    0/  500]\n",
            "Training Accuracy: 94.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.815353 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loop: loss: 0.418596  [    0/  500]\n",
            "Training Accuracy: 95.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.817250 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loop: loss: 0.413861  [    0/  500]\n",
            "Training Accuracy: 95.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.819178 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loop: loss: 0.409187  [    0/  500]\n",
            "Training Accuracy: 95.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.821021 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loop: loss: 0.404568  [    0/  500]\n",
            "Training Accuracy: 95.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.822984 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loop: loss: 0.400007  [    0/  500]\n",
            "Training Accuracy: 95.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.825017 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loop: loss: 0.395494  [    0/  500]\n",
            "Training Accuracy: 95.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.826910 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loop: loss: 0.391045  [    0/  500]\n",
            "Training Accuracy: 95.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.828794 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loop: loss: 0.386644  [    0/  500]\n",
            "Training Accuracy: 95.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.830846 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loop: loss: 0.382300  [    0/  500]\n",
            "Training Accuracy: 95.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.832808 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loop: loss: 0.378011  [    0/  500]\n",
            "Training Accuracy: 95.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.834737 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loop: loss: 0.373772  [    0/  500]\n",
            "Training Accuracy: 95.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.836787 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loop: loss: 0.369588  [    0/  500]\n",
            "Training Accuracy: 96.2%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.838953 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loop: loss: 0.365454  [    0/  500]\n",
            "Training Accuracy: 96.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.840993 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loop: loss: 0.361371  [    0/  500]\n",
            "Training Accuracy: 96.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.842901 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loop: loss: 0.357342  [    0/  500]\n",
            "Training Accuracy: 96.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.844827 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loop: loss: 0.353357  [    0/  500]\n",
            "Training Accuracy: 96.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.846943 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loop: loss: 0.349426  [    0/  500]\n",
            "Training Accuracy: 96.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.849055 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loop: loss: 0.345545  [    0/  500]\n",
            "Training Accuracy: 96.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.851112 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loop: loss: 0.341709  [    0/  500]\n",
            "Training Accuracy: 96.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.853197 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loop: loss: 0.337926  [    0/  500]\n",
            "Training Accuracy: 96.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.855246 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loop: loss: 0.334184  [    0/  500]\n",
            "Training Accuracy: 96.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.857322 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loop: loss: 0.330496  [    0/  500]\n",
            "Training Accuracy: 96.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.859463 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loop: loss: 0.326853  [    0/  500]\n",
            "Training Accuracy: 96.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.861590 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loop: loss: 0.323255  [    0/  500]\n",
            "Training Accuracy: 97.2%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.863629 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loop: loss: 0.319706  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.865793 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loop: loss: 0.316198  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.867990 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loop: loss: 0.312740  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.870068 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loop: loss: 0.309323  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.872100 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loop: loss: 0.305950  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.874301 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loop: loss: 0.302614  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.876523 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loop: loss: 0.299333  [    0/  500]\n",
            "Training Accuracy: 97.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.878601 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loop: loss: 0.296085  [    0/  500]\n",
            "Training Accuracy: 97.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.880761 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loop: loss: 0.292883  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.882915 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loop: loss: 0.289725  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.885136 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loop: loss: 0.286605  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.887288 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loop: loss: 0.283527  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.889423 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loop: loss: 0.280492  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.891516 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loop: loss: 0.277498  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.893772 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loop: loss: 0.274542  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.895960 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loop: loss: 0.271622  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.898113 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loop: loss: 0.268746  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.900295 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loop: loss: 0.265904  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.902516 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loop: loss: 0.263098  [    0/  500]\n",
            "Training Accuracy: 97.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.904611 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loop: loss: 0.260328  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.906767 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loop: loss: 0.257595  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.908981 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loop: loss: 0.254901  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.911189 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loop: loss: 0.252241  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.913384 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loop: loss: 0.249616  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.915552 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loop: loss: 0.247026  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.917684 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loop: loss: 0.244470  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.919894 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loop: loss: 0.241950  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.922059 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loop: loss: 0.239460  [    0/  500]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.924194 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loop: loss: 0.237006  [    0/  500]\n",
            "Training Accuracy: 98.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.926394 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loop: loss: 0.234585  [    0/  500]\n",
            "Training Accuracy: 98.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.928602 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loop: loss: 0.232191  [    0/  500]\n",
            "Training Accuracy: 98.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.930715 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loop: loss: 0.229833  [    0/  500]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.932867 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loop: loss: 0.227505  [    0/  500]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.935155 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loop: loss: 0.225210  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.937318 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loop: loss: 0.222943  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.939405 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loop: loss: 0.220707  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.941530 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loop: loss: 0.218504  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.943740 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loop: loss: 0.216325  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.945879 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loop: loss: 0.214176  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.948061 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loop: loss: 0.212054  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.950274 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loop: loss: 0.209961  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.952409 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loop: loss: 0.207896  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.954451 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loop: loss: 0.205858  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.956602 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loop: loss: 0.203846  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.958750 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loop: loss: 0.201864  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.960886 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loop: loss: 0.199903  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.963043 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loop: loss: 0.197973  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 1.965191 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loop: loss: 0.196065  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.967281 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loop: loss: 0.194186  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.969407 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loop: loss: 0.192327  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.971508 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loop: loss: 0.190493  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.973610 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loop: loss: 0.188685  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.975693 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loop: loss: 0.186899  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.977813 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loop: loss: 0.185136  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.979910 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loop: loss: 0.183399  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.982078 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loop: loss: 0.181682  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.984085 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loop: loss: 0.179989  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.986153 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loop: loss: 0.178316  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.988200 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loop: loss: 0.176667  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.990279 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loop: loss: 0.175039  [    0/  500]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.992433 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loop: loss: 0.173431  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.994491 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loop: loss: 0.171844  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.996582 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loop: loss: 0.170280  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.998608 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loop: loss: 0.168733  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 2.000657 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loop: loss: 0.167211  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 2.002635 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loop: loss: 0.165701  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 2.004713 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loop: loss: 0.164216  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 2.006813 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loop: loss: 0.162748  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.008910 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loop: loss: 0.161298  [    0/  500]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.010851 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loop: loss: 0.159871  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.012813 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loop: loss: 0.158457  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.014824 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loop: loss: 0.157062  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.016803 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loop: loss: 0.155687  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.018790 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loop: loss: 0.154327  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.020835 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loop: loss: 0.152986  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 2.022912 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loop: loss: 0.151661  [    0/  500]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.024872 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loop: loss: 0.150353  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.026757 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loop: loss: 0.149063  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.028713 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loop: loss: 0.147787  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.030721 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loop: loss: 0.146529  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.032704 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loop: loss: 0.145288  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.034681 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loop: loss: 0.144060  [    0/  500]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.036596 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loop: loss: 0.142846  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.038484 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loop: loss: 0.141652  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 2.040423 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loop: loss: 0.140469  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 2.042407 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loop: loss: 0.139301  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 2.044303 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loop: loss: 0.138148  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.046185 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loop: loss: 0.137008  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.048153 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loop: loss: 0.135887  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.050094 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loop: loss: 0.134775  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.051907 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loop: loss: 0.133679  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.053698 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loop: loss: 0.132597  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.055602 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loop: loss: 0.131525  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.057523 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loop: loss: 0.130469  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.059383 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loop: loss: 0.129424  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.061302 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loop: loss: 0.128393  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.063215 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loop: loss: 0.127374  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.064961 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loop: loss: 0.126367  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.066770 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loop: loss: 0.125374  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.068630 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loop: loss: 0.124391  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.070506 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loop: loss: 0.123421  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.072360 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loop: loss: 0.122464  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.074165 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loop: loss: 0.121517  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.075941 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loop: loss: 0.120581  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.077705 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loop: loss: 0.119656  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.079531 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loop: loss: 0.118743  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.081423 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loop: loss: 0.117842  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.083197 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loop: loss: 0.116950  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.084925 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loop: loss: 0.116069  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.086658 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loop: loss: 0.115199  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.088425 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loop: loss: 0.114337  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.090228 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loop: loss: 0.113486  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.092015 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loop: loss: 0.112644  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.093748 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loop: loss: 0.111814  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.095495 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loop: loss: 0.110994  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.097203 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loop: loss: 0.110183  [    0/  500]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.098938 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loop: loss: 0.109380  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.100707 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loop: loss: 0.108588  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.102415 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loop: loss: 0.107803  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.104118 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loop: loss: 0.107028  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.105826 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loop: loss: 0.106262  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.107594 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loop: loss: 0.105504  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.109337 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loop: loss: 0.104755  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.110958 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loop: loss: 0.104016  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.112611 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loop: loss: 0.103284  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.114357 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loop: loss: 0.102561  [    0/  500]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 2.116053 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe7d54f4e10>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "example_train_accs = []\n",
        "example_test_accs = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    training_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    testing_acc, _ = test_loop(test_dataloader, model, loss_fn)\n",
        "    example_train_accs.append(training_acc)\n",
        "    example_test_accs.append(testing_acc)\n",
        "print(\"Done!\")\n",
        "\n",
        "epoch_list = [i for i in range(epochs)]\n",
        "plt.plot(epoch_list, example_train_accs, color ='tab:green', label='training accuracy')\n",
        "plt.plot(epoch_list, example_test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.legend()"
      ],
      "id": "d01bf15a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b237a77"
      },
      "source": [
        "## Underfitting and Overfitting Models"
      ],
      "id": "5b237a77"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b84f296f"
      },
      "source": [
        "When the model is not trained properly, there are generally two extreme phenonemons that can occur. One is that the model is underfitted, which means that the model has not captured the underlying logic of the data. We would have high loss and low accuracy in such a case.\n",
        "\n",
        "In the above trained model, you will notice that the training accuracy is around 100% while the testing accuracy is around 50%. When there is such a significant difference between these accuracies, we say that the model is overfitted. This means that our training has focused on the particular training set so much that the model has missed its classification objective entirely. We don't want that to happen!\n",
        "\n",
        "**Exercise**: We would like you raise the testing accuracy until it is close to the training accuracy. We have a code block after the following section for you to demonstrate final modified model and its hyperparameters."
      ],
      "id": "b84f296f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e6a5c7e"
      },
      "source": [
        "# Understanding Autograd Better"
      ],
      "id": "6e6a5c7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a4ab3a"
      },
      "source": [
        "The autograd has a pack_hook function, which is called everytime an operation saves a tensor for backward.\n",
        "\n",
        "In the following cell, we print out the functions for which respective tensors are stored for computational purposes in one backward pass. We defined two functions, pack_hook and unpack_hook. The output of pack_hook is stored in the computation graph instead of the original tensor. The unpack_hook uses that return value to compute a new tensor, which is the one actually used during the backward pass.  In general, you want unpack_hook(pack_hook(t)) to be equal to t. Under the hood, PyTorch has **packed** and **unpacked** the tensor to prevent reference cycles. As a rule of thumb, you should not rely on the fact that accessing the tensor saved for backward will yield the same tensor object as the original tensor. They will however share the same storage."
      ],
      "id": "b4a4ab3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f8ea12",
        "outputId": "09131098-f920-4740-c8d9-367c5460434e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running one backpass\n",
            "About to run the model on a batch of size 500\n",
            "Packing/Storing following value in computation graph None torch.Size([500, 2352])\n",
            "Packing/Storing following value in computation graph <ReluBackward0 object at 0x7fe7d5def110> torch.Size([500, 1000])\n",
            "Packing/Storing following value in computation graph <TBackward0 object at 0x7fe7d5def110> torch.Size([1000, 20])\n",
            "Packing/Storing following value in computation graph <ReluBackward0 object at 0x7fe7d5d5eb90> torch.Size([500, 1000])\n",
            "About to run the loss\n",
            "Packing/Storing following value in computation graph <LogSoftmaxBackward0 object at 0x7fe7d5def110> torch.Size([500, 20])\n",
            "Packing/Storing following value in computation graph <LogSoftmaxBackward0 object at 0x7fe7d5def110> torch.Size([500, 20])\n",
            "Packing/Storing following value in computation graph None torch.Size([500])\n",
            "Packing/Storing following value in computation graph None torch.Size([])\n",
            "About to set zero grad\n",
            "loss.backward about to run\n",
            "Unpacking/Creating following tensor for backward pass <LogSoftmaxBackward0 object at 0x7fe7d5db8050> torch.Size([500, 20])\n",
            "Unpacking/Creating following tensor for backward pass None torch.Size([500])\n",
            "Unpacking/Creating following tensor for backward pass None torch.Size([])\n",
            "Unpacking/Creating following tensor for backward pass <LogSoftmaxBackward0 object at 0x7fe7d54fbcd0> torch.Size([500, 20])\n",
            "Unpacking/Creating following tensor for backward pass <TBackward0 object at 0x7fe7f6354ad0> torch.Size([1000, 20])\n",
            "Unpacking/Creating following tensor for backward pass <ReluBackward0 object at 0x7fe7d5def110> torch.Size([500, 1000])\n",
            "Updating gradient of None torch.Size([20])\n",
            "Updating gradient of None torch.Size([20, 1000])\n",
            "Unpacking/Creating following tensor for backward pass <ReluBackward0 object at 0x7fe7dc650690> torch.Size([500, 1000])\n",
            "Unpacking/Creating following tensor for backward pass None torch.Size([500, 2352])\n",
            "Updating gradient of None torch.Size([1000])\n",
            "Updating gradient of None torch.Size([1000, 2352])\n",
            "optimizer.step() about to run\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def pack_hook(x):\n",
        "    print(\"Packing/Storing following value in computation graph\", x.grad_fn, x.shape)\n",
        "    return x\n",
        "\n",
        "def unpack_hook(x):\n",
        "    print(\"Unpacking/Creating following tensor for backward pass\", x.grad_fn, x.shape)\n",
        "    return x\n",
        "\n",
        "def grad_hook(x):\n",
        "    print(\"Updating gradient of\", x.grad_fn, x.shape)\n",
        "    return x\n",
        "\n",
        "def train_instance_with_saved_tensor(dataloader, model, loss_fn, optimizer, print_log=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    for p in model.parameters():\n",
        "        p.register_hook(grad_hook)\n",
        "    with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Compute prediction and loss\n",
        "            print(\"About to run the model on a batch of size\", len(X))\n",
        "            pred = model(X.to(device))\n",
        "            print(\"About to run the loss\")\n",
        "            loss = loss_fn(pred, y.to(device))\n",
        "\n",
        "            # Backpropagation\n",
        "            print(\"About to set zero grad\")\n",
        "            optimizer.zero_grad()\n",
        "            print(\"loss.backward about to run\")\n",
        "            loss.backward()\n",
        "            print(\"optimizer.step() about to run\")\n",
        "            optimizer.step()\n",
        "            break\n",
        "        return pred, loss\n",
        "                \n",
        "                \n",
        "print(f\"Running one backpass\")\n",
        "pred, loss = train_instance_with_saved_tensor(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Done!\")"
      ],
      "id": "88f8ea12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569f1a89"
      },
      "source": [
        "## Exercise 2.1.1"
      ],
      "id": "569f1a89"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "XMUnS0nzDdWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a0bdbe-9fbb-4e8e-be08-aeff04b8ecfd"
      },
      "id": "XMUnS0nzDdWy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.1.1)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=3bf581bcd7ba19d7ab4a4803a37b2bf70a366e33b486c689d05090edb8800b64\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(pred, params=dict(list(model.named_parameters())))"
      ],
      "metadata": {
        "id": "bsyTo7KpDjF7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "252887e6-6c26-41e1-a559-eb9a0e19b587"
      },
      "id": "bsyTo7KpDjF7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe7d553a850>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"467pt\" height=\"413pt\"\n viewBox=\"0.00 0.00 467.00 413.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 409)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-409 463,-409 463,4 -4,4\"/>\n<!-- 140633811769488 -->\n<g id=\"node1\" class=\"node\">\n<title>140633811769488</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"268,-31 191,-31 191,0 268,0 268,-31\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (500, 20)</text>\n</g>\n<!-- 140633693099728 -->\n<g id=\"node2\" class=\"node\">\n<title>140633693099728</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"280,-86 179,-86 179,-67 280,-67 280,-86\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 140633693099728&#45;&gt;140633811769488 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140633693099728&#45;&gt;140633811769488</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-66.9688C229.5,-60.1289 229.5,-50.5621 229.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-41.3678 229.5,-31.3678 226.0001,-41.3678 233.0001,-41.3678\"/>\n</g>\n<!-- 140633693098320 -->\n<g id=\"node3\" class=\"node\">\n<title>140633693098320</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"147,-141 46,-141 46,-122 147,-122 147,-141\"/>\n<text text-anchor=\"middle\" x=\"96.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633693098320&#45;&gt;140633693099728 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140633693098320&#45;&gt;140633693099728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119.6668,-121.9197C140.9912,-113.1014 172.8451,-99.9287 196.7654,-90.0369\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"198.2489,-93.2109 206.1524,-86.155 195.5738,-86.7422 198.2489,-93.2109\"/>\n</g>\n<!-- 140633812319120 -->\n<g id=\"node4\" class=\"node\">\n<title>140633812319120</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"161,-207 0,-207 0,-177 161,-177 161,-207\"/>\n<text text-anchor=\"middle\" x=\"80.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.2.bias</text>\n<text text-anchor=\"middle\" x=\"80.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140633812319120&#45;&gt;140633693098320 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140633812319120&#45;&gt;140633693098320</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.5375,-176.7333C86.6204,-168.8571 89.1828,-159.168 91.4011,-150.7803\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.7916,-151.6489 93.9648,-141.0864 88.0243,-149.8592 94.7916,-151.6489\"/>\n</g>\n<!-- 140633693099472 -->\n<g id=\"node5\" class=\"node\">\n<title>140633693099472</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"277,-141 182,-141 182,-122 277,-122 277,-141\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140633693099472&#45;&gt;140633693099728 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140633693099472&#45;&gt;140633693099728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-121.9197C229.5,-114.9083 229.5,-105.1442 229.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-96.3408 229.5,-86.3408 226.0001,-96.3409 233.0001,-96.3408\"/>\n</g>\n<!-- 140633693098704 -->\n<g id=\"node6\" class=\"node\">\n<title>140633693098704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"280,-201.5 179,-201.5 179,-182.5 280,-182.5 280,-201.5\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 140633693098704&#45;&gt;140633693099472 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140633693098704&#45;&gt;140633693099472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-182.2796C229.5,-174.0376 229.5,-161.9457 229.5,-151.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-151.3972 229.5,-141.3972 226.0001,-151.3973 233.0001,-151.3972\"/>\n</g>\n<!-- 140633693100432 -->\n<g id=\"node7\" class=\"node\">\n<title>140633693100432</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"152,-267.5 51,-267.5 51,-248.5 152,-248.5 152,-267.5\"/>\n<text text-anchor=\"middle\" x=\"101.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633693100432&#45;&gt;140633693098704 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140633693100432&#45;&gt;140633693098704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.1123,-248.403C141.6311,-237.3074 177.2541,-218.9393 201.9762,-206.192\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"203.5812,-209.3023 210.8652,-201.6085 200.3732,-203.0807 203.5812,-209.3023\"/>\n</g>\n<!-- 140633812317392 -->\n<g id=\"node8\" class=\"node\">\n<title>140633812317392</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"172,-339 11,-339 11,-309 172,-309 172,-339\"/>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.0.bias</text>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1000)</text>\n</g>\n<!-- 140633812317392&#45;&gt;140633693100432 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140633812317392&#45;&gt;140633693100432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M93.8193,-308.6924C95.2111,-299.5067 96.9963,-287.7245 98.4953,-277.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.9922,-278.1146 100.0298,-267.7031 95.0712,-277.0659 101.9922,-278.1146\"/>\n</g>\n<!-- 140633693101072 -->\n<g id=\"node9\" class=\"node\">\n<title>140633693101072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"268,-267.5 191,-267.5 191,-248.5 268,-248.5 268,-267.5\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 140633693101072&#45;&gt;140633693098704 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140633693101072&#45;&gt;140633693098704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-248.2615C229.5,-238.7077 229.5,-223.8615 229.5,-211.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-211.7784 229.5,-201.7785 226.0001,-211.7785 233.0001,-211.7784\"/>\n</g>\n<!-- 140633693098512 -->\n<g id=\"node10\" class=\"node\">\n<title>140633693098512</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"291,-333.5 190,-333.5 190,-314.5 291,-314.5 291,-333.5\"/>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633693098512&#45;&gt;140633693101072 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140633693098512&#45;&gt;140633693101072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M238.8769,-314.2615C237.2846,-304.7077 234.8103,-289.8615 232.8031,-277.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.2262,-277.067 231.1297,-267.7785 229.3214,-278.2178 236.2262,-277.067\"/>\n</g>\n<!-- 140633812317296 -->\n<g id=\"node11\" class=\"node\">\n<title>140633812317296</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"327,-405 154,-405 154,-375 327,-375 327,-405\"/>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.0.weight</text>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1000, 2352)</text>\n</g>\n<!-- 140633812317296&#45;&gt;140633693098512 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140633812317296&#45;&gt;140633693098512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M240.5,-374.6924C240.5,-365.5067 240.5,-353.7245 240.5,-343.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.0001,-343.703 240.5,-333.7031 237.0001,-343.7031 244.0001,-343.703\"/>\n</g>\n<!-- 140633693099344 -->\n<g id=\"node12\" class=\"node\">\n<title>140633693099344</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"379,-141 302,-141 302,-122 379,-122 379,-141\"/>\n<text text-anchor=\"middle\" x=\"340.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 140633693099344&#45;&gt;140633693099728 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140633693099344&#45;&gt;140633693099728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M321.1653,-121.9197C303.7637,-113.2973 277.9606,-100.512 258.1604,-90.7011\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"259.4999,-87.4588 248.9856,-86.155 256.392,-93.731 259.4999,-87.4588\"/>\n</g>\n<!-- 140633693101584 -->\n<g id=\"node13\" class=\"node\">\n<title>140633693101584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"411,-201.5 310,-201.5 310,-182.5 411,-182.5 411,-201.5\"/>\n<text text-anchor=\"middle\" x=\"360.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633693101584&#45;&gt;140633693099344 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140633693101584&#45;&gt;140633693099344</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M357.2867,-182.2796C354.5041,-173.8623 350.394,-161.4295 346.9374,-150.973\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.2337,-149.7933 343.7718,-141.3972 343.5875,-151.9905 350.2337,-149.7933\"/>\n</g>\n<!-- 140633812316912 -->\n<g id=\"node14\" class=\"node\">\n<title>140633812316912</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"459,-273 286,-273 286,-243 459,-243 459,-273\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.2.weight</text>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 1000)</text>\n</g>\n<!-- 140633812316912&#45;&gt;140633693101584 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140633812316912&#45;&gt;140633693101584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M369.7168,-242.6924C368.0467,-233.5067 365.9044,-221.7245 364.1057,-211.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.4967,-210.9156 362.2642,-201.7031 360.6096,-212.1679 367.4967,-210.9156\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(loss, params=dict(list(model.named_parameters())))"
      ],
      "metadata": {
        "id": "C2_cF6M4Dk0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "a26e8ee6-0a4c-4c51-e446-a632887e9f1b"
      },
      "id": "C2_cF6M4Dk0R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fe7d5d89c10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"467pt\" height=\"523pt\"\n viewBox=\"0.00 0.00 467.00 523.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 519)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-519 463,-519 463,4 -4,4\"/>\n<!-- 140633811769968 -->\n<g id=\"node1\" class=\"node\">\n<title>140633811769968</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"256.5,-31 202.5,-31 202.5,0 256.5,0 256.5,-31\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140634244860624 -->\n<g id=\"node2\" class=\"node\">\n<title>140634244860624</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"286,-86 173,-86 173,-67 286,-67 286,-86\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">NllLossBackward0</text>\n</g>\n<!-- 140634244860624&#45;&gt;140633811769968 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140634244860624&#45;&gt;140633811769968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-66.9688C229.5,-60.1289 229.5,-50.5621 229.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-41.3678 229.5,-31.3678 226.0001,-41.3678 233.0001,-41.3678\"/>\n</g>\n<!-- 140636082824848 -->\n<g id=\"node3\" class=\"node\">\n<title>140636082824848</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"295,-141 164,-141 164,-122 295,-122 295,-141\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LogSoftmaxBackward0</text>\n</g>\n<!-- 140636082824848&#45;&gt;140634244860624 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140636082824848&#45;&gt;140634244860624</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-121.9197C229.5,-114.9083 229.5,-105.1442 229.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-96.3408 229.5,-86.3408 226.0001,-96.3409 233.0001,-96.3408\"/>\n</g>\n<!-- 140633811782224 -->\n<g id=\"node4\" class=\"node\">\n<title>140633811782224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"280,-196 179,-196 179,-177 280,-177 280,-196\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 140633811782224&#45;&gt;140636082824848 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140633811782224&#45;&gt;140636082824848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-176.9197C229.5,-169.9083 229.5,-160.1442 229.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-151.3408 229.5,-141.3408 226.0001,-151.3409 233.0001,-151.3408\"/>\n</g>\n<!-- 140633811781008 -->\n<g id=\"node5\" class=\"node\">\n<title>140633811781008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"147,-251 46,-251 46,-232 147,-232 147,-251\"/>\n<text text-anchor=\"middle\" x=\"96.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633811781008&#45;&gt;140633811782224 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140633811781008&#45;&gt;140633811782224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M119.6668,-231.9197C140.9912,-223.1014 172.8451,-209.9287 196.7654,-200.0369\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"198.2489,-203.2109 206.1524,-196.155 195.5738,-196.7422 198.2489,-203.2109\"/>\n</g>\n<!-- 140633812319120 -->\n<g id=\"node6\" class=\"node\">\n<title>140633812319120</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"161,-317 0,-317 0,-287 161,-287 161,-317\"/>\n<text text-anchor=\"middle\" x=\"80.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.2.bias</text>\n<text text-anchor=\"middle\" x=\"80.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140633812319120&#45;&gt;140633811781008 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140633812319120&#45;&gt;140633811781008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.5375,-286.7333C86.6204,-278.8571 89.1828,-269.168 91.4011,-260.7803\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.7916,-261.6489 93.9648,-251.0864 88.0243,-259.8592 94.7916,-261.6489\"/>\n</g>\n<!-- 140633811781072 -->\n<g id=\"node7\" class=\"node\">\n<title>140633811781072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"277,-251 182,-251 182,-232 277,-232 277,-251\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140633811781072&#45;&gt;140633811782224 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140633811781072&#45;&gt;140633811782224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-231.9197C229.5,-224.9083 229.5,-215.1442 229.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-206.3408 229.5,-196.3408 226.0001,-206.3409 233.0001,-206.3408\"/>\n</g>\n<!-- 140633702029776 -->\n<g id=\"node8\" class=\"node\">\n<title>140633702029776</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"280,-311.5 179,-311.5 179,-292.5 280,-292.5 280,-311.5\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 140633702029776&#45;&gt;140633811781072 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140633702029776&#45;&gt;140633811781072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-292.2796C229.5,-284.0376 229.5,-271.9457 229.5,-261.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-261.3972 229.5,-251.3972 226.0001,-261.3973 233.0001,-261.3972\"/>\n</g>\n<!-- 140633702030224 -->\n<g id=\"node9\" class=\"node\">\n<title>140633702030224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"152,-377.5 51,-377.5 51,-358.5 152,-358.5 152,-377.5\"/>\n<text text-anchor=\"middle\" x=\"101.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633702030224&#45;&gt;140633702029776 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140633702030224&#45;&gt;140633702029776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.1123,-358.403C141.6311,-347.3074 177.2541,-328.9393 201.9762,-316.192\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"203.5812,-319.3023 210.8652,-311.6085 200.3732,-313.0807 203.5812,-319.3023\"/>\n</g>\n<!-- 140633812317392 -->\n<g id=\"node10\" class=\"node\">\n<title>140633812317392</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"172,-449 11,-449 11,-419 172,-419 172,-449\"/>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.0.bias</text>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1000)</text>\n</g>\n<!-- 140633812317392&#45;&gt;140633702030224 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140633812317392&#45;&gt;140633702030224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M93.8193,-418.6924C95.2111,-409.5067 96.9963,-397.7245 98.4953,-387.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.9922,-388.1146 100.0298,-377.7031 95.0712,-387.0659 101.9922,-388.1146\"/>\n</g>\n<!-- 140633702030928 -->\n<g id=\"node11\" class=\"node\">\n<title>140633702030928</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"268,-377.5 191,-377.5 191,-358.5 268,-358.5 268,-377.5\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 140633702030928&#45;&gt;140633702029776 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140633702030928&#45;&gt;140633702029776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.5,-358.2615C229.5,-348.7077 229.5,-333.8615 229.5,-321.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.0001,-321.7784 229.5,-311.7785 226.0001,-321.7785 233.0001,-321.7784\"/>\n</g>\n<!-- 140636139539152 -->\n<g id=\"node12\" class=\"node\">\n<title>140636139539152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"291,-443.5 190,-443.5 190,-424.5 291,-424.5 291,-443.5\"/>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-431.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140636139539152&#45;&gt;140633702030928 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140636139539152&#45;&gt;140633702030928</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M238.8769,-424.2615C237.2846,-414.7077 234.8103,-399.8615 232.8031,-387.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"236.2262,-387.067 231.1297,-377.7785 229.3214,-388.2178 236.2262,-387.067\"/>\n</g>\n<!-- 140633812317296 -->\n<g id=\"node13\" class=\"node\">\n<title>140633812317296</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"327,-515 154,-515 154,-485 327,-485 327,-515\"/>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.0.weight</text>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-492\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1000, 2352)</text>\n</g>\n<!-- 140633812317296&#45;&gt;140636139539152 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140633812317296&#45;&gt;140636139539152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M240.5,-484.6924C240.5,-475.5067 240.5,-463.7245 240.5,-453.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.0001,-453.703 240.5,-443.7031 237.0001,-453.7031 244.0001,-453.703\"/>\n</g>\n<!-- 140633702028752 -->\n<g id=\"node14\" class=\"node\">\n<title>140633702028752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"379,-251 302,-251 302,-232 379,-232 379,-251\"/>\n<text text-anchor=\"middle\" x=\"340.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 140633702028752&#45;&gt;140633811782224 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140633702028752&#45;&gt;140633811782224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M321.1653,-231.9197C303.7637,-223.2973 277.9606,-210.512 258.1604,-200.7011\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"259.4999,-197.4588 248.9856,-196.155 256.392,-203.731 259.4999,-197.4588\"/>\n</g>\n<!-- 140633702031248 -->\n<g id=\"node15\" class=\"node\">\n<title>140633702031248</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"411,-311.5 310,-311.5 310,-292.5 411,-292.5 411,-311.5\"/>\n<text text-anchor=\"middle\" x=\"360.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140633702031248&#45;&gt;140633702028752 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140633702031248&#45;&gt;140633702028752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M357.2867,-292.2796C354.5041,-283.8623 350.394,-271.4295 346.9374,-260.973\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.2337,-259.7933 343.7718,-251.3972 343.5875,-261.9905 350.2337,-259.7933\"/>\n</g>\n<!-- 140633812316912 -->\n<g id=\"node16\" class=\"node\">\n<title>140633812316912</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"459,-383 286,-383 286,-353 459,-353 459,-383\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">linear_relu_stack.2.weight</text>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 1000)</text>\n</g>\n<!-- 140633812316912&#45;&gt;140633702031248 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140633812316912&#45;&gt;140633702031248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M369.7168,-352.6924C368.0467,-343.5067 365.9044,-331.7245 364.1057,-321.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.4967,-320.9156 362.2642,-311.7031 360.6096,-322.1679 367.4967,-320.9156\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d2e8de3"
      },
      "source": [
        "**Question:** Based on the saved_tensor_hook results, what values are stored for backward pass and why? What values are updated in one backward pass?"
      ],
      "id": "6d2e8de3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46ee091",
        "outputId": "e0065986-d33a-42d5-fefb-7721956ddccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For backward pass normally the values that will be useful in gradient calculation will be stored. \n",
            "A linear layer would perform Wx + B, matrix multiplication followed by bias adding. We need to send gradients backward to W and B from a linear layer. \n",
            "In order to do that we store Output of Linear Layer 1 and Linear Layer 2. Linear(x) = Wx + B, so we have a matrix multiplication then a addition \n",
            "Differentiation for addition will be alwyas one so we dont have to store that, but for matrix the gradient is based on the input passed to multiplication,\n",
            " so we store those two values that is incoming value of x and W.\n",
            "So x value that enters Linear Layer 1 and the result of activation of ReLu which is input to linear layer two along with the Weights W1 and W2 matrix is \n",
            "stored.\n",
            "In one backward pass the values that get updated are \n",
            "1) Gradient of Bias of Second Linear Layer\n",
            "2) Gradient for the weights of Second Linear Layer\n",
            "3) Gradient for weights of First Linear Layer\n",
            "4) Gradients for bias of First Linear Layer\n"
          ]
        }
      ],
      "source": [
        "# TODO:Replace the placeholder text and fill out the print statement below to answer the above question\n",
        "print(\"\"\"For backward pass normally the values that will be useful in gradient calculation will be stored. \\nA linear layer would perform Wx + B, matrix multiplication followed by bias adding. We need to send gradients backward to W and B from a linear layer. \\nIn order to do that we store Output of Linear Layer 1 and Linear Layer 2. Linear(x) = Wx + B, so we have a matrix multiplication then a addition \"\"\")\n",
        "print(\"Differentiation for addition will be alwyas one so we dont have to store that, but for matrix the gradient is based on the input passed to multiplication,\\n so we store those two values that is incoming value of x and W.\")\n",
        "print(\"So x value that enters Linear Layer 1 and the result of activation of ReLu which is input to linear layer two along with the Weights W1 and W2 matrix is \\nstored.\")\n",
        "print(\"\"\"In one backward pass the values that get updated are \\n1) Gradient of Bias of Second Linear Layer\\n2) Gradient for the weights of Second Linear Layer\\n3) Gradient for weights of First Linear Layer\\n4) Gradients for bias of First Linear Layer\"\"\")\n",
        "\n",
        "# Gradient values are updated in the backward pass( the weights ). "
      ],
      "id": "a46ee091"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0389ff2"
      },
      "source": [
        "## Exercise 2.1.2"
      ],
      "id": "a0389ff2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b815c24"
      },
      "source": [
        "**Exercise**: As mentioned earlier, we would like you to reduce the overfitting of the model. Overfitting is caused when the model is overparameterized compared to the size and diversity of the training data, and can be improved in the following ways:\n",
        "\n",
        " * Increase the size of the training data.\n",
        " * Reduce the parameters of the model, or change the number of layers.\n",
        " * Increase regularization of the model, for example, by using weight decay.\n",
        "\n",
        "In the code below we have already done the first of those steps: we have increased the size of the training data.  Be sure to shuffle the batches of training data to ensure diversity.\n",
        "\n",
        "You can further reduce overfitting by changing the number of neurons in in your model.  One way to reduce the number of neurons while keeping an expressive model is to arrange fewer neurons in more layers.\n",
        "\n",
        "You can also reduce overfitting by adjusting the hyperparameters.  Weight decay reduces the size of parameters and encourages the model to learn a simpler function that will be less prone to overfitting.\n",
        "\n",
        "In the code below we have defined the a new class `TinyQuickDrawStudentClassifier` for the  classifier and redefined the hyperparameters. You should re-implement this class and change the hyperparameters until you get the testing accuracy to stabilize at 60\\% or above.  It is OK for training accuracy to be low; your aim should be to have training accuracy close to testing accuracy.\n",
        "\n",
        "You can use the **Fine-Tuning Hyperparameters\"** section to understand and record how changes to different parameter values affect your model's accuracy.\n",
        "\n",
        "To get full credit for this section, the testing accuracy should be 60% or above. "
      ],
      "id": "6b815c24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5534080"
      },
      "outputs": [],
      "source": [
        "class TinyQuickDrawStudentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyQuickDrawStudentClassifier, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.class_size = 20\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*28*28, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(250, self.class_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "id": "a5534080"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lebi_x0P67w0"
      },
      "outputs": [],
      "source": [
        "def getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening):\n",
        "  # Train and Test\n",
        "  test_accs = []\n",
        "  test_losses = []\n",
        "  training_accs = []\n",
        "  model = TinyQuickDrawStudentClassifier().to(device)\n",
        "  model.requires_grad_(True)\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                          lr = learning_rate, \n",
        "                          momentum = momentum, \n",
        "                          weight_decay = weight_decay, \n",
        "                          dampening= dampening)\n",
        "  # Loss Func\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # Dataloaders\n",
        "  # Created a bigger training dataset for better model training\n",
        "  train_data_dir = \"tiny_quick_draw/bigger_train\"\n",
        "  train_data = ImageFolder(train_data_dir, transform=Compose([ToTensor()]))\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "  example_train_accs = []\n",
        "  example_test_accs = []\n",
        "  for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      training_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "      testing_acc, _ = test_loop(test_dataloader, model, loss_fn)\n",
        "      example_train_accs.append(training_acc)\n",
        "      example_test_accs.append(testing_acc)\n",
        "  print(\"Done!\")\n",
        "\n",
        "  epoch_list = [i for i in range(epochs)]\n",
        "  plt.plot(epoch_list, example_train_accs, color ='tab:green', label='training accuracy')\n",
        "  plt.plot(epoch_list, example_test_accs, color ='tab:blue', label='testing accuracy')\n",
        "  plt.legend()"
      ],
      "id": "Lebi_x0P67w0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAQ3JpiaNMvg"
      },
      "source": [
        "# Varying Hyperparameters"
      ],
      "id": "SAQ3JpiaNMvg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Model of all the variations tried."
      ],
      "metadata": {
        "id": "hto5wKBF_NI_"
      },
      "id": "hto5wKBF_NI_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19028
        },
        "id": "LHpXVKhJytt4",
        "outputId": "f1d8ea27-2b42-45bb-e8fb-ce841018f829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994054  [    0/ 4000]\n",
            "Training Accuracy: 6.9%\n",
            "Testing loop: \n",
            " Accuracy: 8.6%, Avg loss: 2.972969 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.970397  [    0/ 4000]\n",
            "Training Accuracy: 16.4%\n",
            "Testing loop: \n",
            " Accuracy: 22.0%, Avg loss: 2.924062 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.910456  [    0/ 4000]\n",
            "Training Accuracy: 23.1%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.729979 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.720883  [    0/ 4000]\n",
            "Training Accuracy: 25.4%\n",
            "Testing loop: \n",
            " Accuracy: 24.4%, Avg loss: 2.461624 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.331844  [    0/ 4000]\n",
            "Training Accuracy: 30.0%\n",
            "Testing loop: \n",
            " Accuracy: 34.4%, Avg loss: 2.123321 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.150594  [    0/ 4000]\n",
            "Training Accuracy: 36.1%\n",
            "Testing loop: \n",
            " Accuracy: 37.4%, Avg loss: 1.949421 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 1.975732  [    0/ 4000]\n",
            "Training Accuracy: 40.2%\n",
            "Testing loop: \n",
            " Accuracy: 42.2%, Avg loss: 1.870122 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 1.715003  [    0/ 4000]\n",
            "Training Accuracy: 43.6%\n",
            "Testing loop: \n",
            " Accuracy: 45.0%, Avg loss: 1.773376 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 1.501829  [    0/ 4000]\n",
            "Training Accuracy: 45.9%\n",
            "Testing loop: \n",
            " Accuracy: 48.2%, Avg loss: 1.737669 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 1.807002  [    0/ 4000]\n",
            "Training Accuracy: 49.1%\n",
            "Testing loop: \n",
            " Accuracy: 42.6%, Avg loss: 1.950519 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 1.531920  [    0/ 4000]\n",
            "Training Accuracy: 50.1%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.854571 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 1.986567  [    0/ 4000]\n",
            "Training Accuracy: 51.4%\n",
            "Testing loop: \n",
            " Accuracy: 37.0%, Avg loss: 2.258945 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 1.809715  [    0/ 4000]\n",
            "Training Accuracy: 54.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.538683 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 1.203699  [    0/ 4000]\n",
            "Training Accuracy: 54.1%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.667628 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 1.535734  [    0/ 4000]\n",
            "Training Accuracy: 57.3%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.545320 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 1.375015  [    0/ 4000]\n",
            "Training Accuracy: 57.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.560891 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 1.398507  [    0/ 4000]\n",
            "Training Accuracy: 57.5%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.654847 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 1.157125  [    0/ 4000]\n",
            "Training Accuracy: 59.8%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.556982 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 1.428728  [    0/ 4000]\n",
            "Training Accuracy: 60.1%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.485072 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 1.207479  [    0/ 4000]\n",
            "Training Accuracy: 60.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.435245 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 1.325164  [    0/ 4000]\n",
            "Training Accuracy: 61.9%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.544705 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 1.704166  [    0/ 4000]\n",
            "Training Accuracy: 61.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.444815 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 1.283626  [    0/ 4000]\n",
            "Training Accuracy: 63.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.394400 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 1.155830  [    0/ 4000]\n",
            "Training Accuracy: 63.1%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.439268 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 1.067107  [    0/ 4000]\n",
            "Training Accuracy: 64.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.399727 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 1.196687  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.502196 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 1.294477  [    0/ 4000]\n",
            "Training Accuracy: 65.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.458111 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 0.890817  [    0/ 4000]\n",
            "Training Accuracy: 66.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.477478 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 1.019877  [    0/ 4000]\n",
            "Training Accuracy: 66.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.399001 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 1.296209  [    0/ 4000]\n",
            "Training Accuracy: 66.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.375246 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 1.090364  [    0/ 4000]\n",
            "Training Accuracy: 66.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.409725 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 0.984850  [    0/ 4000]\n",
            "Training Accuracy: 68.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.372084 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 0.947744  [    0/ 4000]\n",
            "Training Accuracy: 68.3%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.449426 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 1.066015  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.386735 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 0.885180  [    0/ 4000]\n",
            "Training Accuracy: 69.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.424721 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 0.866778  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.442201 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 1.118280  [    0/ 4000]\n",
            "Training Accuracy: 71.5%\n",
            "Testing loop: \n",
            " Accuracy: 63.4%, Avg loss: 1.342683 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 0.972163  [    0/ 4000]\n",
            "Training Accuracy: 70.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.388314 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 0.896053  [    0/ 4000]\n",
            "Training Accuracy: 72.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.554516 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 0.817821  [    0/ 4000]\n",
            "Training Accuracy: 71.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.390441 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 0.676885  [    0/ 4000]\n",
            "Training Accuracy: 73.0%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.341922 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 0.795482  [    0/ 4000]\n",
            "Training Accuracy: 72.0%\n",
            "Testing loop: \n",
            " Accuracy: 62.0%, Avg loss: 1.365415 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 0.802049  [    0/ 4000]\n",
            "Training Accuracy: 73.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.435005 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 0.903218  [    0/ 4000]\n",
            "Training Accuracy: 72.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.501744 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 0.847400  [    0/ 4000]\n",
            "Training Accuracy: 75.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.380709 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 0.781570  [    0/ 4000]\n",
            "Training Accuracy: 74.7%\n",
            "Testing loop: \n",
            " Accuracy: 61.8%, Avg loss: 1.385622 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 0.545003  [    0/ 4000]\n",
            "Training Accuracy: 74.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.452886 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 0.758224  [    0/ 4000]\n",
            "Training Accuracy: 75.9%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.382869 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 0.744330  [    0/ 4000]\n",
            "Training Accuracy: 75.3%\n",
            "Testing loop: \n",
            " Accuracy: 62.2%, Avg loss: 1.361297 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 0.673812  [    0/ 4000]\n",
            "Training Accuracy: 78.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.445259 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 0.856776  [    0/ 4000]\n",
            "Training Accuracy: 76.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.397670 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 0.635262  [    0/ 4000]\n",
            "Training Accuracy: 78.5%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 1.402975 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 0.670346  [    0/ 4000]\n",
            "Training Accuracy: 78.0%\n",
            "Testing loop: \n",
            " Accuracy: 61.0%, Avg loss: 1.461618 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 0.761552  [    0/ 4000]\n",
            "Training Accuracy: 77.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.498413 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 0.494636  [    0/ 4000]\n",
            "Training Accuracy: 79.5%\n",
            "Testing loop: \n",
            " Accuracy: 62.2%, Avg loss: 1.395337 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 0.579528  [    0/ 4000]\n",
            "Training Accuracy: 80.2%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.474939 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 0.532430  [    0/ 4000]\n",
            "Training Accuracy: 80.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.487016 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 0.516177  [    0/ 4000]\n",
            "Training Accuracy: 79.0%\n",
            "Testing loop: \n",
            " Accuracy: 62.0%, Avg loss: 1.507122 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 0.415482  [    0/ 4000]\n",
            "Training Accuracy: 78.6%\n",
            "Testing loop: \n",
            " Accuracy: 62.4%, Avg loss: 1.428782 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 0.745988  [    0/ 4000]\n",
            "Training Accuracy: 80.2%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 1.473305 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 0.536042  [    0/ 4000]\n",
            "Training Accuracy: 83.3%\n",
            "Testing loop: \n",
            " Accuracy: 62.8%, Avg loss: 1.568989 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 0.560308  [    0/ 4000]\n",
            "Training Accuracy: 83.9%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.482713 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 0.356800  [    0/ 4000]\n",
            "Training Accuracy: 81.6%\n",
            "Testing loop: \n",
            " Accuracy: 62.4%, Avg loss: 1.488104 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 0.424813  [    0/ 4000]\n",
            "Training Accuracy: 84.7%\n",
            "Testing loop: \n",
            " Accuracy: 61.8%, Avg loss: 1.502537 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 0.435685  [    0/ 4000]\n",
            "Training Accuracy: 84.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.625130 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 0.458166  [    0/ 4000]\n",
            "Training Accuracy: 83.2%\n",
            "Testing loop: \n",
            " Accuracy: 62.0%, Avg loss: 1.468147 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 0.480253  [    0/ 4000]\n",
            "Training Accuracy: 85.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.535598 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 0.366546  [    0/ 4000]\n",
            "Training Accuracy: 86.6%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.560741 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 0.419848  [    0/ 4000]\n",
            "Training Accuracy: 86.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.684024 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 0.378203  [    0/ 4000]\n",
            "Training Accuracy: 85.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.610005 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 0.392488  [    0/ 4000]\n",
            "Training Accuracy: 88.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.0%, Avg loss: 1.666412 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 0.332465  [    0/ 4000]\n",
            "Training Accuracy: 88.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.850801 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 0.481257  [    0/ 4000]\n",
            "Training Accuracy: 88.2%\n",
            "Testing loop: \n",
            " Accuracy: 62.8%, Avg loss: 1.690573 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 0.263413  [    0/ 4000]\n",
            "Training Accuracy: 88.1%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.687350 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 0.267400  [    0/ 4000]\n",
            "Training Accuracy: 87.8%\n",
            "Testing loop: \n",
            " Accuracy: 63.4%, Avg loss: 1.635865 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 0.359020  [    0/ 4000]\n",
            "Training Accuracy: 90.0%\n",
            "Testing loop: \n",
            " Accuracy: 63.2%, Avg loss: 1.809986 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 0.532903  [    0/ 4000]\n",
            "Training Accuracy: 88.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.809125 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 0.235226  [    0/ 4000]\n",
            "Training Accuracy: 90.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.761985 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 0.254104  [    0/ 4000]\n",
            "Training Accuracy: 91.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.2%, Avg loss: 1.683858 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 0.250160  [    0/ 4000]\n",
            "Training Accuracy: 92.2%\n",
            "Testing loop: \n",
            " Accuracy: 62.2%, Avg loss: 1.816731 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 0.304931  [    0/ 4000]\n",
            "Training Accuracy: 91.7%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.747449 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 0.211899  [    0/ 4000]\n",
            "Training Accuracy: 93.5%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 1.776002 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 0.253920  [    0/ 4000]\n",
            "Training Accuracy: 92.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.845838 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 0.316586  [    0/ 4000]\n",
            "Training Accuracy: 91.8%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 1.806668 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 0.234241  [    0/ 4000]\n",
            "Training Accuracy: 91.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 2.065977 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 0.348428  [    0/ 4000]\n",
            "Training Accuracy: 88.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.966273 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 0.274713  [    0/ 4000]\n",
            "Training Accuracy: 93.1%\n",
            "Testing loop: \n",
            " Accuracy: 62.8%, Avg loss: 1.852318 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 0.219909  [    0/ 4000]\n",
            "Training Accuracy: 94.5%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.802583 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 0.134990  [    0/ 4000]\n",
            "Training Accuracy: 90.7%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 1.793359 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 0.287996  [    0/ 4000]\n",
            "Training Accuracy: 92.1%\n",
            "Testing loop: \n",
            " Accuracy: 63.0%, Avg loss: 1.800469 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 0.225217  [    0/ 4000]\n",
            "Training Accuracy: 94.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.968160 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 0.152830  [    0/ 4000]\n",
            "Training Accuracy: 95.0%\n",
            "Testing loop: \n",
            " Accuracy: 62.8%, Avg loss: 1.842353 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 0.149426  [    0/ 4000]\n",
            "Training Accuracy: 96.5%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.991646 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 0.103073  [    0/ 4000]\n",
            "Training Accuracy: 97.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 2.235436 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 0.221194  [    0/ 4000]\n",
            "Training Accuracy: 74.4%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.827887 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 0.484255  [    0/ 4000]\n",
            "Training Accuracy: 86.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.858658 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 0.259064  [    0/ 4000]\n",
            "Training Accuracy: 93.5%\n",
            "Testing loop: \n",
            " Accuracy: 63.2%, Avg loss: 1.923233 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 0.252367  [    0/ 4000]\n",
            "Training Accuracy: 94.7%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 1.886333 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 0.146341  [    0/ 4000]\n",
            "Training Accuracy: 94.7%\n",
            "Testing loop: \n",
            " Accuracy: 63.6%, Avg loss: 1.756151 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 0.074604  [    0/ 4000]\n",
            "Training Accuracy: 96.3%\n",
            "Testing loop: \n",
            " Accuracy: 64.0%, Avg loss: 1.894781 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 0.147376  [    0/ 4000]\n",
            "Training Accuracy: 96.2%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 1.818775 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 0.143003  [    0/ 4000]\n",
            "Training Accuracy: 77.2%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.761069 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 0.925144  [    0/ 4000]\n",
            "Training Accuracy: 75.1%\n",
            "Testing loop: \n",
            " Accuracy: 62.4%, Avg loss: 1.589924 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 0.512621  [    0/ 4000]\n",
            "Training Accuracy: 86.3%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 1.658406 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 0.366392  [    0/ 4000]\n",
            "Training Accuracy: 85.0%\n",
            "Testing loop: \n",
            " Accuracy: 63.4%, Avg loss: 1.591985 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 0.282770  [    0/ 4000]\n",
            "Training Accuracy: 93.7%\n",
            "Testing loop: \n",
            " Accuracy: 65.0%, Avg loss: 1.675073 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 0.141010  [    0/ 4000]\n",
            "Training Accuracy: 92.8%\n",
            "Testing loop: \n",
            " Accuracy: 63.4%, Avg loss: 1.718331 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 0.110045  [    0/ 4000]\n",
            "Training Accuracy: 93.5%\n",
            "Testing loop: \n",
            " Accuracy: 64.2%, Avg loss: 1.717845 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 0.108254  [    0/ 4000]\n",
            "Training Accuracy: 94.6%\n",
            "Testing loop: \n",
            " Accuracy: 62.6%, Avg loss: 1.857322 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 0.084363  [    0/ 4000]\n",
            "Training Accuracy: 95.7%\n",
            "Testing loop: \n",
            " Accuracy: 62.0%, Avg loss: 1.980840 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 0.256551  [    0/ 4000]\n",
            "Training Accuracy: 84.4%\n",
            "Testing loop: \n",
            " Accuracy: 66.0%, Avg loss: 1.759150 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 0.221110  [    0/ 4000]\n",
            "Training Accuracy: 96.3%\n",
            "Testing loop: \n",
            " Accuracy: 65.8%, Avg loss: 1.764246 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 0.058492  [    0/ 4000]\n",
            "Training Accuracy: 98.4%\n",
            "Testing loop: \n",
            " Accuracy: 65.8%, Avg loss: 1.909847 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 0.066795  [    0/ 4000]\n",
            "Training Accuracy: 98.7%\n",
            "Testing loop: \n",
            " Accuracy: 61.8%, Avg loss: 2.050735 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 0.123095  [    0/ 4000]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 64.6%, Avg loss: 1.949694 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 0.061317  [    0/ 4000]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 63.0%, Avg loss: 2.094518 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 0.089230  [    0/ 4000]\n",
            "Training Accuracy: 98.2%\n",
            "Testing loop: \n",
            " Accuracy: 63.8%, Avg loss: 1.991252 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 0.029264  [    0/ 4000]\n",
            "Training Accuracy: 98.4%\n",
            "Testing loop: \n",
            " Accuracy: 64.2%, Avg loss: 1.943905 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 0.042969  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 64.0%, Avg loss: 2.035096 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 0.041956  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 63.8%, Avg loss: 2.072906 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 0.035866  [    0/ 4000]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 2.033803 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 0.076144  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 65.6%, Avg loss: 2.076251 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 0.023970  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 63.8%, Avg loss: 2.078238 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 0.034722  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 65.0%, Avg loss: 2.093748 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 0.025692  [    0/ 4000]\n",
            "Training Accuracy: 99.7%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 2.147251 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 0.017763  [    0/ 4000]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 2.185228 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 0.029338  [    0/ 4000]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 66.0%, Avg loss: 2.250837 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 0.024453  [    0/ 4000]\n",
            "Training Accuracy: 99.7%\n",
            "Testing loop: \n",
            " Accuracy: 64.2%, Avg loss: 2.207429 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 0.021123  [    0/ 4000]\n",
            "Training Accuracy: 99.9%\n",
            "Testing loop: \n",
            " Accuracy: 65.2%, Avg loss: 2.216371 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 0.009221  [    0/ 4000]\n",
            "Training Accuracy: 99.9%\n",
            "Testing loop: \n",
            " Accuracy: 65.0%, Avg loss: 2.237192 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 0.019048  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.0%, Avg loss: 2.264147 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 0.008010  [    0/ 4000]\n",
            "Training Accuracy: 99.9%\n",
            "Testing loop: \n",
            " Accuracy: 63.2%, Avg loss: 2.273953 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 0.033299  [    0/ 4000]\n",
            "Training Accuracy: 99.8%\n",
            "Testing loop: \n",
            " Accuracy: 63.6%, Avg loss: 2.286861 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 0.014002  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.4%, Avg loss: 2.318813 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 0.012301  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.6%, Avg loss: 2.326122 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 0.014481  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.4%, Avg loss: 2.340645 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 0.010635  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.6%, Avg loss: 2.339881 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 0.009699  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 65.0%, Avg loss: 2.338851 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 0.006603  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 2.382130 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 0.011611  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 65.6%, Avg loss: 2.389337 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 0.010270  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 2.392769 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 0.008460  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.0%, Avg loss: 2.380331 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 0.008371  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.6%, Avg loss: 2.466777 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 0.015255  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 65.6%, Avg loss: 2.414607 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 0.006848  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.6%, Avg loss: 2.440067 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 0.007786  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 63.8%, Avg loss: 2.463539 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 0.006768  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.2%, Avg loss: 2.450124 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 0.006677  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 65.0%, Avg loss: 2.473602 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 0.005602  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 64.8%, Avg loss: 2.473226 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 0.006081  [    0/ 4000]\n",
            "Training Accuracy: 100.0%\n",
            "Testing loop: \n",
            " Accuracy: 65.2%, Avg loss: 2.490632 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "learning_rate = 0.03\n",
        "batch_size = 100\n",
        "epochs = 150\n",
        "momentum = 0.5\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening)"
      ],
      "id": "LHpXVKhJytt4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHCbpr2YNVgW"
      },
      "source": [
        "## Weight Decay Changes"
      ],
      "id": "VHCbpr2YNVgW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PTZyL_0Nfnu"
      },
      "source": [
        "### 0.5"
      ],
      "id": "2PTZyL_0Nfnu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37782
        },
        "id": "bgUe7IYt7Oe8",
        "outputId": "d32bcc52-9544-499c-fe2c-507cffc78eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995569  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.8%, Avg loss: 2.994744 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995121  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 4.4%, Avg loss: 2.993959 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994741  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.4%, Avg loss: 2.993494 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.991179  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.993294 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.991183  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.993253 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.991774  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.993315 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.993425  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.993439 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994069  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.993600 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994346  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 4.0%, Avg loss: 2.993787 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994840  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 4.6%, Avg loss: 2.993978 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994513  [    0/ 4000]\n",
            "Training Accuracy: 5.2%\n",
            "Testing loop: \n",
            " Accuracy: 4.8%, Avg loss: 2.994162 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.993324  [    0/ 4000]\n",
            "Training Accuracy: 5.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.994339 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994974  [    0/ 4000]\n",
            "Training Accuracy: 5.4%\n",
            "Testing loop: \n",
            " Accuracy: 4.8%, Avg loss: 2.994509 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994837  [    0/ 4000]\n",
            "Training Accuracy: 5.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.994671 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995604  [    0/ 4000]\n",
            "Training Accuracy: 5.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.2%, Avg loss: 2.994816 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.993452  [    0/ 4000]\n",
            "Training Accuracy: 5.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.2%, Avg loss: 2.994949 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996700  [    0/ 4000]\n",
            "Training Accuracy: 5.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.2%, Avg loss: 2.995067 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995821  [    0/ 4000]\n",
            "Training Accuracy: 5.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.4%, Avg loss: 2.995168 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994315  [    0/ 4000]\n",
            "Training Accuracy: 5.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.4%, Avg loss: 2.995257 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996999  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.8%, Avg loss: 2.995335 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994482  [    0/ 4000]\n",
            "Training Accuracy: 5.5%\n",
            "Testing loop: \n",
            " Accuracy: 6.0%, Avg loss: 2.995402 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995620  [    0/ 4000]\n",
            "Training Accuracy: 5.6%\n",
            "Testing loop: \n",
            " Accuracy: 6.0%, Avg loss: 2.995460 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994701  [    0/ 4000]\n",
            "Training Accuracy: 5.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.8%, Avg loss: 2.995510 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995462  [    0/ 4000]\n",
            "Training Accuracy: 5.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.8%, Avg loss: 2.995551 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996407  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995587 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995288  [    0/ 4000]\n",
            "Training Accuracy: 5.5%\n",
            "Testing loop: \n",
            " Accuracy: 4.6%, Avg loss: 2.995617 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995847  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 4.4%, Avg loss: 2.995642 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994889  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 4.2%, Avg loss: 2.995662 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996036  [    0/ 4000]\n",
            "Training Accuracy: 5.7%\n",
            "Testing loop: \n",
            " Accuracy: 4.6%, Avg loss: 2.995678 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994889  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995691 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995772  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 4.8%, Avg loss: 2.995701 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995609  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995710 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996057  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995716 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996029  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995722 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995556  [    0/ 4000]\n",
            "Training Accuracy: 5.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995726 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995568  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995729 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995914  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995308  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995337  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995736 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996268  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995266  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995343  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995589  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995625  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995739 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995368  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995739 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995522  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995739 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995783  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995739 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996022  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995539  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 2.996114  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995841  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995570  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995738 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995836  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995735  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995248  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995789  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995775  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995737 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995805  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995736 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995876  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995736 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995652  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995735 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995746  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995736 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995672  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995735 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995870  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995735 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995652  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995735 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995576  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995843  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995735 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995657  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995717  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995821  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995846  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995787  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995682  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995753  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995837  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995846  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995837  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995667  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995699  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995734 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995772  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995712  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995615  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995713  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995747  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995698  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995711  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995695  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995632  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995741  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995780  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995772  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995701  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995715  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995702  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995686  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995720  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995729  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995722  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995782  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995758  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995742  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995740  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995752  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995739  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995741  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995736  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995729  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995742  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995718  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995714  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995719  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995745  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995737  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995724  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995747  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995749  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995728  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995728  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995724  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995742  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995735  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995729  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995738  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995736  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995727  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995735  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995737  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995726  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995727  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995729  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 5.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 3.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 3.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995731 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 3.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.5%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995734  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 3.9%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995730  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.0%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.8%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.6%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.3%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.7%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995732  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.2%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995733  [    0/ 4000]\n",
            "Training Accuracy: 4.4%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995732 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loop: loss: 2.995731  [    0/ 4000]\n",
            "Training Accuracy: 4.1%\n",
            "Testing loop: \n",
            " Accuracy: 5.0%, Avg loss: 2.995733 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 500\n",
        "epochs = 300\n",
        "momentum = 0\n",
        "weight_decay = 0.5\n",
        "dampening = 0\n",
        "getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening)"
      ],
      "id": "bgUe7IYt7Oe8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szlFdoNCP7za"
      },
      "source": [
        "### 0.01 and Batch size 100"
      ],
      "id": "szlFdoNCP7za"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37785
        },
        "id": "zkv_G1p7PuIy",
        "outputId": "ad8bc159-780a-4927-ea18-6149883a4aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.998088  [    0/ 4000]\n",
            "Training Accuracy: 5.7%\n",
            "Testing loop: \n",
            " Accuracy: 6.0%, Avg loss: 2.990498 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.991244  [    0/ 4000]\n",
            "Training Accuracy: 7.7%\n",
            "Testing loop: \n",
            " Accuracy: 8.6%, Avg loss: 2.985737 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.982430  [    0/ 4000]\n",
            "Training Accuracy: 9.3%\n",
            "Testing loop: \n",
            " Accuracy: 10.6%, Avg loss: 2.981137 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.977518  [    0/ 4000]\n",
            "Training Accuracy: 11.2%\n",
            "Testing loop: \n",
            " Accuracy: 15.8%, Avg loss: 2.976167 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.975222  [    0/ 4000]\n",
            "Training Accuracy: 16.4%\n",
            "Testing loop: \n",
            " Accuracy: 17.4%, Avg loss: 2.970873 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.971197  [    0/ 4000]\n",
            "Training Accuracy: 17.6%\n",
            "Testing loop: \n",
            " Accuracy: 20.2%, Avg loss: 2.965062 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.962487  [    0/ 4000]\n",
            "Training Accuracy: 20.4%\n",
            "Testing loop: \n",
            " Accuracy: 22.4%, Avg loss: 2.958480 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.956664  [    0/ 4000]\n",
            "Training Accuracy: 22.8%\n",
            "Testing loop: \n",
            " Accuracy: 25.4%, Avg loss: 2.951051 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.946617  [    0/ 4000]\n",
            "Training Accuracy: 26.9%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.942323 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.939415  [    0/ 4000]\n",
            "Training Accuracy: 29.4%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.931750 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.931441  [    0/ 4000]\n",
            "Training Accuracy: 30.0%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.918664 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.908461  [    0/ 4000]\n",
            "Training Accuracy: 30.8%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.902810 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.892784  [    0/ 4000]\n",
            "Training Accuracy: 32.2%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.883434 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.882355  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 33.4%, Avg loss: 2.857437 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.861608  [    0/ 4000]\n",
            "Training Accuracy: 32.3%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.825181 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.816100  [    0/ 4000]\n",
            "Training Accuracy: 31.9%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.783779 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.770061  [    0/ 4000]\n",
            "Training Accuracy: 32.5%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.732236 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.744790  [    0/ 4000]\n",
            "Training Accuracy: 30.1%\n",
            "Testing loop: \n",
            " Accuracy: 32.6%, Avg loss: 2.670334 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.661640  [    0/ 4000]\n",
            "Training Accuracy: 30.9%\n",
            "Testing loop: \n",
            " Accuracy: 31.6%, Avg loss: 2.602064 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 2.627704  [    0/ 4000]\n",
            "Training Accuracy: 30.7%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.536068 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 2.530257  [    0/ 4000]\n",
            "Training Accuracy: 32.1%\n",
            "Testing loop: \n",
            " Accuracy: 29.8%, Avg loss: 2.473577 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 2.437020  [    0/ 4000]\n",
            "Training Accuracy: 30.2%\n",
            "Testing loop: \n",
            " Accuracy: 33.4%, Avg loss: 2.423996 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 2.492660  [    0/ 4000]\n",
            "Training Accuracy: 34.5%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.364927 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 2.406756  [    0/ 4000]\n",
            "Training Accuracy: 35.0%\n",
            "Testing loop: \n",
            " Accuracy: 34.0%, Avg loss: 2.323133 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 2.227657  [    0/ 4000]\n",
            "Training Accuracy: 35.6%\n",
            "Testing loop: \n",
            " Accuracy: 33.8%, Avg loss: 2.324063 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 2.247238  [    0/ 4000]\n",
            "Training Accuracy: 37.6%\n",
            "Testing loop: \n",
            " Accuracy: 34.4%, Avg loss: 2.246970 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 2.206464  [    0/ 4000]\n",
            "Training Accuracy: 37.3%\n",
            "Testing loop: \n",
            " Accuracy: 33.0%, Avg loss: 2.242660 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 2.159231  [    0/ 4000]\n",
            "Training Accuracy: 37.8%\n",
            "Testing loop: \n",
            " Accuracy: 37.8%, Avg loss: 2.197634 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 2.202051  [    0/ 4000]\n",
            "Training Accuracy: 40.4%\n",
            "Testing loop: \n",
            " Accuracy: 36.6%, Avg loss: 2.208701 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 2.216088  [    0/ 4000]\n",
            "Training Accuracy: 41.4%\n",
            "Testing loop: \n",
            " Accuracy: 40.6%, Avg loss: 2.152294 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 2.208086  [    0/ 4000]\n",
            "Training Accuracy: 41.1%\n",
            "Testing loop: \n",
            " Accuracy: 42.0%, Avg loss: 2.117567 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 2.027662  [    0/ 4000]\n",
            "Training Accuracy: 42.9%\n",
            "Testing loop: \n",
            " Accuracy: 39.6%, Avg loss: 2.095863 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 1.883760  [    0/ 4000]\n",
            "Training Accuracy: 43.0%\n",
            "Testing loop: \n",
            " Accuracy: 38.2%, Avg loss: 2.080435 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 1.994534  [    0/ 4000]\n",
            "Training Accuracy: 45.0%\n",
            "Testing loop: \n",
            " Accuracy: 43.2%, Avg loss: 1.986745 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 2.065148  [    0/ 4000]\n",
            "Training Accuracy: 45.9%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 1.952841 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 1.891453  [    0/ 4000]\n",
            "Training Accuracy: 45.4%\n",
            "Testing loop: \n",
            " Accuracy: 44.4%, Avg loss: 1.913469 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 1.855972  [    0/ 4000]\n",
            "Training Accuracy: 47.6%\n",
            "Testing loop: \n",
            " Accuracy: 45.8%, Avg loss: 1.900844 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 1.754290  [    0/ 4000]\n",
            "Training Accuracy: 47.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 1.872186 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 1.964787  [    0/ 4000]\n",
            "Training Accuracy: 48.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.845581 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 1.620454  [    0/ 4000]\n",
            "Training Accuracy: 48.5%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.812410 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 1.911079  [    0/ 4000]\n",
            "Training Accuracy: 49.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.788694 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 1.869403  [    0/ 4000]\n",
            "Training Accuracy: 49.3%\n",
            "Testing loop: \n",
            " Accuracy: 46.0%, Avg loss: 1.790888 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 1.766043  [    0/ 4000]\n",
            "Training Accuracy: 48.8%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.774473 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 1.886609  [    0/ 4000]\n",
            "Training Accuracy: 49.9%\n",
            "Testing loop: \n",
            " Accuracy: 44.2%, Avg loss: 1.915683 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 1.904853  [    0/ 4000]\n",
            "Training Accuracy: 50.3%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.806839 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 1.611814  [    0/ 4000]\n",
            "Training Accuracy: 49.3%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.773314 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 1.685307  [    0/ 4000]\n",
            "Training Accuracy: 50.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.2%, Avg loss: 1.776496 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 1.561015  [    0/ 4000]\n",
            "Training Accuracy: 51.1%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.719398 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 1.610548  [    0/ 4000]\n",
            "Training Accuracy: 51.1%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.747223 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 1.521013  [    0/ 4000]\n",
            "Training Accuracy: 50.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.688379 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 1.558147  [    0/ 4000]\n",
            "Training Accuracy: 52.7%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 1.759186 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 1.721848  [    0/ 4000]\n",
            "Training Accuracy: 52.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.758906 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 1.703302  [    0/ 4000]\n",
            "Training Accuracy: 52.5%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.702293 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 1.646199  [    0/ 4000]\n",
            "Training Accuracy: 52.6%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.703150 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 1.579247  [    0/ 4000]\n",
            "Training Accuracy: 51.1%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.720075 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 1.621199  [    0/ 4000]\n",
            "Training Accuracy: 52.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.673847 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 1.499445  [    0/ 4000]\n",
            "Training Accuracy: 52.1%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.665325 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 1.722682  [    0/ 4000]\n",
            "Training Accuracy: 52.5%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.737714 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 1.594517  [    0/ 4000]\n",
            "Training Accuracy: 53.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.635221 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 1.733901  [    0/ 4000]\n",
            "Training Accuracy: 54.2%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.636613 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 1.568149  [    0/ 4000]\n",
            "Training Accuracy: 53.7%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.751661 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 1.448827  [    0/ 4000]\n",
            "Training Accuracy: 53.6%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.670954 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 1.638303  [    0/ 4000]\n",
            "Training Accuracy: 53.9%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.680785 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 1.520857  [    0/ 4000]\n",
            "Training Accuracy: 54.3%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.697887 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 1.776141  [    0/ 4000]\n",
            "Training Accuracy: 54.8%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.643844 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 1.734374  [    0/ 4000]\n",
            "Training Accuracy: 54.1%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.664524 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 1.571741  [    0/ 4000]\n",
            "Training Accuracy: 54.9%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.689553 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 1.459620  [    0/ 4000]\n",
            "Training Accuracy: 55.7%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.653984 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 1.444106  [    0/ 4000]\n",
            "Training Accuracy: 55.5%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.617449 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 1.345238  [    0/ 4000]\n",
            "Training Accuracy: 56.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.629482 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 1.342197  [    0/ 4000]\n",
            "Training Accuracy: 55.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.594552 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 1.532833  [    0/ 4000]\n",
            "Training Accuracy: 55.8%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.625496 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 1.336553  [    0/ 4000]\n",
            "Training Accuracy: 56.4%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.608233 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 1.401204  [    0/ 4000]\n",
            "Training Accuracy: 56.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.4%, Avg loss: 1.623931 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 1.419580  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.569727 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 1.537159  [    0/ 4000]\n",
            "Training Accuracy: 56.6%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.609412 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 1.387984  [    0/ 4000]\n",
            "Training Accuracy: 57.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.8%, Avg loss: 1.596175 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 1.308891  [    0/ 4000]\n",
            "Training Accuracy: 56.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.577896 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 1.347802  [    0/ 4000]\n",
            "Training Accuracy: 56.9%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.586164 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 1.394196  [    0/ 4000]\n",
            "Training Accuracy: 57.3%\n",
            "Testing loop: \n",
            " Accuracy: 52.4%, Avg loss: 1.613108 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 1.338341  [    0/ 4000]\n",
            "Training Accuracy: 57.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.627337 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 1.459111  [    0/ 4000]\n",
            "Training Accuracy: 57.7%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.579013 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 1.378142  [    0/ 4000]\n",
            "Training Accuracy: 58.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.8%, Avg loss: 1.597736 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 1.492755  [    0/ 4000]\n",
            "Training Accuracy: 57.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.544228 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 1.309149  [    0/ 4000]\n",
            "Training Accuracy: 58.2%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.555760 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 1.272857  [    0/ 4000]\n",
            "Training Accuracy: 58.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.596477 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 1.409889  [    0/ 4000]\n",
            "Training Accuracy: 57.9%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.608951 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 1.327963  [    0/ 4000]\n",
            "Training Accuracy: 59.4%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.553331 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 1.341318  [    0/ 4000]\n",
            "Training Accuracy: 58.9%\n",
            "Testing loop: \n",
            " Accuracy: 52.8%, Avg loss: 1.613553 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 1.212318  [    0/ 4000]\n",
            "Training Accuracy: 58.4%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.559884 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 1.333476  [    0/ 4000]\n",
            "Training Accuracy: 59.0%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.550544 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 1.524925  [    0/ 4000]\n",
            "Training Accuracy: 58.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.597948 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 1.383454  [    0/ 4000]\n",
            "Training Accuracy: 59.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.527232 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 1.350775  [    0/ 4000]\n",
            "Training Accuracy: 60.0%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.577343 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 1.397926  [    0/ 4000]\n",
            "Training Accuracy: 59.9%\n",
            "Testing loop: \n",
            " Accuracy: 52.8%, Avg loss: 1.565004 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 1.242158  [    0/ 4000]\n",
            "Training Accuracy: 59.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.560674 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 1.482065  [    0/ 4000]\n",
            "Training Accuracy: 58.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.532942 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 1.151477  [    0/ 4000]\n",
            "Training Accuracy: 60.3%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.554946 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 1.421759  [    0/ 4000]\n",
            "Training Accuracy: 60.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.511948 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 1.529946  [    0/ 4000]\n",
            "Training Accuracy: 60.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.557954 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 1.497587  [    0/ 4000]\n",
            "Training Accuracy: 60.7%\n",
            "Testing loop: \n",
            " Accuracy: 52.2%, Avg loss: 1.553209 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 1.455311  [    0/ 4000]\n",
            "Training Accuracy: 61.4%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.531206 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 1.327070  [    0/ 4000]\n",
            "Training Accuracy: 60.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.552648 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 1.348905  [    0/ 4000]\n",
            "Training Accuracy: 60.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.529162 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 1.387879  [    0/ 4000]\n",
            "Training Accuracy: 61.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.514281 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 1.279019  [    0/ 4000]\n",
            "Training Accuracy: 60.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.547502 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 1.219012  [    0/ 4000]\n",
            "Training Accuracy: 61.7%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.506543 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 1.277890  [    0/ 4000]\n",
            "Training Accuracy: 61.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.502641 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 1.224555  [    0/ 4000]\n",
            "Training Accuracy: 61.5%\n",
            "Testing loop: \n",
            " Accuracy: 52.8%, Avg loss: 1.571226 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 1.433197  [    0/ 4000]\n",
            "Training Accuracy: 61.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.524343 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 1.240674  [    0/ 4000]\n",
            "Training Accuracy: 62.1%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.531370 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 1.341024  [    0/ 4000]\n",
            "Training Accuracy: 61.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.574292 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 1.295842  [    0/ 4000]\n",
            "Training Accuracy: 61.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.557549 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 1.333235  [    0/ 4000]\n",
            "Training Accuracy: 62.6%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.502768 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 1.115322  [    0/ 4000]\n",
            "Training Accuracy: 61.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.494440 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 1.392380  [    0/ 4000]\n",
            "Training Accuracy: 62.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.527845 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 1.448194  [    0/ 4000]\n",
            "Training Accuracy: 62.3%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.503362 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 1.198851  [    0/ 4000]\n",
            "Training Accuracy: 62.2%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.500811 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 1.178749  [    0/ 4000]\n",
            "Training Accuracy: 61.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.559007 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 1.552895  [    0/ 4000]\n",
            "Training Accuracy: 62.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.508223 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 1.226982  [    0/ 4000]\n",
            "Training Accuracy: 62.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.480011 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 1.297661  [    0/ 4000]\n",
            "Training Accuracy: 62.8%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.528725 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 1.338631  [    0/ 4000]\n",
            "Training Accuracy: 62.5%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.521306 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 1.257038  [    0/ 4000]\n",
            "Training Accuracy: 63.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.505453 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 1.135052  [    0/ 4000]\n",
            "Training Accuracy: 63.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.523136 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 1.153480  [    0/ 4000]\n",
            "Training Accuracy: 63.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.467569 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 1.596274  [    0/ 4000]\n",
            "Training Accuracy: 63.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.459401 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 1.044833  [    0/ 4000]\n",
            "Training Accuracy: 63.7%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.598645 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 1.412494  [    0/ 4000]\n",
            "Training Accuracy: 62.6%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.557949 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 1.070356  [    0/ 4000]\n",
            "Training Accuracy: 62.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.468875 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 1.197183  [    0/ 4000]\n",
            "Training Accuracy: 64.1%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.494507 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 1.290569  [    0/ 4000]\n",
            "Training Accuracy: 63.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.479333 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 1.094471  [    0/ 4000]\n",
            "Training Accuracy: 63.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.496096 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 1.319045  [    0/ 4000]\n",
            "Training Accuracy: 63.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.482813 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 1.214109  [    0/ 4000]\n",
            "Training Accuracy: 63.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.470080 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 1.068604  [    0/ 4000]\n",
            "Training Accuracy: 64.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.476205 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 1.148831  [    0/ 4000]\n",
            "Training Accuracy: 63.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.505063 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 1.129802  [    0/ 4000]\n",
            "Training Accuracy: 63.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.470560 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 1.013315  [    0/ 4000]\n",
            "Training Accuracy: 63.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.471087 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 1.144176  [    0/ 4000]\n",
            "Training Accuracy: 63.8%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.481264 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 1.197652  [    0/ 4000]\n",
            "Training Accuracy: 64.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.457723 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 1.168456  [    0/ 4000]\n",
            "Training Accuracy: 64.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.485500 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 1.342486  [    0/ 4000]\n",
            "Training Accuracy: 64.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.500853 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 1.211017  [    0/ 4000]\n",
            "Training Accuracy: 64.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.447275 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 1.012690  [    0/ 4000]\n",
            "Training Accuracy: 64.3%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.501281 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 1.257036  [    0/ 4000]\n",
            "Training Accuracy: 63.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.499371 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 1.228858  [    0/ 4000]\n",
            "Training Accuracy: 64.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.452617 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 1.107396  [    0/ 4000]\n",
            "Training Accuracy: 64.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.481258 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 1.210685  [    0/ 4000]\n",
            "Training Accuracy: 64.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.453382 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 1.064663  [    0/ 4000]\n",
            "Training Accuracy: 64.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.537398 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loop: loss: 1.137531  [    0/ 4000]\n",
            "Training Accuracy: 64.6%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.502606 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loop: loss: 1.580594  [    0/ 4000]\n",
            "Training Accuracy: 64.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.448908 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loop: loss: 1.117481  [    0/ 4000]\n",
            "Training Accuracy: 64.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.469950 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loop: loss: 0.981954  [    0/ 4000]\n",
            "Training Accuracy: 64.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.465961 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loop: loss: 1.164660  [    0/ 4000]\n",
            "Training Accuracy: 64.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.475940 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loop: loss: 1.156700  [    0/ 4000]\n",
            "Training Accuracy: 66.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.508775 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loop: loss: 0.961958  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.469432 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loop: loss: 1.249816  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.469339 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loop: loss: 1.166601  [    0/ 4000]\n",
            "Training Accuracy: 64.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.442779 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loop: loss: 1.141258  [    0/ 4000]\n",
            "Training Accuracy: 64.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.439851 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loop: loss: 1.051093  [    0/ 4000]\n",
            "Training Accuracy: 65.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.456828 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loop: loss: 0.935639  [    0/ 4000]\n",
            "Training Accuracy: 65.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.506223 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loop: loss: 0.990342  [    0/ 4000]\n",
            "Training Accuracy: 65.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.431321 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loop: loss: 1.172548  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.442004 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loop: loss: 1.214588  [    0/ 4000]\n",
            "Training Accuracy: 66.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.462770 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loop: loss: 1.147305  [    0/ 4000]\n",
            "Training Accuracy: 65.6%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.465107 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loop: loss: 0.957061  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.515498 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loop: loss: 1.180202  [    0/ 4000]\n",
            "Training Accuracy: 65.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.513599 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loop: loss: 1.097927  [    0/ 4000]\n",
            "Training Accuracy: 65.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.442346 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loop: loss: 1.059245  [    0/ 4000]\n",
            "Training Accuracy: 66.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.482186 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loop: loss: 1.356390  [    0/ 4000]\n",
            "Training Accuracy: 66.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.427607 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loop: loss: 1.133626  [    0/ 4000]\n",
            "Training Accuracy: 66.2%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.479064 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loop: loss: 1.227591  [    0/ 4000]\n",
            "Training Accuracy: 66.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.460965 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loop: loss: 1.204340  [    0/ 4000]\n",
            "Training Accuracy: 65.8%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.482666 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loop: loss: 1.276181  [    0/ 4000]\n",
            "Training Accuracy: 66.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.480916 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loop: loss: 1.196910  [    0/ 4000]\n",
            "Training Accuracy: 66.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.455862 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loop: loss: 1.167297  [    0/ 4000]\n",
            "Training Accuracy: 66.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.486559 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loop: loss: 1.266989  [    0/ 4000]\n",
            "Training Accuracy: 66.1%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.431423 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loop: loss: 0.847219  [    0/ 4000]\n",
            "Training Accuracy: 66.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.530243 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loop: loss: 1.197452  [    0/ 4000]\n",
            "Training Accuracy: 66.6%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.420235 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loop: loss: 1.082171  [    0/ 4000]\n",
            "Training Accuracy: 66.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.444483 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loop: loss: 0.985307  [    0/ 4000]\n",
            "Training Accuracy: 66.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.462172 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loop: loss: 0.923532  [    0/ 4000]\n",
            "Training Accuracy: 67.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.422523 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loop: loss: 1.057669  [    0/ 4000]\n",
            "Training Accuracy: 66.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.442799 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loop: loss: 0.969133  [    0/ 4000]\n",
            "Training Accuracy: 66.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.402387 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loop: loss: 1.089435  [    0/ 4000]\n",
            "Training Accuracy: 66.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.486356 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loop: loss: 1.223728  [    0/ 4000]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.440436 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loop: loss: 1.112289  [    0/ 4000]\n",
            "Training Accuracy: 66.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.412765 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loop: loss: 0.947878  [    0/ 4000]\n",
            "Training Accuracy: 67.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.461996 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loop: loss: 1.018093  [    0/ 4000]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.424055 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loop: loss: 1.102345  [    0/ 4000]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.404035 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loop: loss: 1.226372  [    0/ 4000]\n",
            "Training Accuracy: 66.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.411026 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loop: loss: 0.941343  [    0/ 4000]\n",
            "Training Accuracy: 67.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.508025 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loop: loss: 1.183567  [    0/ 4000]\n",
            "Training Accuracy: 66.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.506979 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loop: loss: 1.195530  [    0/ 4000]\n",
            "Training Accuracy: 67.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.437396 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loop: loss: 1.060063  [    0/ 4000]\n",
            "Training Accuracy: 67.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.434637 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loop: loss: 0.836796  [    0/ 4000]\n",
            "Training Accuracy: 67.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.448780 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loop: loss: 1.059998  [    0/ 4000]\n",
            "Training Accuracy: 67.8%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.447601 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loop: loss: 1.021608  [    0/ 4000]\n",
            "Training Accuracy: 67.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.434039 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loop: loss: 1.125783  [    0/ 4000]\n",
            "Training Accuracy: 67.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.438214 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loop: loss: 1.122485  [    0/ 4000]\n",
            "Training Accuracy: 67.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.404461 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loop: loss: 1.106869  [    0/ 4000]\n",
            "Training Accuracy: 67.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.510931 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loop: loss: 1.159753  [    0/ 4000]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.417795 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loop: loss: 0.948759  [    0/ 4000]\n",
            "Training Accuracy: 67.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.428054 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loop: loss: 0.985628  [    0/ 4000]\n",
            "Training Accuracy: 67.5%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.499341 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loop: loss: 1.342229  [    0/ 4000]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.447495 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loop: loss: 1.109501  [    0/ 4000]\n",
            "Training Accuracy: 67.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.442326 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loop: loss: 1.029651  [    0/ 4000]\n",
            "Training Accuracy: 67.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.420292 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loop: loss: 1.039509  [    0/ 4000]\n",
            "Training Accuracy: 67.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.431766 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loop: loss: 1.085159  [    0/ 4000]\n",
            "Training Accuracy: 67.9%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.439951 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loop: loss: 1.305204  [    0/ 4000]\n",
            "Training Accuracy: 67.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.501773 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loop: loss: 1.232119  [    0/ 4000]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.455664 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loop: loss: 1.241696  [    0/ 4000]\n",
            "Training Accuracy: 68.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.400012 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loop: loss: 0.970782  [    0/ 4000]\n",
            "Training Accuracy: 68.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.408935 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loop: loss: 1.012016  [    0/ 4000]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.428775 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loop: loss: 1.022020  [    0/ 4000]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.437882 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loop: loss: 0.910725  [    0/ 4000]\n",
            "Training Accuracy: 67.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.413878 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loop: loss: 1.226166  [    0/ 4000]\n",
            "Training Accuracy: 68.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.493318 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loop: loss: 1.144169  [    0/ 4000]\n",
            "Training Accuracy: 68.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.470116 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loop: loss: 1.086821  [    0/ 4000]\n",
            "Training Accuracy: 69.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.415274 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loop: loss: 1.082344  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.453247 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loop: loss: 0.980105  [    0/ 4000]\n",
            "Training Accuracy: 68.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.455640 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loop: loss: 1.059821  [    0/ 4000]\n",
            "Training Accuracy: 68.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.408218 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loop: loss: 1.088590  [    0/ 4000]\n",
            "Training Accuracy: 69.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.412356 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loop: loss: 0.978860  [    0/ 4000]\n",
            "Training Accuracy: 68.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.430831 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loop: loss: 0.940500  [    0/ 4000]\n",
            "Training Accuracy: 69.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.445950 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loop: loss: 1.023145  [    0/ 4000]\n",
            "Training Accuracy: 69.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.428696 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loop: loss: 1.075827  [    0/ 4000]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.412401 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loop: loss: 1.040741  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.432534 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loop: loss: 0.966611  [    0/ 4000]\n",
            "Training Accuracy: 68.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.392465 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loop: loss: 1.196742  [    0/ 4000]\n",
            "Training Accuracy: 68.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.504172 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loop: loss: 1.228098  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.414057 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loop: loss: 0.890468  [    0/ 4000]\n",
            "Training Accuracy: 69.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.438756 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loop: loss: 0.991700  [    0/ 4000]\n",
            "Training Accuracy: 69.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.440697 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loop: loss: 0.927318  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.449056 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loop: loss: 1.041979  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.403852 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loop: loss: 1.104000  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.476453 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loop: loss: 1.428863  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.465762 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loop: loss: 0.978954  [    0/ 4000]\n",
            "Training Accuracy: 69.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.429645 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loop: loss: 1.018166  [    0/ 4000]\n",
            "Training Accuracy: 68.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.445297 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loop: loss: 0.805236  [    0/ 4000]\n",
            "Training Accuracy: 69.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.443637 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loop: loss: 0.984582  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.428702 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loop: loss: 1.025144  [    0/ 4000]\n",
            "Training Accuracy: 68.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.461053 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loop: loss: 0.855546  [    0/ 4000]\n",
            "Training Accuracy: 68.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.432983 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loop: loss: 0.994093  [    0/ 4000]\n",
            "Training Accuracy: 70.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.418449 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loop: loss: 1.032463  [    0/ 4000]\n",
            "Training Accuracy: 70.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.415099 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loop: loss: 0.815517  [    0/ 4000]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.452586 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loop: loss: 0.888587  [    0/ 4000]\n",
            "Training Accuracy: 69.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.458907 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loop: loss: 1.015282  [    0/ 4000]\n",
            "Training Accuracy: 69.6%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.444924 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loop: loss: 1.073795  [    0/ 4000]\n",
            "Training Accuracy: 69.2%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.451554 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loop: loss: 1.223277  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.412126 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loop: loss: 0.994428  [    0/ 4000]\n",
            "Training Accuracy: 68.2%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.519559 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loop: loss: 1.178978  [    0/ 4000]\n",
            "Training Accuracy: 69.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.425039 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loop: loss: 1.345994  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.431048 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loop: loss: 1.031710  [    0/ 4000]\n",
            "Training Accuracy: 69.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.439492 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loop: loss: 1.093556  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.398358 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loop: loss: 0.898646  [    0/ 4000]\n",
            "Training Accuracy: 69.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.445302 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loop: loss: 1.117986  [    0/ 4000]\n",
            "Training Accuracy: 69.6%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.441374 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loop: loss: 0.937085  [    0/ 4000]\n",
            "Training Accuracy: 69.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.453011 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loop: loss: 1.242623  [    0/ 4000]\n",
            "Training Accuracy: 69.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.453896 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loop: loss: 0.942479  [    0/ 4000]\n",
            "Training Accuracy: 70.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.476007 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loop: loss: 1.170410  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.397605 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loop: loss: 0.987695  [    0/ 4000]\n",
            "Training Accuracy: 69.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.435844 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loop: loss: 1.070721  [    0/ 4000]\n",
            "Training Accuracy: 70.1%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.561400 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loop: loss: 1.023458  [    0/ 4000]\n",
            "Training Accuracy: 70.1%\n",
            "Testing loop: \n",
            " Accuracy: 62.0%, Avg loss: 1.412000 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loop: loss: 0.978954  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.387154 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loop: loss: 1.135926  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.503721 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loop: loss: 1.192880  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.469926 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loop: loss: 0.951438  [    0/ 4000]\n",
            "Training Accuracy: 70.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.394787 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loop: loss: 0.893288  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.421913 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loop: loss: 0.919900  [    0/ 4000]\n",
            "Training Accuracy: 71.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.411373 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loop: loss: 1.067352  [    0/ 4000]\n",
            "Training Accuracy: 70.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.407376 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loop: loss: 1.098686  [    0/ 4000]\n",
            "Training Accuracy: 70.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.424697 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loop: loss: 1.027696  [    0/ 4000]\n",
            "Training Accuracy: 71.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.446277 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loop: loss: 0.718935  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.435102 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loop: loss: 0.905893  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.430811 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loop: loss: 1.026983  [    0/ 4000]\n",
            "Training Accuracy: 70.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.464823 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loop: loss: 0.899341  [    0/ 4000]\n",
            "Training Accuracy: 70.6%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.481473 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loop: loss: 0.947816  [    0/ 4000]\n",
            "Training Accuracy: 70.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.419456 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loop: loss: 0.899128  [    0/ 4000]\n",
            "Training Accuracy: 70.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.394910 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loop: loss: 0.851125  [    0/ 4000]\n",
            "Training Accuracy: 69.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.416523 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loop: loss: 1.150868  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.433217 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loop: loss: 0.977382  [    0/ 4000]\n",
            "Training Accuracy: 71.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.422276 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loop: loss: 0.841285  [    0/ 4000]\n",
            "Training Accuracy: 71.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.426634 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loop: loss: 0.673339  [    0/ 4000]\n",
            "Training Accuracy: 71.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.398221 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loop: loss: 0.794889  [    0/ 4000]\n",
            "Training Accuracy: 70.9%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.468951 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loop: loss: 1.179069  [    0/ 4000]\n",
            "Training Accuracy: 71.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.393189 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loop: loss: 0.996786  [    0/ 4000]\n",
            "Training Accuracy: 71.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.395070 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loop: loss: 1.069672  [    0/ 4000]\n",
            "Training Accuracy: 71.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.412019 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loop: loss: 0.927379  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.432750 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loop: loss: 1.001901  [    0/ 4000]\n",
            "Training Accuracy: 71.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.419859 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loop: loss: 0.968766  [    0/ 4000]\n",
            "Training Accuracy: 71.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.446423 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loop: loss: 1.003054  [    0/ 4000]\n",
            "Training Accuracy: 70.9%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.432032 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loop: loss: 1.027916  [    0/ 4000]\n",
            "Training Accuracy: 70.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.393919 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loop: loss: 0.949233  [    0/ 4000]\n",
            "Training Accuracy: 71.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.423037 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loop: loss: 0.923146  [    0/ 4000]\n",
            "Training Accuracy: 70.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.536467 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loop: loss: 0.888426  [    0/ 4000]\n",
            "Training Accuracy: 71.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.464964 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loop: loss: 1.018946  [    0/ 4000]\n",
            "Training Accuracy: 72.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.416736 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loop: loss: 0.956617  [    0/ 4000]\n",
            "Training Accuracy: 71.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.411908 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loop: loss: 0.879531  [    0/ 4000]\n",
            "Training Accuracy: 71.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.407871 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "epochs = 300\n",
        "momentum = 0\n",
        "weight_decay = 0.01\n",
        "dampening = 0\n",
        "getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening)"
      ],
      "id": "zkv_G1p7PuIy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3RNcOCRdXO"
      },
      "source": [
        "### 0.5 and batch 50"
      ],
      "id": "PZ3RNcOCRdXO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19032
        },
        "id": "B0s1vsEdRhhQ",
        "outputId": "7095cfae-9329-48ae-8b64-3abde8ce1bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.990443  [    0/ 4000]\n",
            "Training Accuracy: 8.9%\n",
            "Testing loop: \n",
            " Accuracy: 9.4%, Avg loss: 2.974171 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.973581  [    0/ 4000]\n",
            "Training Accuracy: 9.4%\n",
            "Testing loop: \n",
            " Accuracy: 11.4%, Avg loss: 2.954972 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.948267  [    0/ 4000]\n",
            "Training Accuracy: 11.3%\n",
            "Testing loop: \n",
            " Accuracy: 11.0%, Avg loss: 2.748883 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.662848  [    0/ 4000]\n",
            "Training Accuracy: 11.9%\n",
            "Testing loop: \n",
            " Accuracy: 12.6%, Avg loss: 2.668898 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.615205  [    0/ 4000]\n",
            "Training Accuracy: 11.8%\n",
            "Testing loop: \n",
            " Accuracy: 10.8%, Avg loss: 2.660789 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.691820  [    0/ 4000]\n",
            "Training Accuracy: 14.7%\n",
            "Testing loop: \n",
            " Accuracy: 13.4%, Avg loss: 2.632962 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.697898  [    0/ 4000]\n",
            "Training Accuracy: 13.1%\n",
            "Testing loop: \n",
            " Accuracy: 10.8%, Avg loss: 2.717361 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.729250  [    0/ 4000]\n",
            "Training Accuracy: 12.8%\n",
            "Testing loop: \n",
            " Accuracy: 15.0%, Avg loss: 2.588013 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.579075  [    0/ 4000]\n",
            "Training Accuracy: 13.8%\n",
            "Testing loop: \n",
            " Accuracy: 15.6%, Avg loss: 2.586461 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.563957  [    0/ 4000]\n",
            "Training Accuracy: 14.1%\n",
            "Testing loop: \n",
            " Accuracy: 11.2%, Avg loss: 2.670571 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.594229  [    0/ 4000]\n",
            "Training Accuracy: 15.4%\n",
            "Testing loop: \n",
            " Accuracy: 18.8%, Avg loss: 2.560055 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.612295  [    0/ 4000]\n",
            "Training Accuracy: 14.8%\n",
            "Testing loop: \n",
            " Accuracy: 15.8%, Avg loss: 2.536986 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.449967  [    0/ 4000]\n",
            "Training Accuracy: 16.5%\n",
            "Testing loop: \n",
            " Accuracy: 16.2%, Avg loss: 2.511489 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.316057  [    0/ 4000]\n",
            "Training Accuracy: 14.4%\n",
            "Testing loop: \n",
            " Accuracy: 16.8%, Avg loss: 2.547852 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.425852  [    0/ 4000]\n",
            "Training Accuracy: 16.3%\n",
            "Testing loop: \n",
            " Accuracy: 17.8%, Avg loss: 2.508615 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.456490  [    0/ 4000]\n",
            "Training Accuracy: 16.4%\n",
            "Testing loop: \n",
            " Accuracy: 15.4%, Avg loss: 2.503157 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.669048  [    0/ 4000]\n",
            "Training Accuracy: 16.2%\n",
            "Testing loop: \n",
            " Accuracy: 17.8%, Avg loss: 2.489175 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.353230  [    0/ 4000]\n",
            "Training Accuracy: 15.3%\n",
            "Testing loop: \n",
            " Accuracy: 17.8%, Avg loss: 2.531146 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.506793  [    0/ 4000]\n",
            "Training Accuracy: 17.3%\n",
            "Testing loop: \n",
            " Accuracy: 16.0%, Avg loss: 2.513858 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 2.500489  [    0/ 4000]\n",
            "Training Accuracy: 17.6%\n",
            "Testing loop: \n",
            " Accuracy: 16.4%, Avg loss: 2.486097 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 2.552547  [    0/ 4000]\n",
            "Training Accuracy: 17.1%\n",
            "Testing loop: \n",
            " Accuracy: 17.4%, Avg loss: 2.457338 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 2.251684  [    0/ 4000]\n",
            "Training Accuracy: 16.8%\n",
            "Testing loop: \n",
            " Accuracy: 16.2%, Avg loss: 2.480836 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 2.496067  [    0/ 4000]\n",
            "Training Accuracy: 18.7%\n",
            "Testing loop: \n",
            " Accuracy: 22.2%, Avg loss: 2.379887 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 2.274458  [    0/ 4000]\n",
            "Training Accuracy: 21.9%\n",
            "Testing loop: \n",
            " Accuracy: 17.2%, Avg loss: 2.480924 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 2.323388  [    0/ 4000]\n",
            "Training Accuracy: 21.1%\n",
            "Testing loop: \n",
            " Accuracy: 23.0%, Avg loss: 2.414021 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 2.249582  [    0/ 4000]\n",
            "Training Accuracy: 21.9%\n",
            "Testing loop: \n",
            " Accuracy: 18.8%, Avg loss: 2.455896 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 2.604734  [    0/ 4000]\n",
            "Training Accuracy: 24.3%\n",
            "Testing loop: \n",
            " Accuracy: 20.4%, Avg loss: 2.406297 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 2.089178  [    0/ 4000]\n",
            "Training Accuracy: 23.3%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.304218 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 2.355364  [    0/ 4000]\n",
            "Training Accuracy: 23.8%\n",
            "Testing loop: \n",
            " Accuracy: 26.2%, Avg loss: 2.299747 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 2.312791  [    0/ 4000]\n",
            "Training Accuracy: 25.0%\n",
            "Testing loop: \n",
            " Accuracy: 20.4%, Avg loss: 2.396073 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 2.206515  [    0/ 4000]\n",
            "Training Accuracy: 24.1%\n",
            "Testing loop: \n",
            " Accuracy: 27.8%, Avg loss: 2.312279 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 2.183946  [    0/ 4000]\n",
            "Training Accuracy: 24.3%\n",
            "Testing loop: \n",
            " Accuracy: 27.8%, Avg loss: 2.259510 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 2.456384  [    0/ 4000]\n",
            "Training Accuracy: 23.6%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.302758 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 2.257131  [    0/ 4000]\n",
            "Training Accuracy: 25.2%\n",
            "Testing loop: \n",
            " Accuracy: 25.8%, Avg loss: 2.289713 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 2.343058  [    0/ 4000]\n",
            "Training Accuracy: 28.0%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.240680 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 2.267058  [    0/ 4000]\n",
            "Training Accuracy: 26.8%\n",
            "Testing loop: \n",
            " Accuracy: 25.4%, Avg loss: 2.237144 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 2.064270  [    0/ 4000]\n",
            "Training Accuracy: 25.2%\n",
            "Testing loop: \n",
            " Accuracy: 25.2%, Avg loss: 2.287944 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 2.213226  [    0/ 4000]\n",
            "Training Accuracy: 24.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.235007 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 2.270851  [    0/ 4000]\n",
            "Training Accuracy: 26.4%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.252987 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 2.107692  [    0/ 4000]\n",
            "Training Accuracy: 27.2%\n",
            "Testing loop: \n",
            " Accuracy: 27.8%, Avg loss: 2.268057 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 2.191147  [    0/ 4000]\n",
            "Training Accuracy: 29.0%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.198255 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 2.130968  [    0/ 4000]\n",
            "Training Accuracy: 28.1%\n",
            "Testing loop: \n",
            " Accuracy: 32.6%, Avg loss: 2.199107 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 2.253624  [    0/ 4000]\n",
            "Training Accuracy: 26.8%\n",
            "Testing loop: \n",
            " Accuracy: 25.6%, Avg loss: 2.280825 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 2.123778  [    0/ 4000]\n",
            "Training Accuracy: 28.8%\n",
            "Testing loop: \n",
            " Accuracy: 18.4%, Avg loss: 2.390375 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 2.598045  [    0/ 4000]\n",
            "Training Accuracy: 25.6%\n",
            "Testing loop: \n",
            " Accuracy: 25.0%, Avg loss: 2.248413 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 2.213840  [    0/ 4000]\n",
            "Training Accuracy: 26.9%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.177687 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 2.041059  [    0/ 4000]\n",
            "Training Accuracy: 25.8%\n",
            "Testing loop: \n",
            " Accuracy: 24.2%, Avg loss: 2.290967 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 2.356470  [    0/ 4000]\n",
            "Training Accuracy: 29.4%\n",
            "Testing loop: \n",
            " Accuracy: 24.0%, Avg loss: 2.306978 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 2.566231  [    0/ 4000]\n",
            "Training Accuracy: 27.1%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.171108 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 2.052754  [    0/ 4000]\n",
            "Training Accuracy: 29.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.242258 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 2.011920  [    0/ 4000]\n",
            "Training Accuracy: 28.6%\n",
            "Testing loop: \n",
            " Accuracy: 29.2%, Avg loss: 2.186560 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 2.116069  [    0/ 4000]\n",
            "Training Accuracy: 28.8%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.189998 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 2.113432  [    0/ 4000]\n",
            "Training Accuracy: 27.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.205945 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 2.227485  [    0/ 4000]\n",
            "Training Accuracy: 29.3%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.225376 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 2.032336  [    0/ 4000]\n",
            "Training Accuracy: 26.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.0%, Avg loss: 2.202651 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 2.156033  [    0/ 4000]\n",
            "Training Accuracy: 27.3%\n",
            "Testing loop: \n",
            " Accuracy: 23.2%, Avg loss: 2.431750 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 2.233766  [    0/ 4000]\n",
            "Training Accuracy: 26.7%\n",
            "Testing loop: \n",
            " Accuracy: 31.2%, Avg loss: 2.198699 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 2.177222  [    0/ 4000]\n",
            "Training Accuracy: 28.1%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.141116 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 2.119727  [    0/ 4000]\n",
            "Training Accuracy: 28.1%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.218652 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 2.053555  [    0/ 4000]\n",
            "Training Accuracy: 26.3%\n",
            "Testing loop: \n",
            " Accuracy: 26.4%, Avg loss: 2.241986 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 2.289501  [    0/ 4000]\n",
            "Training Accuracy: 28.9%\n",
            "Testing loop: \n",
            " Accuracy: 29.8%, Avg loss: 2.185649 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 2.010238  [    0/ 4000]\n",
            "Training Accuracy: 29.0%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.166777 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 2.045229  [    0/ 4000]\n",
            "Training Accuracy: 28.2%\n",
            "Testing loop: \n",
            " Accuracy: 26.0%, Avg loss: 2.268266 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 2.393159  [    0/ 4000]\n",
            "Training Accuracy: 27.8%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.198681 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 2.108338  [    0/ 4000]\n",
            "Training Accuracy: 27.4%\n",
            "Testing loop: \n",
            " Accuracy: 27.0%, Avg loss: 2.299031 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 2.209436  [    0/ 4000]\n",
            "Training Accuracy: 27.0%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.228961 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 2.320800  [    0/ 4000]\n",
            "Training Accuracy: 27.8%\n",
            "Testing loop: \n",
            " Accuracy: 24.6%, Avg loss: 2.285424 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 2.344179  [    0/ 4000]\n",
            "Training Accuracy: 25.7%\n",
            "Testing loop: \n",
            " Accuracy: 24.0%, Avg loss: 2.297636 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 2.215152  [    0/ 4000]\n",
            "Training Accuracy: 28.7%\n",
            "Testing loop: \n",
            " Accuracy: 21.6%, Avg loss: 2.286870 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 2.115336  [    0/ 4000]\n",
            "Training Accuracy: 27.9%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.247029 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 2.052449  [    0/ 4000]\n",
            "Training Accuracy: 28.8%\n",
            "Testing loop: \n",
            " Accuracy: 30.0%, Avg loss: 2.192765 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 2.195354  [    0/ 4000]\n",
            "Training Accuracy: 27.4%\n",
            "Testing loop: \n",
            " Accuracy: 27.4%, Avg loss: 2.197690 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 2.044806  [    0/ 4000]\n",
            "Training Accuracy: 24.9%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.161384 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 2.122541  [    0/ 4000]\n",
            "Training Accuracy: 30.8%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.222347 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 2.236533  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 25.0%, Avg loss: 2.268924 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 2.177628  [    0/ 4000]\n",
            "Training Accuracy: 28.9%\n",
            "Testing loop: \n",
            " Accuracy: 29.4%, Avg loss: 2.166891 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 2.391663  [    0/ 4000]\n",
            "Training Accuracy: 28.4%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.184139 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 2.282278  [    0/ 4000]\n",
            "Training Accuracy: 27.3%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.209784 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 2.066990  [    0/ 4000]\n",
            "Training Accuracy: 28.7%\n",
            "Testing loop: \n",
            " Accuracy: 26.2%, Avg loss: 2.258813 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 2.231459  [    0/ 4000]\n",
            "Training Accuracy: 29.7%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.283802 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 2.211731  [    0/ 4000]\n",
            "Training Accuracy: 27.8%\n",
            "Testing loop: \n",
            " Accuracy: 31.6%, Avg loss: 2.172642 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 2.040237  [    0/ 4000]\n",
            "Training Accuracy: 30.2%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.174209 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 2.218404  [    0/ 4000]\n",
            "Training Accuracy: 30.0%\n",
            "Testing loop: \n",
            " Accuracy: 29.2%, Avg loss: 2.148543 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 2.295756  [    0/ 4000]\n",
            "Training Accuracy: 29.1%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.204644 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 2.120328  [    0/ 4000]\n",
            "Training Accuracy: 29.5%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.183079 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 2.157734  [    0/ 4000]\n",
            "Training Accuracy: 28.6%\n",
            "Testing loop: \n",
            " Accuracy: 25.2%, Avg loss: 2.210630 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 1.944518  [    0/ 4000]\n",
            "Training Accuracy: 28.3%\n",
            "Testing loop: \n",
            " Accuracy: 30.0%, Avg loss: 2.182573 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 2.095187  [    0/ 4000]\n",
            "Training Accuracy: 28.2%\n",
            "Testing loop: \n",
            " Accuracy: 33.0%, Avg loss: 2.196107 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 2.090622  [    0/ 4000]\n",
            "Training Accuracy: 28.3%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.172816 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 2.206164  [    0/ 4000]\n",
            "Training Accuracy: 28.8%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.201236 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 2.066482  [    0/ 4000]\n",
            "Training Accuracy: 28.2%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.173276 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 2.245384  [    0/ 4000]\n",
            "Training Accuracy: 29.5%\n",
            "Testing loop: \n",
            " Accuracy: 27.4%, Avg loss: 2.243365 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 2.082449  [    0/ 4000]\n",
            "Training Accuracy: 29.5%\n",
            "Testing loop: \n",
            " Accuracy: 31.2%, Avg loss: 2.106603 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 2.270906  [    0/ 4000]\n",
            "Training Accuracy: 29.2%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.194008 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 2.457428  [    0/ 4000]\n",
            "Training Accuracy: 28.9%\n",
            "Testing loop: \n",
            " Accuracy: 24.2%, Avg loss: 2.264734 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 1.954920  [    0/ 4000]\n",
            "Training Accuracy: 27.4%\n",
            "Testing loop: \n",
            " Accuracy: 23.8%, Avg loss: 2.303176 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 2.279319  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 32.6%, Avg loss: 2.156736 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 2.037627  [    0/ 4000]\n",
            "Training Accuracy: 30.0%\n",
            "Testing loop: \n",
            " Accuracy: 24.6%, Avg loss: 2.260127 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 2.364624  [    0/ 4000]\n",
            "Training Accuracy: 27.2%\n",
            "Testing loop: \n",
            " Accuracy: 29.4%, Avg loss: 2.147607 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 2.046383  [    0/ 4000]\n",
            "Training Accuracy: 28.8%\n",
            "Testing loop: \n",
            " Accuracy: 27.0%, Avg loss: 2.181215 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 2.016498  [    0/ 4000]\n",
            "Training Accuracy: 29.5%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.126283 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 2.224710  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 29.2%, Avg loss: 2.207565 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 2.079133  [    0/ 4000]\n",
            "Training Accuracy: 29.0%\n",
            "Testing loop: \n",
            " Accuracy: 27.2%, Avg loss: 2.217726 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 2.056984  [    0/ 4000]\n",
            "Training Accuracy: 27.7%\n",
            "Testing loop: \n",
            " Accuracy: 17.8%, Avg loss: 2.449968 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 2.523582  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 25.0%, Avg loss: 2.250632 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 2.252200  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.180139 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 2.427129  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 35.4%, Avg loss: 2.090521 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 2.116194  [    0/ 4000]\n",
            "Training Accuracy: 31.8%\n",
            "Testing loop: \n",
            " Accuracy: 32.6%, Avg loss: 2.105666 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 2.264745  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 33.6%, Avg loss: 2.109920 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 2.015831  [    0/ 4000]\n",
            "Training Accuracy: 32.3%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.182317 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 2.172514  [    0/ 4000]\n",
            "Training Accuracy: 31.9%\n",
            "Testing loop: \n",
            " Accuracy: 28.0%, Avg loss: 2.213797 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 2.120793  [    0/ 4000]\n",
            "Training Accuracy: 33.3%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.225615 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 2.197929  [    0/ 4000]\n",
            "Training Accuracy: 31.6%\n",
            "Testing loop: \n",
            " Accuracy: 36.0%, Avg loss: 2.090617 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 1.889971  [    0/ 4000]\n",
            "Training Accuracy: 32.1%\n",
            "Testing loop: \n",
            " Accuracy: 26.6%, Avg loss: 2.301451 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 2.030532  [    0/ 4000]\n",
            "Training Accuracy: 31.3%\n",
            "Testing loop: \n",
            " Accuracy: 31.2%, Avg loss: 2.106897 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 2.011696  [    0/ 4000]\n",
            "Training Accuracy: 30.9%\n",
            "Testing loop: \n",
            " Accuracy: 30.2%, Avg loss: 2.200534 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 2.145715  [    0/ 4000]\n",
            "Training Accuracy: 31.2%\n",
            "Testing loop: \n",
            " Accuracy: 24.8%, Avg loss: 2.333173 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 2.204970  [    0/ 4000]\n",
            "Training Accuracy: 31.0%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.181317 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 1.929899  [    0/ 4000]\n",
            "Training Accuracy: 33.2%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.211915 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 2.163445  [    0/ 4000]\n",
            "Training Accuracy: 35.4%\n",
            "Testing loop: \n",
            " Accuracy: 36.6%, Avg loss: 2.077093 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 1.887668  [    0/ 4000]\n",
            "Training Accuracy: 32.7%\n",
            "Testing loop: \n",
            " Accuracy: 35.8%, Avg loss: 2.082497 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 2.001972  [    0/ 4000]\n",
            "Training Accuracy: 36.3%\n",
            "Testing loop: \n",
            " Accuracy: 36.4%, Avg loss: 2.052972 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 2.062330  [    0/ 4000]\n",
            "Training Accuracy: 34.4%\n",
            "Testing loop: \n",
            " Accuracy: 32.0%, Avg loss: 2.164107 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 2.189664  [    0/ 4000]\n",
            "Training Accuracy: 34.2%\n",
            "Testing loop: \n",
            " Accuracy: 37.4%, Avg loss: 2.066439 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 1.883559  [    0/ 4000]\n",
            "Training Accuracy: 35.8%\n",
            "Testing loop: \n",
            " Accuracy: 33.8%, Avg loss: 2.074941 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 2.108896  [    0/ 4000]\n",
            "Training Accuracy: 33.9%\n",
            "Testing loop: \n",
            " Accuracy: 34.6%, Avg loss: 2.107899 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 2.049571  [    0/ 4000]\n",
            "Training Accuracy: 38.2%\n",
            "Testing loop: \n",
            " Accuracy: 36.0%, Avg loss: 2.039407 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 2.055032  [    0/ 4000]\n",
            "Training Accuracy: 37.5%\n",
            "Testing loop: \n",
            " Accuracy: 37.0%, Avg loss: 2.065789 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 2.020371  [    0/ 4000]\n",
            "Training Accuracy: 36.3%\n",
            "Testing loop: \n",
            " Accuracy: 32.2%, Avg loss: 2.180729 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 1.972739  [    0/ 4000]\n",
            "Training Accuracy: 38.5%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.146425 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 2.030508  [    0/ 4000]\n",
            "Training Accuracy: 36.8%\n",
            "Testing loop: \n",
            " Accuracy: 31.4%, Avg loss: 2.154290 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 2.156721  [    0/ 4000]\n",
            "Training Accuracy: 37.5%\n",
            "Testing loop: \n",
            " Accuracy: 37.8%, Avg loss: 2.028682 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 2.014088  [    0/ 4000]\n",
            "Training Accuracy: 39.6%\n",
            "Testing loop: \n",
            " Accuracy: 39.4%, Avg loss: 2.014165 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 1.613967  [    0/ 4000]\n",
            "Training Accuracy: 37.6%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.208185 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 2.309523  [    0/ 4000]\n",
            "Training Accuracy: 35.9%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.118052 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 2.042950  [    0/ 4000]\n",
            "Training Accuracy: 39.4%\n",
            "Testing loop: \n",
            " Accuracy: 38.4%, Avg loss: 2.043518 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 2.053460  [    0/ 4000]\n",
            "Training Accuracy: 38.7%\n",
            "Testing loop: \n",
            " Accuracy: 33.2%, Avg loss: 2.128057 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 2.220712  [    0/ 4000]\n",
            "Training Accuracy: 38.2%\n",
            "Testing loop: \n",
            " Accuracy: 41.8%, Avg loss: 1.948991 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 2.127075  [    0/ 4000]\n",
            "Training Accuracy: 39.9%\n",
            "Testing loop: \n",
            " Accuracy: 38.0%, Avg loss: 2.017473 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 1.570924  [    0/ 4000]\n",
            "Training Accuracy: 36.2%\n",
            "Testing loop: \n",
            " Accuracy: 31.6%, Avg loss: 2.158759 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 2.323404  [    0/ 4000]\n",
            "Training Accuracy: 35.9%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.228278 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 2.312485  [    0/ 4000]\n",
            "Training Accuracy: 37.2%\n",
            "Testing loop: \n",
            " Accuracy: 40.0%, Avg loss: 1.980791 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 1.809003  [    0/ 4000]\n",
            "Training Accuracy: 38.1%\n",
            "Testing loop: \n",
            " Accuracy: 32.8%, Avg loss: 2.139029 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 2.076848  [    0/ 4000]\n",
            "Training Accuracy: 38.0%\n",
            "Testing loop: \n",
            " Accuracy: 34.6%, Avg loss: 2.031365 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 2.053241  [    0/ 4000]\n",
            "Training Accuracy: 36.9%\n",
            "Testing loop: \n",
            " Accuracy: 39.6%, Avg loss: 1.973760 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 2.016298  [    0/ 4000]\n",
            "Training Accuracy: 39.1%\n",
            "Testing loop: \n",
            " Accuracy: 35.4%, Avg loss: 2.012541 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 1.913304  [    0/ 4000]\n",
            "Training Accuracy: 38.8%\n",
            "Testing loop: \n",
            " Accuracy: 34.8%, Avg loss: 2.088835 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 1.846179  [    0/ 4000]\n",
            "Training Accuracy: 37.9%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.250464 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 2.196541  [    0/ 4000]\n",
            "Training Accuracy: 35.2%\n",
            "Testing loop: \n",
            " Accuracy: 38.6%, Avg loss: 1.987792 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 1.872342  [    0/ 4000]\n",
            "Training Accuracy: 37.9%\n",
            "Testing loop: \n",
            " Accuracy: 31.6%, Avg loss: 2.176868 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 50\n",
        "epochs = 150\n",
        "momentum = 0.9\n",
        "weight_decay = 0.05\n",
        "dampening = 0\n",
        "getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening)"
      ],
      "id": "B0s1vsEdRhhQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcIf-G9LOlND"
      },
      "source": [
        "## BatchSize Variations"
      ],
      "id": "vcIf-G9LOlND"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGg1qIq4Nm9A"
      },
      "source": [
        "### 100, momentun 0.3 "
      ],
      "id": "iGg1qIq4Nm9A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37780
        },
        "id": "HKvfOX1JNpSW",
        "outputId": "2cb3978f-00be-459b-f43c-028b6f7500a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.994647  [    0/ 4000]\n",
            "Training Accuracy: 5.1%\n",
            "Testing loop: \n",
            " Accuracy: 6.6%, Avg loss: 2.989008 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.982416  [    0/ 4000]\n",
            "Training Accuracy: 8.7%\n",
            "Testing loop: \n",
            " Accuracy: 11.4%, Avg loss: 2.982104 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.985350  [    0/ 4000]\n",
            "Training Accuracy: 11.0%\n",
            "Testing loop: \n",
            " Accuracy: 10.4%, Avg loss: 2.975524 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.972277  [    0/ 4000]\n",
            "Training Accuracy: 15.1%\n",
            "Testing loop: \n",
            " Accuracy: 16.0%, Avg loss: 2.967580 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.973943  [    0/ 4000]\n",
            "Training Accuracy: 19.4%\n",
            "Testing loop: \n",
            " Accuracy: 19.6%, Avg loss: 2.958011 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.959793  [    0/ 4000]\n",
            "Training Accuracy: 22.7%\n",
            "Testing loop: \n",
            " Accuracy: 24.0%, Avg loss: 2.944798 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.933980  [    0/ 4000]\n",
            "Training Accuracy: 28.4%\n",
            "Testing loop: \n",
            " Accuracy: 29.8%, Avg loss: 2.927897 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.932166  [    0/ 4000]\n",
            "Training Accuracy: 33.1%\n",
            "Testing loop: \n",
            " Accuracy: 33.6%, Avg loss: 2.904601 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.891055  [    0/ 4000]\n",
            "Training Accuracy: 33.8%\n",
            "Testing loop: \n",
            " Accuracy: 38.2%, Avg loss: 2.872057 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.873743  [    0/ 4000]\n",
            "Training Accuracy: 33.4%\n",
            "Testing loop: \n",
            " Accuracy: 33.6%, Avg loss: 2.825808 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.819594  [    0/ 4000]\n",
            "Training Accuracy: 34.6%\n",
            "Testing loop: \n",
            " Accuracy: 27.4%, Avg loss: 2.757592 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.772018  [    0/ 4000]\n",
            "Training Accuracy: 31.8%\n",
            "Testing loop: \n",
            " Accuracy: 32.8%, Avg loss: 2.668324 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.624079  [    0/ 4000]\n",
            "Training Accuracy: 32.5%\n",
            "Testing loop: \n",
            " Accuracy: 32.4%, Avg loss: 2.572910 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.471679  [    0/ 4000]\n",
            "Training Accuracy: 32.8%\n",
            "Testing loop: \n",
            " Accuracy: 32.8%, Avg loss: 2.468980 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.443797  [    0/ 4000]\n",
            "Training Accuracy: 34.5%\n",
            "Testing loop: \n",
            " Accuracy: 36.2%, Avg loss: 2.373267 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.413235  [    0/ 4000]\n",
            "Training Accuracy: 37.0%\n",
            "Testing loop: \n",
            " Accuracy: 40.8%, Avg loss: 2.282823 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.259162  [    0/ 4000]\n",
            "Training Accuracy: 40.2%\n",
            "Testing loop: \n",
            " Accuracy: 40.2%, Avg loss: 2.205417 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.026428  [    0/ 4000]\n",
            "Training Accuracy: 41.4%\n",
            "Testing loop: \n",
            " Accuracy: 39.0%, Avg loss: 2.159301 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.122186  [    0/ 4000]\n",
            "Training Accuracy: 43.4%\n",
            "Testing loop: \n",
            " Accuracy: 39.6%, Avg loss: 2.083586 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 1.918423  [    0/ 4000]\n",
            "Training Accuracy: 44.9%\n",
            "Testing loop: \n",
            " Accuracy: 44.6%, Avg loss: 2.008852 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 1.975433  [    0/ 4000]\n",
            "Training Accuracy: 45.7%\n",
            "Testing loop: \n",
            " Accuracy: 44.0%, Avg loss: 1.953594 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 1.819738  [    0/ 4000]\n",
            "Training Accuracy: 46.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.886079 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 1.908177  [    0/ 4000]\n",
            "Training Accuracy: 47.6%\n",
            "Testing loop: \n",
            " Accuracy: 45.6%, Avg loss: 1.899689 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 1.813937  [    0/ 4000]\n",
            "Training Accuracy: 48.1%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.817234 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 1.523912  [    0/ 4000]\n",
            "Training Accuracy: 49.2%\n",
            "Testing loop: \n",
            " Accuracy: 49.0%, Avg loss: 1.796017 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 1.440796  [    0/ 4000]\n",
            "Training Accuracy: 49.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.768104 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 1.900057  [    0/ 4000]\n",
            "Training Accuracy: 49.9%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.751765 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 1.612233  [    0/ 4000]\n",
            "Training Accuracy: 50.7%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.726758 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 1.668776  [    0/ 4000]\n",
            "Training Accuracy: 50.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.712975 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 1.496105  [    0/ 4000]\n",
            "Training Accuracy: 51.6%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.698209 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 1.544086  [    0/ 4000]\n",
            "Training Accuracy: 51.1%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.680881 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 1.735751  [    0/ 4000]\n",
            "Training Accuracy: 52.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.6%, Avg loss: 1.795154 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 1.614652  [    0/ 4000]\n",
            "Training Accuracy: 53.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.662979 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 1.562586  [    0/ 4000]\n",
            "Training Accuracy: 53.4%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.639440 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 1.369561  [    0/ 4000]\n",
            "Training Accuracy: 53.9%\n",
            "Testing loop: \n",
            " Accuracy: 51.8%, Avg loss: 1.658110 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 1.451606  [    0/ 4000]\n",
            "Training Accuracy: 54.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.709084 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 1.695276  [    0/ 4000]\n",
            "Training Accuracy: 54.2%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.638251 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 1.409960  [    0/ 4000]\n",
            "Training Accuracy: 54.9%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.643974 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 1.444412  [    0/ 4000]\n",
            "Training Accuracy: 55.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.643556 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 1.409071  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.623948 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 1.656507  [    0/ 4000]\n",
            "Training Accuracy: 55.9%\n",
            "Testing loop: \n",
            " Accuracy: 52.4%, Avg loss: 1.603089 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 1.232694  [    0/ 4000]\n",
            "Training Accuracy: 57.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.680887 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 1.541265  [    0/ 4000]\n",
            "Training Accuracy: 57.2%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.569742 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 1.584196  [    0/ 4000]\n",
            "Training Accuracy: 57.2%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.580826 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 1.382590  [    0/ 4000]\n",
            "Training Accuracy: 57.7%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.586935 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 1.189654  [    0/ 4000]\n",
            "Training Accuracy: 58.1%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.592924 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 1.425420  [    0/ 4000]\n",
            "Training Accuracy: 59.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.547941 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 1.496330  [    0/ 4000]\n",
            "Training Accuracy: 58.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.552058 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 1.561676  [    0/ 4000]\n",
            "Training Accuracy: 59.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.602873 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 1.249003  [    0/ 4000]\n",
            "Training Accuracy: 59.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.593803 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 1.422832  [    0/ 4000]\n",
            "Training Accuracy: 60.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.529795 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 1.222340  [    0/ 4000]\n",
            "Training Accuracy: 59.8%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.562917 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 1.348742  [    0/ 4000]\n",
            "Training Accuracy: 60.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.8%, Avg loss: 1.676261 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 1.356113  [    0/ 4000]\n",
            "Training Accuracy: 60.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.541561 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 1.392497  [    0/ 4000]\n",
            "Training Accuracy: 60.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.547000 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 1.591421  [    0/ 4000]\n",
            "Training Accuracy: 60.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.532642 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 1.329859  [    0/ 4000]\n",
            "Training Accuracy: 61.3%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.526934 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 1.255642  [    0/ 4000]\n",
            "Training Accuracy: 61.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.522841 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 1.304580  [    0/ 4000]\n",
            "Training Accuracy: 61.8%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.490851 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 1.149756  [    0/ 4000]\n",
            "Training Accuracy: 62.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.519431 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 1.481495  [    0/ 4000]\n",
            "Training Accuracy: 62.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.507035 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 1.377190  [    0/ 4000]\n",
            "Training Accuracy: 62.4%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.526482 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 1.409105  [    0/ 4000]\n",
            "Training Accuracy: 63.5%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.501583 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 1.349833  [    0/ 4000]\n",
            "Training Accuracy: 62.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.540311 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 1.222022  [    0/ 4000]\n",
            "Training Accuracy: 63.7%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.579672 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 1.319239  [    0/ 4000]\n",
            "Training Accuracy: 63.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.489931 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 1.105740  [    0/ 4000]\n",
            "Training Accuracy: 64.1%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.492222 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 1.371583  [    0/ 4000]\n",
            "Training Accuracy: 64.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.498510 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 1.256480  [    0/ 4000]\n",
            "Training Accuracy: 64.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.447957 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 0.920485  [    0/ 4000]\n",
            "Training Accuracy: 64.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.485580 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 1.076366  [    0/ 4000]\n",
            "Training Accuracy: 64.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.541091 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 1.053177  [    0/ 4000]\n",
            "Training Accuracy: 65.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.482010 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 1.229986  [    0/ 4000]\n",
            "Training Accuracy: 65.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.538898 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 1.366530  [    0/ 4000]\n",
            "Training Accuracy: 65.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.492701 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 1.089175  [    0/ 4000]\n",
            "Training Accuracy: 65.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.459595 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 0.941817  [    0/ 4000]\n",
            "Training Accuracy: 65.1%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 1.483302 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 1.254736  [    0/ 4000]\n",
            "Training Accuracy: 66.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.433313 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 1.022022  [    0/ 4000]\n",
            "Training Accuracy: 66.8%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.483154 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 0.901238  [    0/ 4000]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.432760 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 1.154142  [    0/ 4000]\n",
            "Training Accuracy: 66.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.440997 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 1.084185  [    0/ 4000]\n",
            "Training Accuracy: 67.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.461332 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 1.136952  [    0/ 4000]\n",
            "Training Accuracy: 66.7%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.477087 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 1.094863  [    0/ 4000]\n",
            "Training Accuracy: 67.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.445024 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 1.160357  [    0/ 4000]\n",
            "Training Accuracy: 68.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.434581 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 1.030712  [    0/ 4000]\n",
            "Training Accuracy: 67.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.478349 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 1.005919  [    0/ 4000]\n",
            "Training Accuracy: 68.2%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.531002 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 1.378397  [    0/ 4000]\n",
            "Training Accuracy: 67.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.419660 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 1.482972  [    0/ 4000]\n",
            "Training Accuracy: 68.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.499156 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 1.176622  [    0/ 4000]\n",
            "Training Accuracy: 68.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.415338 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 0.960769  [    0/ 4000]\n",
            "Training Accuracy: 68.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.456767 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 0.917715  [    0/ 4000]\n",
            "Training Accuracy: 68.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.448909 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 0.859904  [    0/ 4000]\n",
            "Training Accuracy: 69.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.416442 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 0.859947  [    0/ 4000]\n",
            "Training Accuracy: 70.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.415317 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 0.954211  [    0/ 4000]\n",
            "Training Accuracy: 69.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.418080 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 0.789347  [    0/ 4000]\n",
            "Training Accuracy: 69.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.425343 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 1.117967  [    0/ 4000]\n",
            "Training Accuracy: 70.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.512162 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 0.948392  [    0/ 4000]\n",
            "Training Accuracy: 70.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.425807 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 1.085177  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.458295 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 1.002189  [    0/ 4000]\n",
            "Training Accuracy: 70.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.405764 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 0.820837  [    0/ 4000]\n",
            "Training Accuracy: 70.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.457518 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 1.086175  [    0/ 4000]\n",
            "Training Accuracy: 71.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.428728 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 0.884273  [    0/ 4000]\n",
            "Training Accuracy: 70.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.441532 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 0.966101  [    0/ 4000]\n",
            "Training Accuracy: 71.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.396349 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 1.004049  [    0/ 4000]\n",
            "Training Accuracy: 72.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.408774 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 0.956472  [    0/ 4000]\n",
            "Training Accuracy: 71.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.415861 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 0.915177  [    0/ 4000]\n",
            "Training Accuracy: 71.3%\n",
            "Testing loop: \n",
            " Accuracy: 61.0%, Avg loss: 1.446176 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 1.109206  [    0/ 4000]\n",
            "Training Accuracy: 71.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.438247 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 1.045574  [    0/ 4000]\n",
            "Training Accuracy: 72.6%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.434887 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 0.732639  [    0/ 4000]\n",
            "Training Accuracy: 72.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.414440 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 0.821969  [    0/ 4000]\n",
            "Training Accuracy: 72.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.456734 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 0.915465  [    0/ 4000]\n",
            "Training Accuracy: 72.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.558043 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 0.985183  [    0/ 4000]\n",
            "Training Accuracy: 72.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.426937 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 0.921330  [    0/ 4000]\n",
            "Training Accuracy: 72.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.454821 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 1.111314  [    0/ 4000]\n",
            "Training Accuracy: 73.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.423997 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 0.720089  [    0/ 4000]\n",
            "Training Accuracy: 72.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.465811 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 0.746720  [    0/ 4000]\n",
            "Training Accuracy: 72.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.439342 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 0.982376  [    0/ 4000]\n",
            "Training Accuracy: 74.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.432769 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 1.000170  [    0/ 4000]\n",
            "Training Accuracy: 74.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.576316 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 1.003670  [    0/ 4000]\n",
            "Training Accuracy: 74.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.425833 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 0.800350  [    0/ 4000]\n",
            "Training Accuracy: 74.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.420830 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 0.841464  [    0/ 4000]\n",
            "Training Accuracy: 74.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.429217 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 0.628117  [    0/ 4000]\n",
            "Training Accuracy: 74.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.444763 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 0.647715  [    0/ 4000]\n",
            "Training Accuracy: 74.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.432441 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 0.644087  [    0/ 4000]\n",
            "Training Accuracy: 73.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.422743 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 0.832243  [    0/ 4000]\n",
            "Training Accuracy: 74.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.483201 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 1.034599  [    0/ 4000]\n",
            "Training Accuracy: 74.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.444511 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 0.883858  [    0/ 4000]\n",
            "Training Accuracy: 75.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.437677 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 0.938253  [    0/ 4000]\n",
            "Training Accuracy: 75.2%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 1.478618 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 0.759022  [    0/ 4000]\n",
            "Training Accuracy: 75.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.466397 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 0.731740  [    0/ 4000]\n",
            "Training Accuracy: 76.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.507829 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 0.877809  [    0/ 4000]\n",
            "Training Accuracy: 75.5%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.603331 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 0.883886  [    0/ 4000]\n",
            "Training Accuracy: 75.9%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.503076 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 0.919111  [    0/ 4000]\n",
            "Training Accuracy: 75.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.471001 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 0.507930  [    0/ 4000]\n",
            "Training Accuracy: 77.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.506511 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 0.727415  [    0/ 4000]\n",
            "Training Accuracy: 76.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.461607 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 0.746737  [    0/ 4000]\n",
            "Training Accuracy: 77.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.527400 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 0.951832  [    0/ 4000]\n",
            "Training Accuracy: 76.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.478372 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 0.792293  [    0/ 4000]\n",
            "Training Accuracy: 77.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.456673 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 0.840706  [    0/ 4000]\n",
            "Training Accuracy: 76.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.561369 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 0.896300  [    0/ 4000]\n",
            "Training Accuracy: 77.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.519633 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 0.752860  [    0/ 4000]\n",
            "Training Accuracy: 77.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.502484 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 0.866788  [    0/ 4000]\n",
            "Training Accuracy: 77.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.496908 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 0.594793  [    0/ 4000]\n",
            "Training Accuracy: 78.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.480717 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 0.700214  [    0/ 4000]\n",
            "Training Accuracy: 78.3%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.533673 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 0.597610  [    0/ 4000]\n",
            "Training Accuracy: 78.2%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.582959 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 0.982168  [    0/ 4000]\n",
            "Training Accuracy: 78.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.524172 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 0.866944  [    0/ 4000]\n",
            "Training Accuracy: 78.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.493150 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 0.611135  [    0/ 4000]\n",
            "Training Accuracy: 79.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.506093 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 0.518884  [    0/ 4000]\n",
            "Training Accuracy: 78.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.554069 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 0.515825  [    0/ 4000]\n",
            "Training Accuracy: 78.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.483043 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loop: loss: 0.562046  [    0/ 4000]\n",
            "Training Accuracy: 79.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.521906 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loop: loss: 0.810601  [    0/ 4000]\n",
            "Training Accuracy: 79.1%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.482018 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loop: loss: 0.552585  [    0/ 4000]\n",
            "Training Accuracy: 79.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.498748 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loop: loss: 0.478013  [    0/ 4000]\n",
            "Training Accuracy: 79.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.518154 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loop: loss: 0.636813  [    0/ 4000]\n",
            "Training Accuracy: 79.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.518006 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loop: loss: 0.807106  [    0/ 4000]\n",
            "Training Accuracy: 80.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.508796 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loop: loss: 0.650117  [    0/ 4000]\n",
            "Training Accuracy: 80.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.590676 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loop: loss: 0.526032  [    0/ 4000]\n",
            "Training Accuracy: 80.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.516867 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loop: loss: 0.646272  [    0/ 4000]\n",
            "Training Accuracy: 81.5%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.574567 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loop: loss: 0.613391  [    0/ 4000]\n",
            "Training Accuracy: 80.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.522569 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loop: loss: 0.615576  [    0/ 4000]\n",
            "Training Accuracy: 79.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.500479 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loop: loss: 0.664431  [    0/ 4000]\n",
            "Training Accuracy: 81.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 1.586753 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loop: loss: 0.707825  [    0/ 4000]\n",
            "Training Accuracy: 81.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.573536 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loop: loss: 0.501561  [    0/ 4000]\n",
            "Training Accuracy: 81.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.533478 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loop: loss: 0.617528  [    0/ 4000]\n",
            "Training Accuracy: 82.1%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.553839 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loop: loss: 0.609227  [    0/ 4000]\n",
            "Training Accuracy: 81.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.514185 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loop: loss: 0.577929  [    0/ 4000]\n",
            "Training Accuracy: 82.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.531507 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loop: loss: 0.523860  [    0/ 4000]\n",
            "Training Accuracy: 82.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.574131 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loop: loss: 0.613763  [    0/ 4000]\n",
            "Training Accuracy: 81.0%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.563434 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loop: loss: 0.712860  [    0/ 4000]\n",
            "Training Accuracy: 82.7%\n",
            "Testing loop: \n",
            " Accuracy: 57.0%, Avg loss: 1.670724 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loop: loss: 0.526988  [    0/ 4000]\n",
            "Training Accuracy: 81.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.573142 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loop: loss: 0.546080  [    0/ 4000]\n",
            "Training Accuracy: 82.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.595785 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loop: loss: 0.627896  [    0/ 4000]\n",
            "Training Accuracy: 82.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.556933 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loop: loss: 0.459696  [    0/ 4000]\n",
            "Training Accuracy: 83.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.560170 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loop: loss: 0.408986  [    0/ 4000]\n",
            "Training Accuracy: 84.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.560062 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loop: loss: 0.651829  [    0/ 4000]\n",
            "Training Accuracy: 84.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.610834 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loop: loss: 0.470049  [    0/ 4000]\n",
            "Training Accuracy: 83.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.597077 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loop: loss: 0.389751  [    0/ 4000]\n",
            "Training Accuracy: 85.5%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.773020 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loop: loss: 0.773845  [    0/ 4000]\n",
            "Training Accuracy: 84.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.593528 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loop: loss: 0.575387  [    0/ 4000]\n",
            "Training Accuracy: 85.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.574506 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loop: loss: 0.491149  [    0/ 4000]\n",
            "Training Accuracy: 84.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.779663 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loop: loss: 0.692772  [    0/ 4000]\n",
            "Training Accuracy: 85.8%\n",
            "Testing loop: \n",
            " Accuracy: 57.8%, Avg loss: 1.663871 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loop: loss: 0.578310  [    0/ 4000]\n",
            "Training Accuracy: 86.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.569641 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loop: loss: 0.373714  [    0/ 4000]\n",
            "Training Accuracy: 85.8%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.759426 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loop: loss: 0.601905  [    0/ 4000]\n",
            "Training Accuracy: 84.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.718041 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loop: loss: 0.515766  [    0/ 4000]\n",
            "Training Accuracy: 84.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.844581 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loop: loss: 0.703988  [    0/ 4000]\n",
            "Training Accuracy: 85.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.624957 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loop: loss: 0.580642  [    0/ 4000]\n",
            "Training Accuracy: 86.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.679870 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loop: loss: 0.534802  [    0/ 4000]\n",
            "Training Accuracy: 85.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.611191 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loop: loss: 0.542915  [    0/ 4000]\n",
            "Training Accuracy: 86.6%\n",
            "Testing loop: \n",
            " Accuracy: 61.0%, Avg loss: 1.625575 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loop: loss: 0.584693  [    0/ 4000]\n",
            "Training Accuracy: 86.9%\n",
            "Testing loop: \n",
            " Accuracy: 61.0%, Avg loss: 1.672398 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loop: loss: 0.304599  [    0/ 4000]\n",
            "Training Accuracy: 86.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.636614 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loop: loss: 0.414228  [    0/ 4000]\n",
            "Training Accuracy: 87.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.767814 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loop: loss: 0.624812  [    0/ 4000]\n",
            "Training Accuracy: 85.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.653486 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loop: loss: 0.418859  [    0/ 4000]\n",
            "Training Accuracy: 87.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.655352 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loop: loss: 0.395839  [    0/ 4000]\n",
            "Training Accuracy: 87.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.652714 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loop: loss: 0.507074  [    0/ 4000]\n",
            "Training Accuracy: 87.3%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.668754 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loop: loss: 0.424816  [    0/ 4000]\n",
            "Training Accuracy: 87.6%\n",
            "Testing loop: \n",
            " Accuracy: 58.2%, Avg loss: 1.688188 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loop: loss: 0.315930  [    0/ 4000]\n",
            "Training Accuracy: 87.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.715370 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loop: loss: 0.391875  [    0/ 4000]\n",
            "Training Accuracy: 88.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 1.891926 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loop: loss: 0.408794  [    0/ 4000]\n",
            "Training Accuracy: 88.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.671843 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loop: loss: 0.277315  [    0/ 4000]\n",
            "Training Accuracy: 88.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.699869 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loop: loss: 0.369819  [    0/ 4000]\n",
            "Training Accuracy: 89.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.771440 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loop: loss: 0.359194  [    0/ 4000]\n",
            "Training Accuracy: 90.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.713500 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loop: loss: 0.218027  [    0/ 4000]\n",
            "Training Accuracy: 89.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.763363 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loop: loss: 0.340070  [    0/ 4000]\n",
            "Training Accuracy: 90.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.721681 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loop: loss: 0.222517  [    0/ 4000]\n",
            "Training Accuracy: 89.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.733056 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loop: loss: 0.257671  [    0/ 4000]\n",
            "Training Accuracy: 89.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.741345 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loop: loss: 0.378999  [    0/ 4000]\n",
            "Training Accuracy: 90.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.834322 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loop: loss: 0.347704  [    0/ 4000]\n",
            "Training Accuracy: 88.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.771140 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loop: loss: 0.414127  [    0/ 4000]\n",
            "Training Accuracy: 91.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.8%, Avg loss: 1.834136 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loop: loss: 0.280154  [    0/ 4000]\n",
            "Training Accuracy: 89.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.720501 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loop: loss: 0.264601  [    0/ 4000]\n",
            "Training Accuracy: 90.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.744271 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loop: loss: 0.346878  [    0/ 4000]\n",
            "Training Accuracy: 90.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.816386 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loop: loss: 0.335161  [    0/ 4000]\n",
            "Training Accuracy: 90.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.801944 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loop: loss: 0.232066  [    0/ 4000]\n",
            "Training Accuracy: 88.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.680812 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loop: loss: 0.441194  [    0/ 4000]\n",
            "Training Accuracy: 92.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.810342 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loop: loss: 0.260420  [    0/ 4000]\n",
            "Training Accuracy: 90.6%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 2.172688 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loop: loss: 0.636555  [    0/ 4000]\n",
            "Training Accuracy: 91.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.860279 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loop: loss: 0.234243  [    0/ 4000]\n",
            "Training Accuracy: 93.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.845754 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loop: loss: 0.242396  [    0/ 4000]\n",
            "Training Accuracy: 92.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 1.793359 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loop: loss: 0.198925  [    0/ 4000]\n",
            "Training Accuracy: 93.4%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.883805 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loop: loss: 0.380493  [    0/ 4000]\n",
            "Training Accuracy: 92.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.829843 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loop: loss: 0.358520  [    0/ 4000]\n",
            "Training Accuracy: 90.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.742854 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loop: loss: 0.380994  [    0/ 4000]\n",
            "Training Accuracy: 93.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.871822 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loop: loss: 0.283739  [    0/ 4000]\n",
            "Training Accuracy: 91.6%\n",
            "Testing loop: \n",
            " Accuracy: 57.4%, Avg loss: 1.826942 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loop: loss: 0.371089  [    0/ 4000]\n",
            "Training Accuracy: 92.0%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 1.802499 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loop: loss: 0.203337  [    0/ 4000]\n",
            "Training Accuracy: 94.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.920053 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loop: loss: 0.241713  [    0/ 4000]\n",
            "Training Accuracy: 93.7%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.849295 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loop: loss: 0.322547  [    0/ 4000]\n",
            "Training Accuracy: 94.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.861390 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loop: loss: 0.153302  [    0/ 4000]\n",
            "Training Accuracy: 87.8%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 1.829730 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loop: loss: 0.244867  [    0/ 4000]\n",
            "Training Accuracy: 95.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 1.904870 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loop: loss: 0.177429  [    0/ 4000]\n",
            "Training Accuracy: 94.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 1.865726 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loop: loss: 0.232728  [    0/ 4000]\n",
            "Training Accuracy: 93.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.892451 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loop: loss: 0.202601  [    0/ 4000]\n",
            "Training Accuracy: 93.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.856011 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loop: loss: 0.163593  [    0/ 4000]\n",
            "Training Accuracy: 95.1%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 1.936337 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loop: loss: 0.156397  [    0/ 4000]\n",
            "Training Accuracy: 95.8%\n",
            "Testing loop: \n",
            " Accuracy: 58.8%, Avg loss: 1.936594 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loop: loss: 0.185907  [    0/ 4000]\n",
            "Training Accuracy: 94.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.938607 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loop: loss: 0.161808  [    0/ 4000]\n",
            "Training Accuracy: 94.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.2%, Avg loss: 2.036177 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loop: loss: 0.318605  [    0/ 4000]\n",
            "Training Accuracy: 93.8%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 1.834333 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loop: loss: 0.169661  [    0/ 4000]\n",
            "Training Accuracy: 95.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 2.001676 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loop: loss: 0.148890  [    0/ 4000]\n",
            "Training Accuracy: 92.7%\n",
            "Testing loop: \n",
            " Accuracy: 45.4%, Avg loss: 2.845489 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loop: loss: 1.341612  [    0/ 4000]\n",
            "Training Accuracy: 92.8%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.925790 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loop: loss: 0.191760  [    0/ 4000]\n",
            "Training Accuracy: 96.3%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 1.930216 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loop: loss: 0.134859  [    0/ 4000]\n",
            "Training Accuracy: 96.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 2.014188 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loop: loss: 0.161973  [    0/ 4000]\n",
            "Training Accuracy: 96.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.023413 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loop: loss: 0.209578  [    0/ 4000]\n",
            "Training Accuracy: 96.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.979530 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loop: loss: 0.147083  [    0/ 4000]\n",
            "Training Accuracy: 96.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 1.931028 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loop: loss: 0.126087  [    0/ 4000]\n",
            "Training Accuracy: 96.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 2.055528 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loop: loss: 0.148767  [    0/ 4000]\n",
            "Training Accuracy: 94.9%\n",
            "Testing loop: \n",
            " Accuracy: 61.6%, Avg loss: 1.959463 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loop: loss: 0.076706  [    0/ 4000]\n",
            "Training Accuracy: 97.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.961239 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loop: loss: 0.108912  [    0/ 4000]\n",
            "Training Accuracy: 97.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.6%, Avg loss: 1.990643 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loop: loss: 0.069253  [    0/ 4000]\n",
            "Training Accuracy: 94.4%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 2.318105 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loop: loss: 0.398004  [    0/ 4000]\n",
            "Training Accuracy: 96.6%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 2.060522 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loop: loss: 0.114583  [    0/ 4000]\n",
            "Training Accuracy: 97.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.985418 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loop: loss: 0.094391  [    0/ 4000]\n",
            "Training Accuracy: 97.9%\n",
            "Testing loop: \n",
            " Accuracy: 61.2%, Avg loss: 2.028648 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loop: loss: 0.127989  [    0/ 4000]\n",
            "Training Accuracy: 91.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 1.845036 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loop: loss: 0.138486  [    0/ 4000]\n",
            "Training Accuracy: 97.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 1.998397 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loop: loss: 0.134471  [    0/ 4000]\n",
            "Training Accuracy: 98.2%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.089565 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loop: loss: 0.121624  [    0/ 4000]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.140980 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loop: loss: 0.238716  [    0/ 4000]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 2.136053 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loop: loss: 0.232756  [    0/ 4000]\n",
            "Training Accuracy: 98.3%\n",
            "Testing loop: \n",
            " Accuracy: 61.8%, Avg loss: 2.081385 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loop: loss: 0.091370  [    0/ 4000]\n",
            "Training Accuracy: 98.4%\n",
            "Testing loop: \n",
            " Accuracy: 57.6%, Avg loss: 2.145858 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loop: loss: 0.055475  [    0/ 4000]\n",
            "Training Accuracy: 94.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.029888 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loop: loss: 0.057479  [    0/ 4000]\n",
            "Training Accuracy: 98.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.115506 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loop: loss: 0.061024  [    0/ 4000]\n",
            "Training Accuracy: 98.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.4%, Avg loss: 2.236483 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loop: loss: 0.125522  [    0/ 4000]\n",
            "Training Accuracy: 98.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 2.049998 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loop: loss: 0.096346  [    0/ 4000]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.8%, Avg loss: 2.088511 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loop: loss: 0.081972  [    0/ 4000]\n",
            "Training Accuracy: 98.9%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 2.157345 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loop: loss: 0.088328  [    0/ 4000]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 2.096979 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loop: loss: 0.096067  [    0/ 4000]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.163484 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loop: loss: 0.106974  [    0/ 4000]\n",
            "Training Accuracy: 98.7%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.129509 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loop: loss: 0.094200  [    0/ 4000]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.160342 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loop: loss: 0.059950  [    0/ 4000]\n",
            "Training Accuracy: 98.8%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 2.512780 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loop: loss: 0.291011  [    0/ 4000]\n",
            "Training Accuracy: 88.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 1.963627 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loop: loss: 0.113902  [    0/ 4000]\n",
            "Training Accuracy: 98.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 2.103455 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loop: loss: 0.077469  [    0/ 4000]\n",
            "Training Accuracy: 99.0%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 2.189196 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loop: loss: 0.074993  [    0/ 4000]\n",
            "Training Accuracy: 98.9%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 2.143665 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loop: loss: 0.073599  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.6%, Avg loss: 2.168301 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loop: loss: 0.060919  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.153275 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loop: loss: 0.062033  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 2.167532 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loop: loss: 0.058998  [    0/ 4000]\n",
            "Training Accuracy: 99.1%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 2.167337 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loop: loss: 0.060165  [    0/ 4000]\n",
            "Training Accuracy: 99.5%\n",
            "Testing loop: \n",
            " Accuracy: 61.0%, Avg loss: 2.202378 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loop: loss: 0.104844  [    0/ 4000]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 59.4%, Avg loss: 2.185119 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loop: loss: 0.094466  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 2.217677 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loop: loss: 0.059089  [    0/ 4000]\n",
            "Training Accuracy: 82.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 1.852535 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loop: loss: 0.195915  [    0/ 4000]\n",
            "Training Accuracy: 96.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.038756 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loop: loss: 0.147078  [    0/ 4000]\n",
            "Training Accuracy: 98.7%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 2.089669 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loop: loss: 0.116719  [    0/ 4000]\n",
            "Training Accuracy: 98.0%\n",
            "Testing loop: \n",
            " Accuracy: 61.4%, Avg loss: 2.095227 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loop: loss: 0.096106  [    0/ 4000]\n",
            "Training Accuracy: 98.9%\n",
            "Testing loop: \n",
            " Accuracy: 58.0%, Avg loss: 2.165410 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loop: loss: 0.089026  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.4%, Avg loss: 2.107909 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loop: loss: 0.080646  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.137023 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loop: loss: 0.046751  [    0/ 4000]\n",
            "Training Accuracy: 99.2%\n",
            "Testing loop: \n",
            " Accuracy: 58.6%, Avg loss: 2.208935 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loop: loss: 0.074048  [    0/ 4000]\n",
            "Training Accuracy: 99.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.0%, Avg loss: 2.184827 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loop: loss: 0.043168  [    0/ 4000]\n",
            "Training Accuracy: 99.4%\n",
            "Testing loop: \n",
            " Accuracy: 59.8%, Avg loss: 2.203751 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loop: loss: 0.058137  [    0/ 4000]\n",
            "Training Accuracy: 99.5%\n",
            "Testing loop: \n",
            " Accuracy: 59.2%, Avg loss: 2.279596 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loop: loss: 0.063729  [    0/ 4000]\n",
            "Training Accuracy: 99.3%\n",
            "Testing loop: \n",
            " Accuracy: 60.2%, Avg loss: 2.245864 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loop: loss: 0.058219  [    0/ 4000]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.251106 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loop: loss: 0.055418  [    0/ 4000]\n",
            "Training Accuracy: 99.6%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.250780 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loop: loss: 0.069413  [    0/ 4000]\n",
            "Training Accuracy: 99.5%\n",
            "Testing loop: \n",
            " Accuracy: 60.0%, Avg loss: 2.201062 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gUVffHP7Ml2SQkmx5SSIOElkIJJfTeQZGmIK+igooillfF8tpFf4IFG4pYQKUqiAjSBKRKDy0QkpCENFJI71vm98cmS0ISAySQAPfzPHmyuzNz5+zu7PeeOffccyVZlhEIBALB7YWisQ0QCAQCQcMjxF0gEAhuQ4S4CwQCwW2IEHeBQCC4DRHiLhAIBLchqsY2AMDZ2Vn29fVtbDMEAoHgluLIkSOZsiy71LStSYi7r68vhw8fbmwzBAKB4JZCkqSE2raJsIxAIBDchghxFwgEgtsQIe4CgUBwG9IkYu41odPpSEpKoqSkpLFNEdxANBoNXl5eqNXqxjZFILitqFPcJUn6DhgFpMuyHFT+miOwEvAF4oGJsixnS5IkAQuAEUAR8KAsy0evx7CkpCRsbW3x9fXF1KzgdkOWZS5dukRSUhJ+fn6NbY5AcFtxNWGZH4BhV7w2B/hLluUA4K/y5wDDgYDyvxnAwus1rKSkBCcnJyHstzGSJOHk5CTuzgSCG0Cd4i7L8i4g64qX7wKWlD9eAtxd6fWlsol/AHtJktyv1zgh7Lc/4jsWCG4M1xtzd5NlObX88UXArfyxJ5BYab+k8tdSuQJJkmZg8u7x9va+TjMEAoHg5pNfls+hi4fo5dmL/LJ8nKycqu2TW5pLib4EV2tXzmadRUYmIS+BlvYt2Z+yH6NspKV9S0KcQ7DX2De4jfUeUJVlWZYk6ZqLwsuyvAhYBBAWFtbkisrn5OSwbNkyZs6cec3HjhgxgmXLlmFvX/sX9tprr9GnTx8GDRpUHzMFAkE9kGUZg2xApTBJYXpROskFyXR07QhAqaGUxLxEdEYdu5J2kV+Wj9ZSy9aErZzJOmNuZ+fEnVUEfvnZ5bx/8H2MshFXa1fSi9JrteGlri8xue3kBn9v1yvuaZIkucuynFoedqmwPBloUWk/r/LXbjlycnL48ssvaxR3vV6PSlX7R7dx48Y623/rrbfqZV9jUNf7FgiaKmWGMlQKFQqpaiT61b2vEp0dzcf9P2Zz/Ga+P/U9OaU53N3qbmaGzuTxbY8Tmxtr3l+j1FBiKMFKZcUTHZ7gmxPfUGYsI6c0B4WkYGnkUgLsA5h7YC59vPrQybUT+1P281DQQ7hYueBm48apzFN0d++Oq7Ur57LP4dXM64a85+v9pf4OPAC8X/5/XaXXn5QkaQXQDcitFL65pZgzZw6xsbF06NCBwYMHM3LkSP73v//h4ODA2bNnOXfuHHfffTeJiYmUlJQwe/ZsZsyYAVwup1BQUMDw4cPp1asX+/btw9PTk3Xr1mFlZcWDDz7IqFGjGD9+PL6+vjzwwAOsX78enU7H6tWradOmDRkZGUyePJmUlBTCw8PZunUrR44cwdnZuYqtjz/+OIcOHaK4uJjx48fz5ptvAnDo0CFmz55NYWEhlpaW/PXXX1hbW/Piiy+yadMmFAoF06dPZ9asWWabnZ2dOXz4MP/973/ZuXMnb7zxBrGxsZw/fx5vb2/ee+89pk6dSmFhIQCff/45PXr0AOD//u//+Omnn1AoFAwfPpzp06czYcIEjh41JUxFR0czadIk83OB4EaxK2kXhbpCAuwDWHBsATsTdzLMdxghLiHsTdlLsa4YvaznRMYJAO767S5KDaUEOQUR4hLCxvMbcbZyJj4vnjld56CUlAz3G47WUkupoRRZltGoNAQ6BDJ7x2zKDGX8cf4PFp9cDICf1o9P+n2CWqnm4eCHq9gW6hJqftzZrfMN+wyuJhVyOdAPcJYkKQl4HZOor5Ik6WEgAZhYvvtGTGmQMZhSIac1hJH/d/D/OJt1tiGaMtPGsQ0vdn2x1u3vv/8+p06dIiIiAoCdO3dy9OhRTp06ZU7b++6773B0dKS4uJguXbowbtw4nJyqxt6io6NZvnw533zzDRMnTuTXX3/l/vvvr3Y+Z2dnjh49ypdffsn8+fNZvHgxb775JgMGDOCll15i06ZNfPvttzXa+u677+Lo6IjBYGDgwIGcOHGCNm3aMGnSJFauXEmXLl3Iy8vDysqKRYsWER8fT0REBCqViqysK8fKqxMZGcmePXuwsrKiqKiIrVu3otFoiI6O5r777uPw4cP8+eefrFu3jgMHDmBtbU1WVhaOjo5otVoiIiLo0KED33//PdOmNcglIRBUoURfwrM7n6W/d3+6u3fn6R1PozPqALBUWgJwMvMkMTkxxOTE0FLbkqySLNys3fCx8yHyUiRLhi2hvXN7vjr+FbuSdpFdko3WUsuUtlOqnKuiPQALpQVgCt9UdBQKScELXV5ArWzcuRt1irssy/fVsmlgDfvKwBP1Naqp0rVr1yr52J9++ilr164FIDExkejo6Gri7ufnR4cOHQDo3Lkz8fHxNbZ9zz33mPdZs2YNAHv27DG3P2zYMBwcHGo8dtWqVSxatAi9Xk9qaiqRkZFIkoS7uztdunQBwM7ODoBt27bx2GOPmcMrjo6Odb7vMWPGYGVlBZgmlz355JNERESgVCo5d+6cud1p06ZhbW1dpd1HHnmE77//no8++oiVK1dy8ODBOs8nENTF+tj1FOuLGeo7FDsLO17b+xq7k3eTVpTGwdSDqBQq3u/9PiWGEjq6dOSX6F9YGrkUjVLDva3v5ZXur5Bflo/OqMNaZU2RvghHjematVaZruGM4gzz49qwUJjEvcxQxtG0owz3G84r3V5Ba6m9sR/AVXBLBFD/zcO+mdjY2Jgf79y5k23btrF//36sra3p169fjfnalpaXe3mlUklxcXGNbVfsp1Qq0ev1V21TXFwc8+fP59ChQzg4OPDggw9eV964SqXCaDQCVDu+8vv++OOPcXNz4/jx4xiNRjQazb+2O27cOPMdSOfOnat1fgLB1aAz6lhyegl/J/7N5LaTeXnPywDE5cbR0r4lf8b/CYDWUsvRtKMM9hnMEN8h5uM9m3miN+opMBbgZWuKcdta2Jq3a1SXr2Mbtel6zyjKwFpdh7iXe+5xuXGkF6fTybVTkxB2ELVlasXW1pb8/Pxat+fm5uLg4IC1tTVnz57ln3/+aXAbevbsyapVqwDYsmUL2dnZ1fbJy8vDxsYGrVZLWloaf/5pushbt25Namoqhw4dAiA/Px+9Xs/gwYP5+uuvzR1IRVjG19eXI0eOAPDrr7/WalNubi7u7u4oFAp+/PFHDAYDAIMHD+b777+nqKioSrsajYahQ4fy+OOPi5CM4KoxGA28tf8tRqwZwX/+/A+T/pjEgqMLiMiIYN6heeb9Ii9F8snRT+javCv9W/QnuySbrJIsXKyqljj3bOZpflwh7rVhFvfiDPPj2qgQ9xOZppBMsEvw1b/JG4wQ91pwcnKiZ8+eBAUF8fzzz1fbPmzYMPR6PW3btmXOnDl07969wW14/fXX2bJlC0FBQaxevZrmzZtja2tbZZ/Q0FA6duxImzZtmDx5Mj179gTAwsKClStXMmvWLEJDQxk8eDAlJSU88sgjeHt7ExISQmhoKMuWLTOfa/bs2YSFhaFUKmu1aebMmSxZsoTQ0FDOnj1r9uqHDRvGmDFjCAsLo0OHDsyfP998zJQpU1AoFAwZMqS2ZgV3IPll+Xx78lvyy0xOVKGukKe2P0V8bjzxefGsPrcaFysXJCSslFbM7TWXEX4jyCjOAGCo71AiMiLILc1lTMsxOGocScpPQi/rzSGWCqqIex3ZKRXeelZJVp2ee0X8Pac0BwA7td01fAI3GFmWG/2vc+fO8pVERkZWe+1Oo6SkRNbpdLIsy/K+ffvk0NDQRrbo+pg3b5786quv1rpdfNd3HkajUX5u53Ny0A9B8keHP5JlWZa3J2yXg34IkpecWiLvuLBDDvohSI5Ij6hy3KqoVXLQD0HyiF9HyCvOrJCDfgiSg34IkuNy4uRPjnxifr4+dn2V40r0JeZtBWUF/2rbodRD5n2f3fHsv+57IfeCHPRDkDx141Q56IcgOa0w7To+jesHOCzXoqu3RMz9TuXChQtMnDgRo9GIhYUF33zzTWObdM2MHTuW2NhYtm/f3timCG4iZYYyIi9FEuoSavam/7f3f7zZ403TDM3U/WyO34yjxpEVZ1cwrf00jmUcAyA+L95clsLbturs9YrUwSDnIAIdAwGwt7THx84He8vLkwav9NwtlZa4WrmiM+rqDLVU3n61YZm80jzzeZoKQtybMAEBARw7dqyxzagXFdk+gjuLJaeX8OmxT2ll34qYnBhmdZzF8YzjPPHXEywbuYxfz/2KvaU9Cwct5L4N9/FFxBdEZUcBkJCXgFJS0kzdrIpgA/jZ+THSfySj/UcTYB8AmPLGJUn6V3EHaGHXAr2x7mSFyqGYqx1QrQgtVR6YbWyEuAsEggYnPi8egJicmCr/04rSeHzb45zLPse9re+lnVM77m19L8vOLrt8bG48aqWaFrYtqhWWkySJ93u/b37+YPsH6ebeDQAHzeVU4ZrE/Y3wN5Cpu9JJZW+9zlTICnHXmcS9IjWyKSAGVAUCwXUTlRXFj5E/Ygr/Yv6fkFd13eak/CQA5vaaS1xuHPaW9kxsbZr7+GTHJ+nj1QeAcPdw0ovTOZd1jha2LaiL58Keo5dnL4AqnruDZfU5Ib5aX/y0da8bUFnQrzYsU6wvxlJp2aSqnArPXSAQXBVG2YhBNqBWqFkfu56lkUvJKs4ivTidYOdg0ovSee/ge/x212/E5MQwqfUk+rXox+PbHudC/gWaqZsx3G84Q32HVqnxYmthyxcDv8BgNLA9cTv7U/eTUZxxVeJemQpBt7Owq9fsUCuVFRISMnKd4q6SVOZ9m1K8HYTnLhAIrpKvT3zNuN/HIcsya2PWcjbrLDmlOVirrFkZtZJDFw+RWZzJyqiVFOoKCXQIJNjZlPedW5qLnYUpTfDK4l0VKBVK/LX+5uct7Vtek30VZXNrCslcC5IkmWPtdcXcJUkyi7pG2XTi7SDEvVYqqkJeL5988ol5Qg+YygDn5OQ0hGkCwQ0lpSCF8znnq71+9tJZ4nLjiM+L51TmKcYFjGPjPRu5q9VdbI7fTGRWJACfHfsMgECHQJqpmyFhClXYWdadA+6v9Wd+3/ksGryIEX4jrsnuZupmqCRVvcUdLodmbFT/7rkD5rsES5Xw3G8JGlrcN27c+K/13Zsi11IGQXD78M4/7zB7x+xqr1fUJF92ZhnF+mJ6ePTAzcaNXp690Bl15sJZAB42HrR2bI1SoTRP89da1D0tX5IkhvoOJdwjHKWi9sl0tR1rr7FvEHGvCMfU5bnD5fRHEZa5Rahc8rdihuq8efPo0qULISEhvP766wAUFhYycuRIQkNDCQoKYuXKlXz66aekpKTQv39/+vfvD5im92dmZhIfH0/btm2ZPn067du3Z8iQIeZ6M4cOHSIkJMR8zqCgoGp2FRQUMHDgQDp16kRwcDDr1q0zb1u6dKl55unUqVMBSEtLY+zYsYSGhhIaGsq+ffuIj4+v0vb8+fN54403AOjXrx9PP/00YWFhLFiwgPXr19OtWzc6duzIoEGDSEtLM9sxbdo0goODCQkJ4ddff+W7777j6aefNrf7zTff8MwzzzTUVyK4ScTkxBCfF09OSdU7zQpxXxG1Aricc165hO3EwIlMD57OylErsVKZis1V1Fq5Gs+9vjwe+rh5oLY+VIh6XTF3uJwh09TCMrfEgOqb608TmZLXoG2287Dj9dHta91+ZcnfLVu2EB0dzcGDB5FlmTFjxrBr1y4yMjLw8PBgw4YNgKn2ilar5aOPPmLHjh3Vaq9D7WWAp02bxjfffEN4eDhz5sypdhyYarWsXbsWOzs7MjMz6d69O2PGjCEyMpJ33nmHffv24ezsbK7t8tRTT9G3b1/Wrl2LwWCgoKCgxho1lSkrK+Pw4cMAZGdn888//yBJEosXL+aDDz7gww8/5O2330ar1XLy5Enzfmq1mnfffZd58+ahVqv5/vvv+frrr+v4JgSNiVE2FYtTSAqSC5Ip1BWSWmhagiEiI4K/k/7mVOYpZoTMILMk03zcYJ/B5pWHtJZafO18ic+Lp4dnDwZ6Vy0Yq7XQkkiiOeZ+I2kIYYdKYZmrEffyjJmmFpa5JcS9KbBlyxa2bNlCx46m5bcKCgqIjo6md+/ePPfcc7z44ouMGjWK3r1719lWTWWAc3JyyM/PJzw8HIDJkyfzxx9/VDtWlmVefvlldu3ahUKhIDk5mbS0NLZv386ECRPMnUlFyd3t27ezdOlSwFRxUqvV1inukyZNMj9OSkpi0qRJpKamUlZWZi55vG3bNlasWGHer6Ic8YABA/jjjz9o27YtOp2O4OCmU0hJcJllZ5ahUWlYF7MOJysn3uv9Hg9vfpjsksvXxqztswBwsXLh+b+fxygbCXcPx9bClnd6vVOlvQ6uHYjPi68yIFqB2XO/CeLeUJjDMnXkucNlcRee+3Xwbx72zUKWZV566SUeffTRatuOHj3Kxo0befXVVxk4cCCvvfbav7Z1tWWAa+Lnn38mIyODI0eOoFar8fX1veYSv5XL+8K/l/idNWsWzz77LGPGjDGvzPRvPPLII8ydO5c2bdqIKpBNlIuFF/ng0AcYZIP5NaWkJLmg+oqYM0Nn0tymOa/tM13T97e735yTXplR/qPILM6sMX2xIhxzM8IyDcXVZsvA5bCMiLnfIlxZ8nfo0KF89913FBQUAJCcnEx6ejopKSlYW1tz//338/zzz5uXkKurZPCV2NvbY2try4EDBwCqeMWVyc3NxdXVFbVazY4dO0hIME0WGTBgAKtXr+bSpUvA5ZK7AwcOZOHChQAYDAZyc3Nxc3MjPT2dS5cuUVpaWuMdQuXzeXqaKuotWbLE/PrgwYP54osvzM8r7ga6detGYmIiy5Yt4777alvnRdCYrIxaiYxMmFsYI/1HYqO2YVP8Jro0Ny3sIiExu9NsRvuPZkbIDIKcL4/PuFq71thmN/duLBy00LzQdGUqBlJvJc9dhGVuYyqX/B0+fDjz5s3jzJkz5rBJs2bN+Omnn4iJieH5559HoVCgVqvNQjpjxgyGDRuGh4cHO3bsuKpzfvvtt0yfPh2FQkHfvn3RaqtnF0yZMoXRo0cTHBxMWFgYbdq0AaB9+/a88sor9O3bF6VSSceOHfnhhx9YsGABM2bM4Ntvv0WpVLJw4ULCw8N57bXX6Nq1K56enuY2auKNN95gwoQJODg4MGDAAOLi4gB49dVXeeKJJwgKCkKpVPL666+bV5OaOHEiERERta4cJbj5lBnKOH3pNJZKS1acXUH/Fv35pP8nAGy/sJ28sjzGtBzDqLWjkGWZR4IfMR/rr/XHSmVFsb64VnH/N27mgGpD4aBxwFZtW2NndSVmcW9innujl/uVRclfM/n5+ebH7733nvzUU081ojXXz8iRI+Vt27Zd9f534nd9M8krzZNHrRllLmPbcWlHOT43vsZ9tydslzee31jt9f9s/I/ccWlH2Wg0XvP5l55eKgf9ECTvTd57zcc2FlnFWfKpjFNXte8T256Qg34Ikt/e//YNtqo6iJK/twYbNmzgvffeQ6/X4+Pjww8//NDYJl0TOTk5dO3aldDQUAYOrLbEruAmUWooZcaWGYQ1D8PFyoUN5zeQmJ/IWz3e4uDFg3R07YiPnU+Nx/b37l/j62NajsGzmed11U6p8NyvJs+9qeCgcahSiOzfEAOqgjqZNGlSlUyVWw17e3vzgtmCm0uxvpgH/nyAh4IfIq0wjaPpRzmabhr/cbVy5aWuLzE2YCxjA8ZeV/vjAscxLnDcdR3b06MnU9pOIdAh8LqOb+qoFU1zhmqTFndZlptUlTVBwyPLdZdgFVRFb9Tz85mfGeo7lBMZJ1gauRRXa1fOZJ1hS/wWDl08RLh7OF3du+Jr58sgn0GNaq+TlRNzutY8b+N2oKnWlmmy4q7RaLh06RJOTk5C4G9TZFnm0qVLaDRN60fRlNAZdexK2kW4e7g5LW9T/CbmH57PirMrSCpIQqPUcDzjOAA7LuxAL+sZHzieIb5izdqbQVMdUG2y4u7l5UVSUhIZGRmNbYrgBqLRaPDy+vcFi+9USg2lPLHtCQ5cPEBLbUsWDVmEs5Uz3578Fo1SQ1JBEu2c2vHZgM94Zc8rqBQq9iTvQUKia/OujW3+HYM55t6EVmGCJizuarXaPBtSILgTiMuN49mdz/LZgM/wsvViW8I2Dlw8wAPtHmBF1ArmH5pPuEc4MTkxzO01lwJdAf1b9MfV2pVvhnzDsfRj7EneQ1untubyt4IbT1OdxNRkxV0guNPYmbiTmJwYFhxdQEZxBkW6IlytXHk27Fk0Kg1fn/ianUk76eDSgZH+I6vVRW/r2BYbtU2NM0gFNw5zVUgxoCoQCCooKCtgXew6RvqNNMfNN8VvMm+/t/W9KCQFDwU9RF5ZHlklWTzZ4ckaF7zQqDT8dtdvDVLyVnD1VNRzFwOqAoHAzJLIJXx1/CsWnVhEVkkWtmpb8nX5DPIexO7k3YxuORow1Th5udvLdbbX3Kb5jTZZcAUiLCMQCKqgN+pZE72GYOdgTmaaSifP7DATJysnBvkMQkK6qunvgsZFLNYhENzh6I16UgpSzM9XRq0kvSidh4Me5rVwU9XFHp49GO43HLVCLYT9FkEUDhMI7kCis6PZkrCFCYET2JqwlfmH5rN69GoisyJ5/+D79PbsTd8WfVEpVIz2H93k0ukEdVNROdJWbdvIllRFagozBMPCwuSKlX8EgtuBtdFrWRG1guySbFILU9FaavFq5sXpS6dp69iW6JxoOrt25stBX5o9P8Gtic6gY3/q/kbJUpIk6Ygsy2E1bROeu0BQD/Yk7yEhL4EpbadwPvc8v8f8jpXKiiWnl1CgK0ApKXm528vMPTCX3NJcHCwdOJN1hi7Nu/Bhvw+FsN8GqJXqJpl+Wi9xlyTpGeARQAZOAtMAd2AF4AQcAabKslxWTzsFgiZDib4Eo2xEqVDy+t7XySzJZECLAXx+7HO2JWxDRkYpKVk+ajlaCy1etl78Gfcnx9KP8X7v93GzccNf6y/KaghuKNct7pIkeQJPAe1kWS6WJGkVcC8wAvhYluUVkiR9BTwMLGwQawWCJsDLe17maNpRhvgOIb04HTANju5P2c89AffwcPDD5JXm0d7p8vKQj4U+xnenviOseZjw1gU3hfqGZVSAlSRJOsAaSAUGAJPLty8B3kCIu+AWxSgbkZCQJImEvAQyizPNxbmWn13OIO9BlBnL+PbUtwD09OxpWkf0irG1Hh496OHRoxHegeBO5brFXZblZEmS5gMXgGJgC6YwTI4sy/ry3ZIAz5qOlyRpBjADwNvb+3rNEAganCJdEWqFGrVSzewds9EZdYwLGMcre16hWG9azHxur7loLbX09uxNQl4Cu5J2Aaa1RAWCpkB9wjIOwF2AH5ADrAaGXe3xsiwvAhaBKVvmeu0QCOpDRlEGtha25hTEi4UXuX/j/dhZ2vFE6BPsTNwJwN7kvQQ5BZFblouVyopR/qPMMXNfrS9Lhy8lNif2lloEWnB7U5+wzCAgTpblDABJktYAPQF7SZJU5d67F5BcfzMFgobHKBsZ/dtorFXWbLhnAzqjjse3PU6BroDskmye3vk0DpYODPcbTqmh1LzgRKmhtNpgaEfXjnR07dgYb0MgqJH6iPsFoLskSdaYwjIDgcPADmA8poyZB4B19TVSIGhIcktzsVZZE5sbS6GukEJdIaPXjkalUJFWlMbCQQvRWmjZnridUJdQenn2qnK8mGgkuBWoT8z9gCRJvwBHAT1wDFOYZQOwQpKkd8pf+7YhDBUIrpeMogycrJxQSApyS3MZsWYEtha2tHNqB8CbPd5kZ+JO9EY9L3V9ie7u3QFo69S2Mc0WCOqFmKEquG05fPEwSoWShzc/zD0B95BckEyxvpgjaUdws3YjrSgNa5U1B6YcaGxTBYLrQsxQFdxxRGdHM23zNCQkZGRWRq00b+vp0ZPnuzzP3evuZqjv0Ea0UiC4cQhxF9w2lBpKzWVXN8ZtRCkpsbe0Z3zgeH6M/JHRLUcT6hJKmFsY7s3c2Tp+q8huEdy2CHEX3PLojXq+jPiS7099zwj/EexL2UdBWQHdPbqzcOBCJEliarup2FnYVclyEQtbCG5nhLgLblmSC5KZvX02BtlATE4Mrexb8Xvs73g280SlUTGlzRSzmGsttY1srUBwcxHiLmjyVJRUddI40dK+JTmlObx34D3yyvKIy43D286bt3u+zWj/0Wy9sJVw93Ah5oI7HiHugiZHqaGUvcl7KdQVsjJqJTZqG/al7ANMixC3dmxtXkz6oaCHeKbzM+Zjh/le9SRpgeC2Roi7oMmx5PQSPjv2GWAS8xJDCTNCZtDWsS0Ljy/keMZx7m51N20c2zC21dhGtlYgaJoIcRc0KYyykbXRa2nv1J4pbafQv0V/YnJiCHUJRZIkgpyDWHJ6CTNCZuCgcWhscwWCJosQd8FNR5ZlorKj2J20m1H+o9ieuJ2c0hw2nN+As5UzSQVJzOwwk9EtRwPQwbWD+djmNs15seuLjWW6QHDLIMRdcEP5LeY3TmWe4tXurwKw+ORi1kSvQSEpSMhLYNnZZWQWZwLQ2a0zOoOOvl59GeQzqDHNFghueYS4C24YOoOOBUcXkFmcyYTACThZOfH18a8pMZQA4KhxJLM4k6c7PU1nt87m0ItAIKg/QtwFDc6+5H2kFaWhUWnMXvnC4wvJLM5EZ9Qxr888Mosz6enZk91Ju5nabqoQdYGggRHiLmhQDEYDb+5/k4tFF2lu3RxfO1/aObVjY9xGbNQ2vN/n/Srpin5av0a0ViC4fRHiLqgXOoOO1/e9Tn5ZPsP8hhGbE0tKYQoSEimFKXwx8AvC3ML4T/v/4G3rja2Fbd2NCgSCeiPEXXDNrI9dj1qpZpjvMF7d+yob4zaiUWrYmbQTAFcrV3bM7q8AACAASURBVJ4Le46E/AT6ePUBoL1T+0a0WCC48xDiLrgm0grTeG3fa+iNenYl7mJj3EYeC32MSa0nkVGUgd6oR2upxdtOLHouEDQmQtwF18TSyKXIssyAFgNYf349LWxb8EjwI1gqLXG2cm5s8wQCQTlC3AW1Issy8w7Po8xQRrh7ON523qyKWsUIvxHM7T2X87nnsVZZm2uoCwSCpoMQd4EZWZaZf3g+fb36siNxB8X6Yn6N/hWFpDCvZKRSqJjZYSYA/lr/xjRXIBD8C0LcBWY2x29maeRSfo/9nZzSHAB87HxYNnIZ0dnRLD65mM5unfGy9WpkSwUCQV0Icb9DkWWZxPxEPJt5klKQglKh5OMjH2OjtiGnNAdHjSNPdHiCEJcQ7Czs6OzWmc5unRvbbIFAcJUIcb8DScpPYuZfM4nLjaOfVz/2JO/BIBtQK9R8M+Qbvoz4kqF+Q5kQOKGxTRUIBNeJEPc7jGJ9MY9ufZSc0hz6efVjZ9JOnK2c6ezWmVH+o+jk1onFQxc3tpkCgaCeCHG/A8goyuCXc78wPnA8u5J2cSH/Al8N+oqOrh15Y/8b3BNwD93duze2mQKBoAER4n4bI8syv8X8xrzD88gvyyciI4KM4gwCHQLp4dEDSZL4oM8HjW2mQCC4AQhxv01JL0pnTfQavoj4wlxO97tT3wEwt9dcUYVRILjNEeJ+m2AwGtibshe9Uc/3p74nIiMCgOF+w3m/9/sA+Nr5EuAQQJBzUGOaKhAIbgJC3G9xdAYdWxO2EpcXx1fHvwLA3cadpzs9jbedN/28+qGQFACMDRCLSQsEdwpC3G9R9iTvYc7uOUxqPYlFJxYB0NerL+MDxxPuES5KAggEdzhC3G8xskuy2Ry/mfXn15NbmmuOow/2GcycrnNwtXZtZAsFAkFTQIj7LcY7/7zDloQtACgkBXqjnomBE/lf+P8a2TKBQNCUEOJ+C5Bfls+e5D3E5sSyJWELg30Go1FqcLF24btT39HLs1djmygQNFnOpeWjVEi0dGnW2KbcVIS4N3GyS7KZvmU6UdlRAPRr0Y+5veaiUWnIKcnBSmV1R4u7zmBk0a7z3N/dB62V+oac4+W1J9EbjHwwPvSGtH+7kZ5fwu8RKTzcyw9JksgpKuPnAxd4tI8/KqWiXm3rDUZGfrqHqeE+3N/dp8q2ojI9liolSkXVNN/nfzlBM0slPz9y4yfqFZbq+WFfPNN7+2Ohqt97rS/1OrskSfaSJP0iSdJZSZLOSJIULkmSoyRJWyVJii7/79BQxt4pyLLMpeJLJOUn8dDmh4jPi+ejfh+xdfxWPhvwGRqVBgB7jT2PhT6GWnljRK0h+ftcBu9tPIMsy9d8rNEoczwxB1mWkWWZIwlZ5nb2xGQyb3MUa48mNbTJZnaeTee3YykUlupv2DmuhzK9kVnLj3Hfon84diH7pp+/oFTPmdS8aq//diyZdzacITq9AIBlBy8wb3MUB+Ky6n3OM6n5RKXl89XfsRiNl6+l3GIdgz78m+dXH692TGJWEYlZxdVeT88rIS6z8LrsiE7L57+rj1OqN1R5fcPJVOZtjuJgA7zX+lLfrmUBsEmW5TZAKHAGmAP8JctyAPBX+XPBVaIz6nhqx1P0W9WP4WuGk1yQzOcDP2ewz2Ca2zRvbPOq8N7GM3y89dxV7fvcquN8ves8+89fqrZtZ1Q64xbuo0RX9YdSojPwy5EkPtp6jru+2MuXO2PZeS6DcQv3893eeIYv2M3P/yQAcDC+5h+TwSjz65Ek0vNKrsrO4jIDvx1LNotWQamelNwSygxGdkdnXFUba48l8Z/vDtbZkeUUlbHmaBIG47V3eABbI9NYfzyFA3GXWLo/wfz6le3NWn6M5Qcv8OzKCBbvPl9lm8EoVxFJo1G+6g74g01nueuLvRSVVe30krNNQhp1MR+AbZFpAOYOqERnYMXBC+gMxlrbNl7xHir2PRBnun6SsosZ+NHfrD+eAsBHW6JIyS1hbUQyMen55uOKywxkFZaRmltcrc1nVkUwfuE+jiRks/n0xSrbLlwq4tO/ovnzZCppeSVsOpVaZfuygxf45UgSm05dZNWhRHPbJ5NyAYjNKOCXI0nkleiqvTf9v7zvhuS6wzKSJGmBPsCDALIslwFlkiTdBfQr320JsBN4sT5G3gn8FPkTOxN3olKq2Ju8l/va3IeDxoGxrcY2OVEH093FqsOJONhY8MzgQADySnToDTKONhbm/XZHZ3A+oxA7KxWZBaV8vj2GHi2rLse37MAFjiRks/1sOiOC3UnOKebH/Qn4OlkzZ81JAJpZqpi/JYouvo4AzN14BoNRNovwwTiTN1955q0syzz64xG2nUkj3N+JZdO7/evM3NwiHWMX7uV8RiHd/Bz56v7OnErJNW//7VgKQ9s3J69ET9TFfFq62ODUrHrK6aJdcZxJzWPtsWQSLhXx9KCAaufVGYzM+PEIB+OyyCvW8WBPv1rtSswqwsvBCkmSMBhl5m+JoqVLM1YcvICnvRVdfB3YfjYdvcFISk4JkxbtZ3pvfx7q5UdusY71x1OITS/g7MU8tkelc393H/44kYqtRsX3e+OIzShkzrA2jOvsxX9/OU5+iZ5v/hNmPv/BuCwiU3Jp5WpLVFo+U7p5k55Xyh8nUinTGzmRlEt3fyfz/sk5l8W9u38pxxJNawMcvWD6/86GSH765wL21mqGBbmbj0vJKaa5nYb8Ej2hb23hnbuDmBDmxf9+O8Vvx1J4bkgghxOy8bS3wlaj4uzFfPbFZtIn0IXlBxMZEdycHWczmPrtQTp5O4AEIZ7a8s9b5od98Ry5kE3/1q70DnBmX+wlZBnGf7UPWYZerZzRWqsJ8dSSklPMkvIO09vRmgtZRTwzKBC90cjTgwLZHZ0JwAu/nKBUb0ShkDiTmmf22BfvOU9iVjH2G9TseK4fDjYWpOQUU1RmYPRnewhsbouXgxUA93ZpQe8Al1q//+ulPjF3PyAD+F6SpFDgCDAbcJNluaKbuwi41XSwJEkzgBkA3t533mLK+5L3EZ8Xz+S2k9l4fiP/d+j/UClUyLLMq91eZVKbSTfs3HuiM+nkY4+1xdV9/aeSc3FqZoG71nQx/t+ms+j0RrKLdOSV6CnVGzhwPovnVh/HQqlg67N9+PVoMscSsllzLBkAjdp0k7j//CX+PJnKlsg0PpwQWu4Rm34oH289x7qIZM6lFRCXWUi/1qYLPtzfibfuas+Yz/eafzwGo4wkgSyDm50laXmlnM8sNA+a7YnOxMFGzbYzaXRoYc/+85cY8vEuXhrRhgFt3IhIzMHLwQrnSuI8Z80JErOKcLKxIDajkEeWHuZIgsnbHNrejU2nL/L676dJzCpiR1QGDtZqfn+yFy0crc1txKTnmzucF345gd4o4+1ojaeDVRUB/GJHDAfjsvB2tObDLee4u6Mn9tamTvFcWj6vrD1JmUHm/m7ePP/LCT69ryNjQj2IupjPwp2x5naeH9oaf2cbfotI4XBCNm//EUlqbgk//pPAtJ6+nE42dU6R5TblFOnYeDKV/1YKXzS30/DSmpO087Bj48lUrC1ULD94gfjMQl4a0ZY3fj9tPh4gt6iMT7fHmJ8fvZCNq60lF7KK6NfalaQKzz0tn9+PpyDLEOKlZfvZdB747qD5Duh8ZiEGo8z2s+l4O1oz9JNddGhhz9yxwQC8+tspTiblsupwEi1dbPho6znUSgXDg5ozb0IoQz7+m+xCHZtPX6TMYGRGn5ZM7e7Lwr9jOXsxj4u5JWw6ddkjf7fcIdgXk8nMfq2QZWjtZktqbjH3dPJid3QGkal5bDl9kZ6tnPF3sUFvkLmQVUQzSxUfbzPdpeoMMjHpBagUEqV6kxc+51fTd11BRRgop0jHL0eS8HW2YfrSw+brTZZlzpZ/ptlF1b37hqA+4q4COgGzZFk+IEnSAq4IwciyLEuSVOM9nizLi4BFAGFhYdd3X3qLUrE2aWxOLJZKS9498C6d3Trzcb+PKSgroIVdiwY/Z16JDltLFadT8rj/2wO8NqodD/Wq3VusQG8wMmXxAXq1cuaLKZ04k5pXRVwMRpmp3x7kYFwW7loNyTnF/Hf1cbZGpqEzXP5aS3RG+rd2YUdUBu9sOENyTjH3dPJEZzBSrDPQ2s3kFVbEaQEOxWXhaW/F8hmmgbAxoR6sPJzI3R082HjyIv8b1Za3N5xhzvA2PLfqOPM2RfHllE7EZBRw/7cHCPEyeW1v3dWeTacusvpIEgu2RRPgasv4hfvwdrTGykLJ3R08mRruw7YzaUzt7ouHvYZ3NpzhUmGp2ZbPJ3fitXWn+PnABQBGhriz+1wGD3x/kPYeWk4n5/JoX39Sc0uQJHC1NXU4AM+VC+n6J3vhYmvJ4t3nWfpPAqNDPXgg3IfxX+1nf+wlhge7k1+i45Elh8kv0ZFdpCOy/M7hcHwWH22JYniwydOdOzYYfxcbOvs4mDxHCdZFJHM6JY927nZEpubx7oYzZBRcfg8ALraW5lAGmIT9l8fDGf7Jbh747iAlOiMlujJeKr9jaudhaquiAwU4l2b6jjztrVAo4INNUXywyTTg/8G4ELPnfjIplxNJOXT1c2RsR09OJJ1kb0wmCknCKMucu5jPy2tOsvJwIr0DTHdzEYk5/FkpBLLycCJP9G/JlG4+DPtkF652GqaV3+U4WFuQVVjG+uMp+DhZE+qlRZIkwluaOtG5G8+waNflMJTBKNOmuS1nL+bz6V/RBHtqWflod4rKDGbRXXbgAi+vPcnRhGy6+Tsxa0Artp9NZ1BbN347lkxkah5f/W26/mf2b8Xi3ecJ9tRWGU9QKkx3WO097FAqJH49mkR+iR61UiKzoJTpvf14ZWQ7bjT1EfckIEmW5QPlz3/BJO5pkiS5y7KcKkmSO5BeXyNvN85lnyMmx+T5vLH/Dfy0fizovwCtpRYHTf3Gn02eXQwz+rSknYcdYLrd7fPBDpybWRLkaXrtdEoeL/xynLS8Up4eFEBHbweiLubj5WCFjeXly+JEci65xToOlw9ifrEjpto5D8ZlMTrUgw/GhTBvcxTf7Y3DVqNi4ZQOZBSUmoViVIgHO6IyzD/+n/+5gI+TNRZKBUsf7srOqHTWHkvmn/OmH0phmYEgTyvzeR7u7cexxGyeG9KaD8aHYqFSMKmLNxYqBZcKynhnwxm2RF4kNsM0SHYiKReVQiLQzZYQL3vc7DS8/vtpnlt1HEmCC1lF6I0yWYVldPS2R2eQ6erniI2lEjDdFVSgViqY0aclyw8mAjBrQCseCPdlxo+H2XAiBT9nG15Ze4o27raEeGrp5OPAj/sTeGlEW9YeS+LCpSI+3xFNazdbFu+Jo01zW14d2RYHawus1Ep+i0jmr7PpdPV15EJWEUsf6srbf0SaO7tTybnEXypi+UFT5zIq1B07jdpsm6+zjdlLfXJAK55eGcHiPXEAWKoUlOqNtHSxIcDVlp3nTD/JSWEtmBrug5eDNW/e1Z5nV1UfjHzhlxNIEvz+ZC9+j0jh3Y2mTq9Nc1v+mNWL6UsPk5hVzPCg5uSX6Hnh1xMAaK3UXCwf5/hwQgfCfE3X9ZhQD5QKicd+OsJvEZc7mT0xmebHmZU6pEC3Zjw9KBC1UsHBVwZhqVKYQ1yONhacS8s3haK6tKgW+gouD8lU5on+rXh2VQT5pXoe7euPtYWqyh2sj5PpLiyvRI+nvRUhXvaEeNkDEOSpJSO/lOUHTSGlqd19eLJ/K06n5DLn15M8MziAj7dG097TjjVHkwnx0tLSpRnvbDiDhVLBkoe6kpRVzIgQ92p23QiuW9xlWb4oSVKiJEmtZVmOAgYCkeV/DwDvl/9f1yCW3ibIsszPZ35GKSm5t829nMw4ySf9P0FrWf1CvFYSs4q458u9FJYZ2Hw6jU1P98bHyYYTSbnojTIX80rMP7gtpy+SX579IUmw4N6OjP58Dx287PnpkW5YqBQ8sewoO8+ahCAtr7Tcq7rIgz18WXM0CVc7DbEZBcgyTAzzwspCyWuj23Fv1xZIQICbLQCfb48hOaeY3gHO2GlU5JXocbKxYOuZNEK8tAQ2b4abnYZJXbxp76Fl0a7z7IxKJ69EXyXkEehmy5Zn+lZ5zxXpZg/08OWDTVEcS8ypkqnQyrUZGrVJrO/u6GnKZIjPYmp3HyZ1acE/5y/xzoYzfFM+0NjJx75K+0/2b4WHvamD8XO2oV9rFzLyS2nT3NRJ/jm7N5cKysgt1jFl8QFOJecxvbcfTw4IYFwnL4I8tTzcy4+PtkTx6fYYoi7mE+Kl5fcnL6evdvKxZ/Np06BjRGIOCgm6+jnSK8C5krhfDqvYW6vNwl5Bazdb/iwX9zAfB9Y83oPI1Dxe+OUELRytMRhlerVyxlajZlP54OGEMJN9APd08iItr5S4zAJWHTZlHnXyNn0WXf2ccLPT4NTMFDa6kFWEj5MNKqWCx/q2xMHGgrljg4m/VMiwT3YD8HAvP06n5DIqxINe5V75fV0vh19bN7dlZ1QG1hZKAtxsOZ6YY45t5xabwhTOzSyZPyEUdXn6ZMX3WIGDjQVJ2cWU6o142ltxJRV3bi62lhSV6iksMxDe0om+ga5cyCpkeFB1kfWudL1VxMQr42JryVMDA8zPLVQSHb0d2PxMHwCGBbmz/OAF1hxNJtjTnmFBzTmRlMsDPXzo7OMILas1ecOob577LOBnSZIsgPPANEwZOKskSXoYSAAm1vMctw1G2cjcA3NZG7OW+9vez4tdr3+cWZZl0vJKaa7VkFei4+u/Y9FaqSksM/DpfR15avkxDpzPYtXhRApLDUgSrH40nKeWH0NrbWGOC08Ka8GqI4n89E8CZXojB+OzGPXZbl4f3Z4NJ6pmCLy05iQGo8yDPXzp7u+EjaWSV9aeIquwjG5+l+PJgeWiXkEnHwfyS3S42FrSzsOOf85n8VAvP+ZtjuLYhRwmhl1ecDvIU8un93Vk4lf7ORifRQsHa64GtVJBgFszdp3L5OzFPHOYp627nXkfrZWav57rS2ZBKQGutlioFPg52zB/SxSbT6dhq1HhaqtBlmUcrNXYW1vw36Gtq5znyymdqmSjuGutcNdaUaIzmD3krn5OaK3UaCt5jhPCWvDp9hjiLxXxyBXhsK6+TuyNMWWBxKQX4O9ig0atpE+AC9/vjQegrFKGhY9j9c+kdXOTuNtpVLjYWuJqpyHIU0up3kiolxYfJxssVYoqWSHeV7TzeL+WlOgMZnF/dVQ708Bkpc8PTB19hTfbzd+JbuVjCRUdHkCfQJcqInglXuXfa2cfB4I8tRxPzKGTt30Vcf96amfzeWrC0drCHPN202qqbfd2tMZOo8LD3orCUj3F5eGXT+/rgMEoV8uHB3DXalApJPRGucYO42oI83HAycaCnq2ccLSx4NP7Ol5XO/WlXqmQsixHyLIcJstyiCzLd8uynC3L8iVZlgfKshwgy/IgWZYbP+GzkYm8FMmb+99k3O/jWBm1kmntp/FClxdq3T+3WMfx8gwDMKWO7Y+9hCzLlOpNj/86k07P/9tOXGYhKw5e4IsdsczdeBZXW0sGtzWNYf9yJIkvdsTyw754vB2tCfN1ZO+cATzYwzT5w8fJmtmDAlBIEh9vPYeTjQUL7u1Aen4ps1ccA8BOo+LdsaYSwWcv5tMn0AVfZxuGBTWnd4ALEzp71TlhY87wNnz7YBckyeTl2FqquL+bj/nH1a6SAFfg62z68dfkPdVGO3c7zqTmIcswZ0QbVAqJDi2qioObnYb2HlqzvTaWKt6/JwTA/LlJksTkbt5M6VZ9oN/aQoWtpvq8Ao1aSVc/UyZPF9/qobUWjtaVPGHHKttGhrjT1dfRnGXUurxz7BvowueTOzK4nVu1tq6k4phWrs2qhCemdvchxMserZUajVppHnC2Uitxsa2e6aNRK3EvF8orO2m7SpPE7GuZMHZPJ0+gesdxJUPaudHV15F37w6mY/l31LG8I6kQd8s6JgE5VMrKcq9B3CVJ4j/hvowOcWdsR08ml3+ftX2HACqlAs/ya87zGq69ygS42XLkf4PxcbK5ruMbCjFD9QZyPuc8OqOO2Ttmk1+WT4hzCFPaTmF84Phq+y4/eIGVhxJZ/Vg43+4+z8K/Yzn++hCsLVR8tj2aL3bEMqGzFwFuzZi78Sx3dfDAYJQ5GHepysBlR297rCyUOFirq6TxVfxQJUkyP+7q64iHvRUvDG3Ne3+eZUAbV+7q4Mn+2EusOGSKLe96oT/21haU6Y1kF5ZVS9mb9S/eWQWe9lZmL2jWgFZM7uqN1lpNsKeWiMQc2nlUD0lV/DBqErLaaOdhB0dM5+sX6MLmZ/rUKTJgCtd083ekWaWxhueHtrnq81Yws18ruvs7mbNeruTeLt6cSyuoJu6tXJux6rFwXvjlOKsOJ9G6uen7USgkRoV4cDi+6gSlirhwZSqOCXC1rbatMi1dmiFJJvGtLS3Uz9kGpUKq8nkAVWYA1zYb+INxIczs17JKOmxNuNlpWPVYOGAKddzf3ZsRwe68syGSnPLskYoMq9pwtLlsQ3O76uIOVLvzuhq8Ha1JuFR03Z57U0GI+w1ClmVm/jWT1MJUjLKRT/p/wkDvgbXu/+b605TojPwekUJCVhE6g0zUxXw6tLBnXUQKtpYqVh9JwsHadEFfnhiSU+X2ssL78bC34nTK5fS1Ns0v/+jbutvRzt2O0aEeAMzo44+tRk3PVqbb614Bzqw4lIi3o7VZqKb9Sx72tWBtocLa0XTZhbd04lRyLm3cqwtSr1bOrItINovW1VBxBzCorSuSdG21RCrSPOtDeEsnc6ZGTUwI82J0qAdWFsoatwd72ZvE/QqPuXm5V1qRBePvXP19+TiZMmf6t/n3qqBWFkr8nGzwd6ndq3xxWBsKapiNezXirlIqaFVHB1OTTe/cbUp/1KiUlTz3mj+nChwqdaKudg1X4rqlSzMiEnPq7KCaOkLcbxDH0o+RXJCMQlLg2cyTfl79qu2z8tAFTibn8s7dwThYW5CaW8LCv2PNAh6ZmodRNs3Ge3dsEPM2R5lzYgvLTLM5j17INg/4gSknHKqKu0KiSnhCo1aycXZv8/OKMEQFPVs6I0kQ7FX/Qd5/Y2a/lgxu51ZtcBAgtIV9tcHTughtYc/woOZMuaLmSFNBkqRahR1MoYrd5zKqdRAVXmmfQBdeG92Ozj7Vwz5KhcSvj/e4KjsW/adzlYyoKwltUXOcu7Kg21vfmJIXlmqlOVvGsg7P3cnGJOiONhZ1dgTXwlMDA5gQ5nXLL0UpxP0GUGYo46czP2GlsmLZiGVoVBqUiuoX34u/mlIEnx/ShtTcEhys1cSkF2CnMX0tkSl5nE7Jw1KlYHSoB3EZhSzeE2ceuFMrJaLTC8gr1jOknRtvjGlvFnqPcm+vVytn5k0IqfW2tSYcbCx4664g8+y+G4WtRl1lwK6+aNRKFt7fucHau9m42WlYVGlmaAUVnvuVE6Gul2v1rCvQqJVYqBSU6Y1V4u8NSeU4+5XZMVfiUB6WcbuGa/tqcLSxuOW9dqh/bRnBFeiNeh7e/DBbE7Zyb5t7aeXQCi9br2r7ZeRfzuX9p7xexpjyMEleiemW+FB8FuuOJTM61AM7jZpZAwP49L6O5pmbQ9o3R5bhYl4Jng5WVTz4isfeTta4a62u2QuZ2t2nVg9OcHNp625HF18HevxLyOdmUeG936gKnJXj7HUNqFYIcPMGDMncTghxbyDyyvJ44e8XmPjHRCIyInirx1s82/lZ8/aPtp5j+tLD6AxG9AYjWyIvp6TtjDLlko8qF3cw1VI5l1ZAYZnBHDLRWqkZE+pB6/KUs0lhl2eyXjn4Yxb3axiQFDRNtFZqVj/Wo0nUI6+4q6xt0Li+VHjrkgQWdZQHtlIr0agVNG+A8ZLbERGWaQAKdYU8vPlhYnJicLdx5+5Wd1dbjPrTv6IBmPj1fpKzi0mv5LnvOGuqU9LJ2wGNWkGJzshzQwJJyi7GXasxp4pV0DfQhfXHUwj1ssff2YbzmYVVvHa4LOp+zo2bjiW4vbjxnrtJ3CvPRK0NSZL4cEKHaxp0v5MQ4l5PMooyeGn3S0RnR/PZgM/o7dW72j6yLGNrqSK/fCJFiJeWPoEuODezZObPR7mYV0K4vxNKhYS/czMiU/Po0dK51ou2s48DO/7bD4D/hPvwxvrIaulxIV5afpjW5YZUmxPcudyssMzVDpCOvElT+W9FhLjXg9zSXCZvnEx2STZv9niT3l69eXZVBOl5pXx6X0dzTDCnSEd+qZ5XR7blkd7+5uP1BlPBJ6OMOR0wwM0k7u72VzdI9EAPXwa2dauWDy5JEv1ai8WyBQ1LhahXhGcaGk25qNeV4y6oGyHu9eDtf94msziTpcOWEuwSjCzLrDlqKnE7ZfEBVj3anR/2xpsLZV05Y02lVOBmpyE1t8Q8TX5o++YUluprTA+sCUmSrmmij0BQH9y0GpybWdZ7ubzauByWabjUxjsVIe7XgcFoILkgmc3xm3k05FGCXYLZGZVuvjD7tXZh17kMur77F8WVVheqaXCzubZc3MsHSUcEuzMiWNxqCpomM/u1YmJYw5ekrsDSHJYRnnt9EeJ+jegMOu75/R4KdYUoJAU9XceYp41XMK2nH/d2acHu6ExyinRsOGkqwFWTuLtrNRyXTOEYgaCpo7VS37B4O1z23OvKcRfUjRD3a6BIV8RfF/4iPi8egD5efXh+ZRwJl6ousuvvbEMLR2uGBblTWKo3i3tNsxNHBLvj3MxSXMwCAZdj7sJzrz9C3K+SzOJMxq4bS25pLi2sAylMmIG/pzsb0hN5ZlAggW7NePzno1ioFFXSEm0sVbRzt6O2rK5RIR6MCvGoeaNAcIdhDsuIAdV6I8T9KojJyGVR5IcU6Aro7h5OSsxdJGTq+WybqXKij5M1Xcor/fk52VSr9yDP/AAAD8NJREFUE71+Vi9u7SoVAsHNwZwtIwZU643oHmtg8+mL5JWYCnRtPXeGQR/uYu1+NdPaT2OEy2uculB1yVdvJ2ucm1kS7KmtsdiWUiGhqGFhAIFAUBWN8NwbDOG5X0F6XgmP/niEV0e2pYOPhpnL9wHNMeZ1w0MO4Y0/ThPawp5+gS4sKJ91WjFQumx6N/OSYAKB4NoxD6gKz73eCCW6gooV3iNT85i86Ai6Ekem93dAIUn8d/UJ/r+9u4+tq77vOP7++l4/xs+PCY4TJ01YSKAjzGNMFKSG0o6saujGNqqqyzQ6pq2VqLppA6FN7bRNbaV167ppFQOkbGKjHZSBJk0r0DDWSk0IJSEJIYlhJMGJH2LH9/r58bc/zs/hztgh9r32uefcz0uyfO45Nz7fX37OJ7/7O091FSV86zfeu+R5TUmCBn+xUlVZsQ6MimRBI/fc0ch9nrl7Sf/3yV4mp4u44frXePgTf8av/mya7tQ4He3BE3um/DMt267wRBsRWRpdxJQ7Cvd5+ny4948Ec+6/27EHCB7+m/kA4I3+wOlCjzwTkeWZC3WN3LOncJ9nbuQOwW1H79xy44LvK0kW8fnbNrGzLXcPmxApdEu9cZgsTuE+T+/Q2OXltrqKK/6SPXTXdatRkkjBeO8KVY3cs6W/wXleO//W5eUrPURYRHJPc+65o3DPMDQ5xJt9XSSTkwALPmVeRFZOmW4cljOalsnw6NFHmZqq5qa2GobHktx2bWPYJYkUlCp/q+uVegB3IVG4e/1j/Tx+4BWY+gzXNjfxl5++IeySRApOa205T3z+F+ho14kK2VK4e0+/8RJDZz8LQP0KPfxXRD7YrVv0iTkXNLHlvfD2q5eXm6tLQ6xERCR7GrkDw5PDHO8J7vD4tV+5gbt3toZckYhIdjRyB17pfoXpqeC0x4/vWKv7w4hI5CncgYPdBymaqaU4YdRV6Ci9iESfwp0g3GuTG2iqLNVNwEQkFgo+3AfGBzh16RRltNBUXRZ2OSIiOZH1AVUzSwCHgC7n3CfNbBPwJNAAvAp8zjk3me1+cm121vHvh7u4lHgpWDFTTVOdzpIRkXjIxcj9AeBExuuvA3/tnNsCXALuy8E+cu5oV4ovf+8I33imiB3115Ma1SmQIhIfWYW7ma0Hfhl41L82YBfwlH/LPuDubPaxUs4PBnd/nBivodXdw8DIJM1VCncRiYdsR+5/A/wRMOtfNwCDzrlp//pdYMGTxs3sfjM7ZGaH+vr6sixj6brT40EdiRH+51hwg7DmKs25i0g8LDvczeyTQK9z7tUPfPMCnHOPOOc6nHMdTU1Nyy1j2brT4ySKHCUN++lNT9NYWcrHtjeveh0iIishmwOqtwKfMrPdQBlQDXwLqDWzpB+9rwe6si8z93pS45SVTtC87izXr7uG3761XSN3EYmNZY/cnXMPOefWO+fagXuBHzrnPgvsB+7xb9sLPJt1lct0/HyKm//iBXr9FEym7vQ4RcVptjRcw7c/s5OdG3QXOhGJj5U4z/2PgS+bWSfBHPxjK7CPq3L43CC9QxO82T30vm3dqXGmrI/26vbVL0xEZIXl5MZhzrmXgJf88tvAzbn4udnqSQUj9u55I3fnHN3pcWaqBmiv0X3bRSR+Yn2F6lyoz4X8nPT4NONTsxQl02yu2RxGaSIiKyrm4T7hv///cO+ZOw0ymeK6+utWvS4RkZUW73BPBRcq9cwL97P9owA0VSeoLatd9bpERFZazMN94Tn3d/pHANixdvXPrxcRWQ2xDfexyRnS48GFst2pcZ45/QwzszMAdPaloGiMD7dsCbNEEZEVE9twnxutt9WXc3F4kj/58Vd5+d2XATh+oY+ikotsb9R8u4jEU3zD3U/JTBd3AuCmqzg7dBaAM/0jJEsH6GjpCK0+EZGVFNtw7xocBmDAvQYE4X7q0ikmpmdIjyZprSuhsqQyzBJFRFZMbMP9R2feBOC3fu4OANrLbuG/Djbz7Zd/AhSxs3V9iNWJiKys2Ib7mf5hLJHmU9feBsBYeisXe7fwdz8YJJEc5b6fvy3kCkVEVk5sw703PUOiZJC22uCGYMMjwRRMSc0R/uOLt7Nj7YK3mRcRiYWc3FsmHw0OF1FRMU5VWTHFCaM/Hfw/9tzv/DrbGltCrk5EZGXFcuQ+PTPLyFgptWumMTPq15Qw66CmvJhtjTq3XUTiL5bhfiE1DhTRXJ0AoK6iBIDGypIQqxIRWT2xDPdzA8G9Y1rrgicr1a+ZC3c9AFtECkMsw/3t/gEANtZXAVA3F+5VCncRKQyxDPezly4BsKm+EYB6Py3TpJG7iBSIWIb7hfQQ2BRtNcFdHy+P3DXnLiIFIpbh3jM0hiVGaK5oBqC+ohjQnLuIFI5Yhvul0SksOUxDeQOQOXJXuItIYYhluKdGZ0kkx6hIVgCwbW01pckirm2pCrkyEZHVEcsrVEfGjbKK4AImgJ9ZW8XJP78r5KpERFZPLEfu4xNJKktd2GWIiIQmduE+OjnNzGyS6goLuxQRkdDELtz7hycBqPNnyIiIFKIYhvsEAE1VZSFXIiISntiFe1cqBcDaKj1CT0QKV+zC/czgIADX1FSHXImISHhiF+7nLqUB2NRQF3IlIiLhiV24d10axRJDtKxpCLsUEZHQxC7cu9PjWHGKpoqmsEsREQlN7ML94tA0xSUjNJRp5C4ihSt24T40mqB2jbt86wERkUK07HA3szYz229mb5jZcTN7wK+vN7Pnzey0/75qRzZHJ6eZmi6huUr3bReRwpbNyH0a+APn3HbgFuALZrYdeBB40Tm3FXjRv14VZweGANhQr7s/ikhhW3a4O+cuOOd+6peHgBNAK7AH2Offtg+4O9sir9bR7nMAbGlsXK1diojkpZzMuZtZO7ATOAC0OOcu+E3dQMsif+Z+MztkZof6+vpyUQYnersBuH5ta05+nohIVGUd7mZWCTwNfMk5l87c5pxzwIL33nXOPeKc63DOdTQ15ea0xbf6UsAsN67dmJOfJyISVVmFu5kVEwT7E8657/vVPWa2zm9fB/RmV+LVOz84SVFxmsY1ujpVRApbNmfLGPAYcMI5982MTc8Be/3yXuDZ5Ze3NP1po6J8RKdBikjBy2bkfivwOWCXmR32X7uBrwF3mtlp4GP+9apIj5ZSUzm9WrsTEclby36GqnPuR8BiQ+Q7lvtzl2t4YprpqTJaamJ3XZaIyJLFJgmDg6nQVq+HdIiIxCbcj10Izr7c0lQbciUiIuGLTbif7rsIwLbmBU+rFxEpKMuec88359NDgGNrwzVhlyIiErrYhHtPegxLzrCucm3YpYiIhC424T4wMkmyeJLSRGnYpYiIhC42c+7pMagonQm7DBGRvBCbcB+bSFBdritTRUQgJuHunGNqqoy6NcVhlyIikhdiEe7dw5dwsyU0V5WHXYqISF6IRbif6jsPQGtNdciViIjkh8ifLfPjzovsffQMABvr6kOuRkQkP0R+5P4vB89eXv5QQ3OIlYiI5I/Ih/uaksTl5c31CncREYhBuPekJyhKTLK5/S1aa3VAVUQEYhDuF1KjFJV38mu3FFNUpPPcRUQgDuGeHsWSaXY07gi7FBGRvBHpcJ+cnmVozGHJNNsbtoddjohI3oh0uPcNTwBQXjZFQ1lDyNWIiOSPSId7T3ocgLXVZZhpvl1EZE6kw73Xh3tbna5MFRHJFOlwP58aBWBLQ1PIlYiI5JdIh3tlxTjJ6sNsbdSj9UREMkU63Ne3pChvfZIN1evDLkVEJK9EOtzPpc8BsKFqQ8iViIjkl0iHe1NFEx9t+yhNFZpzFxHJFOlb/u7asItdG3aFXYaISN6J9MhdREQWpnAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIbMORd2DZhZH3BmmX+8EbiYw3LCpLbkJ7UlP6ktsNE5t+Al+nkR7tkws0POuY6w68gFtSU/qS35SW25Mk3LiIjEkMJdRCSG4hDuj4RdQA6pLflJbclPassVRH7OXURE3i8OI3cREZlH4S4iEkORDncz+yUzO2lmnWb2YNj1LJWZvWNmR83ssJkd8uvqzex5Mzvtv9eFXedCzOxxM+s1s2MZ6xas3QJ/6/vpdTO7KbzK32+RtnzFzLp83xw2s90Z2x7ybTlpZp8Ip+r3M7M2M9tvZm+Y2XEze8Cvj1y/XKEtUeyXMjM7aGZHfFu+6tdvMrMDvubvmlmJX1/qX3f67e3L2rFzLpJfQAJ4C9gMlABHgO1h17XENrwDNM5b9w3gQb/8IPD1sOtcpPbbgZuAYx9UO7Ab+E/AgFuAA2HXfxVt+Qrwhwu8d7v/XSsFNvnfwUTYbfC1rQNu8stVwClfb+T65QptiWK/GFDpl4uBA/7v+3vAvX79d4Df88u/D3zHL98LfHc5+43yyP1moNM597ZzbhJ4EtgTck25sAfY55f3AXeHWMuinHMvAwPzVi9W+x7gn1zgJ0Ctma1bnUo/2CJtWcwe4Enn3IRz7n+BToLfxdA55y44537ql4eAE0ArEeyXK7RlMfncL845N+xfFvsvB+wCnvLr5/fLXH89BdxhZrbU/UY53FuBcxmv3+XKnZ+PHPADM3vVzO7361qccxf8cjfQEk5py7JY7VHtqy/66YrHM6bHItEW/1F+J8EoMdL9Mq8tEMF+MbOEmR0GeoHnCT5ZDDrnpv1bMuu93Ba/PQU0LHWfUQ73OPiIc+4m4C7gC2Z2e+ZGF3wui+S5qlGu3fsH4EPAjcAF4K/CLefqmVkl8DTwJedcOnNb1PplgbZEsl+cczPOuRuB9QSfKLat9D6jHO5dQFvG6/V+XWQ457r8917gGYJO75n7aOy/94ZX4ZItVnvk+so51+P/Qc4C/8h7H/Hzui1mVkwQhk84577vV0eyXxZqS1T7ZY5zbhDYD/wiwTRY0m/KrPdyW/z2GqB/qfuKcri/Amz1R5xLCA48PBdyTVfNzNaYWdXcMvBx4BhBG/b6t+0Fng2nwmVZrPbngN/0Z2fcAqQypgny0ry5508T9A0EbbnXn9GwCdgKHFzt+hbi52UfA044576ZsSly/bJYWyLaL01mVuuXy4E7CY4h7Afu8W+b3y9z/XUP8EP/iWtpwj6SnOVR6N0ER9HfAh4Ou54l1r6Z4Oj+EeD4XP0Ec2svAqeBF4D6sGtdpP5/JfhYPEUwX3jfYrUTnC3w976fjgIdYdd/FW35Z1/r6/4f27qM9z/s23ISuCvs+jPq+gjBlMvrwGH/tTuK/XKFtkSxXz4MvOZrPgb8qV+/meA/oE7g34BSv77Mv+702zcvZ7+6/YCISAxFeVpGREQWoXAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMTQ/wESa+Kz6ngmcAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "epochs = 300\n",
        "momentum = 0.3\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "getAccuracies(learning_rate, batch_size, momentum, weight_decay, dampening)"
      ],
      "id": "HKvfOX1JNpSW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPRJaFPNk0B"
      },
      "source": [
        "### Default"
      ],
      "id": "PoPRJaFPNk0B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37805
        },
        "id": "2a1521d4",
        "outputId": "1e9d10f0-05e4-4e16-ad57-004ef4e5a910",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Training loop: loss: 2.999373  [    0/ 4000]\n",
            "Training Accuracy: 6.2%\n",
            "Testing loop: \n",
            " Accuracy: 6.6%, Avg loss: 2.994164 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Training loop: loss: 2.999091  [    0/ 4000]\n",
            "Training Accuracy: 5.9%\n",
            "Testing loop: \n",
            " Accuracy: 7.0%, Avg loss: 2.992778 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Training loop: loss: 2.992199  [    0/ 4000]\n",
            "Training Accuracy: 6.0%\n",
            "Testing loop: \n",
            " Accuracy: 6.2%, Avg loss: 2.991532 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Training loop: loss: 2.993323  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 6.2%, Avg loss: 2.990354 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Training loop: loss: 2.992879  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 6.4%, Avg loss: 2.989195 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Training loop: loss: 2.989807  [    0/ 4000]\n",
            "Training Accuracy: 5.7%\n",
            "Testing loop: \n",
            " Accuracy: 6.2%, Avg loss: 2.988096 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Training loop: loss: 2.986430  [    0/ 4000]\n",
            "Training Accuracy: 5.9%\n",
            "Testing loop: \n",
            " Accuracy: 6.2%, Avg loss: 2.987023 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Training loop: loss: 2.986023  [    0/ 4000]\n",
            "Training Accuracy: 5.8%\n",
            "Testing loop: \n",
            " Accuracy: 6.6%, Avg loss: 2.985981 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Training loop: loss: 2.987183  [    0/ 4000]\n",
            "Training Accuracy: 7.5%\n",
            "Testing loop: \n",
            " Accuracy: 7.2%, Avg loss: 2.984944 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Training loop: loss: 2.985714  [    0/ 4000]\n",
            "Training Accuracy: 7.1%\n",
            "Testing loop: \n",
            " Accuracy: 8.4%, Avg loss: 2.983913 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Training loop: loss: 2.983711  [    0/ 4000]\n",
            "Training Accuracy: 8.0%\n",
            "Testing loop: \n",
            " Accuracy: 8.8%, Avg loss: 2.982879 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Training loop: loss: 2.980400  [    0/ 4000]\n",
            "Training Accuracy: 9.1%\n",
            "Testing loop: \n",
            " Accuracy: 9.2%, Avg loss: 2.981860 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Training loop: loss: 2.982789  [    0/ 4000]\n",
            "Training Accuracy: 10.0%\n",
            "Testing loop: \n",
            " Accuracy: 9.4%, Avg loss: 2.980841 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Training loop: loss: 2.982968  [    0/ 4000]\n",
            "Training Accuracy: 10.5%\n",
            "Testing loop: \n",
            " Accuracy: 10.0%, Avg loss: 2.979815 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Training loop: loss: 2.979726  [    0/ 4000]\n",
            "Training Accuracy: 11.1%\n",
            "Testing loop: \n",
            " Accuracy: 11.2%, Avg loss: 2.978791 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Training loop: loss: 2.979053  [    0/ 4000]\n",
            "Training Accuracy: 12.4%\n",
            "Testing loop: \n",
            " Accuracy: 12.0%, Avg loss: 2.977760 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Training loop: loss: 2.976221  [    0/ 4000]\n",
            "Training Accuracy: 12.1%\n",
            "Testing loop: \n",
            " Accuracy: 12.8%, Avg loss: 2.976703 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Training loop: loss: 2.976139  [    0/ 4000]\n",
            "Training Accuracy: 13.0%\n",
            "Testing loop: \n",
            " Accuracy: 13.2%, Avg loss: 2.975620 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Training loop: loss: 2.973745  [    0/ 4000]\n",
            "Training Accuracy: 14.5%\n",
            "Testing loop: \n",
            " Accuracy: 13.6%, Avg loss: 2.974518 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Training loop: loss: 2.970961  [    0/ 4000]\n",
            "Training Accuracy: 14.4%\n",
            "Testing loop: \n",
            " Accuracy: 14.6%, Avg loss: 2.973381 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Training loop: loss: 2.971240  [    0/ 4000]\n",
            "Training Accuracy: 14.5%\n",
            "Testing loop: \n",
            " Accuracy: 15.2%, Avg loss: 2.972227 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Training loop: loss: 2.971544  [    0/ 4000]\n",
            "Training Accuracy: 15.8%\n",
            "Testing loop: \n",
            " Accuracy: 16.8%, Avg loss: 2.971045 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Training loop: loss: 2.969868  [    0/ 4000]\n",
            "Training Accuracy: 16.5%\n",
            "Testing loop: \n",
            " Accuracy: 17.8%, Avg loss: 2.969829 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Training loop: loss: 2.971232  [    0/ 4000]\n",
            "Training Accuracy: 17.7%\n",
            "Testing loop: \n",
            " Accuracy: 18.2%, Avg loss: 2.968578 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Training loop: loss: 2.970033  [    0/ 4000]\n",
            "Training Accuracy: 16.7%\n",
            "Testing loop: \n",
            " Accuracy: 18.2%, Avg loss: 2.967308 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Training loop: loss: 2.966941  [    0/ 4000]\n",
            "Training Accuracy: 18.4%\n",
            "Testing loop: \n",
            " Accuracy: 18.6%, Avg loss: 2.965999 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Training loop: loss: 2.962367  [    0/ 4000]\n",
            "Training Accuracy: 18.6%\n",
            "Testing loop: \n",
            " Accuracy: 19.2%, Avg loss: 2.964655 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Training loop: loss: 2.961926  [    0/ 4000]\n",
            "Training Accuracy: 19.4%\n",
            "Testing loop: \n",
            " Accuracy: 19.0%, Avg loss: 2.963279 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Training loop: loss: 2.964210  [    0/ 4000]\n",
            "Training Accuracy: 20.0%\n",
            "Testing loop: \n",
            " Accuracy: 19.8%, Avg loss: 2.961845 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Training loop: loss: 2.962288  [    0/ 4000]\n",
            "Training Accuracy: 19.4%\n",
            "Testing loop: \n",
            " Accuracy: 21.8%, Avg loss: 2.960362 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Training loop: loss: 2.957993  [    0/ 4000]\n",
            "Training Accuracy: 21.8%\n",
            "Testing loop: \n",
            " Accuracy: 22.2%, Avg loss: 2.958824 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Training loop: loss: 2.957457  [    0/ 4000]\n",
            "Training Accuracy: 21.8%\n",
            "Testing loop: \n",
            " Accuracy: 23.4%, Avg loss: 2.957238 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Training loop: loss: 2.955082  [    0/ 4000]\n",
            "Training Accuracy: 22.3%\n",
            "Testing loop: \n",
            " Accuracy: 24.8%, Avg loss: 2.955599 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Training loop: loss: 2.950915  [    0/ 4000]\n",
            "Training Accuracy: 23.5%\n",
            "Testing loop: \n",
            " Accuracy: 25.8%, Avg loss: 2.953903 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Training loop: loss: 2.955031  [    0/ 4000]\n",
            "Training Accuracy: 23.7%\n",
            "Testing loop: \n",
            " Accuracy: 26.8%, Avg loss: 2.952138 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Training loop: loss: 2.944898  [    0/ 4000]\n",
            "Training Accuracy: 25.0%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.950319 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Training loop: loss: 2.950678  [    0/ 4000]\n",
            "Training Accuracy: 26.2%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.948417 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Training loop: loss: 2.946959  [    0/ 4000]\n",
            "Training Accuracy: 26.5%\n",
            "Testing loop: \n",
            " Accuracy: 28.8%, Avg loss: 2.946433 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Training loop: loss: 2.950837  [    0/ 4000]\n",
            "Training Accuracy: 27.8%\n",
            "Testing loop: \n",
            " Accuracy: 29.4%, Avg loss: 2.944396 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Training loop: loss: 2.942769  [    0/ 4000]\n",
            "Training Accuracy: 27.5%\n",
            "Testing loop: \n",
            " Accuracy: 30.0%, Avg loss: 2.942291 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Training loop: loss: 2.939489  [    0/ 4000]\n",
            "Training Accuracy: 28.2%\n",
            "Testing loop: \n",
            " Accuracy: 30.0%, Avg loss: 2.940095 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Training loop: loss: 2.938824  [    0/ 4000]\n",
            "Training Accuracy: 28.6%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.937816 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Training loop: loss: 2.938228  [    0/ 4000]\n",
            "Training Accuracy: 30.0%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.935441 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Training loop: loss: 2.927586  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.932978 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Training loop: loss: 2.925690  [    0/ 4000]\n",
            "Training Accuracy: 28.1%\n",
            "Testing loop: \n",
            " Accuracy: 32.4%, Avg loss: 2.930422 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Training loop: loss: 2.929828  [    0/ 4000]\n",
            "Training Accuracy: 29.4%\n",
            "Testing loop: \n",
            " Accuracy: 32.4%, Avg loss: 2.927744 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Training loop: loss: 2.925396  [    0/ 4000]\n",
            "Training Accuracy: 29.9%\n",
            "Testing loop: \n",
            " Accuracy: 31.4%, Avg loss: 2.924947 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Training loop: loss: 2.925219  [    0/ 4000]\n",
            "Training Accuracy: 30.7%\n",
            "Testing loop: \n",
            " Accuracy: 31.2%, Avg loss: 2.922005 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Training loop: loss: 2.922932  [    0/ 4000]\n",
            "Training Accuracy: 30.1%\n",
            "Testing loop: \n",
            " Accuracy: 31.4%, Avg loss: 2.918935 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Training loop: loss: 2.916050  [    0/ 4000]\n",
            "Training Accuracy: 30.8%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.915690 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Training loop: loss: 2.905605  [    0/ 4000]\n",
            "Training Accuracy: 30.1%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.912297 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Training loop: loss: 2.904828  [    0/ 4000]\n",
            "Training Accuracy: 31.0%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.908731 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Training loop: loss: 2.906453  [    0/ 4000]\n",
            "Training Accuracy: 31.4%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.904975 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Training loop: loss: 2.899558  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.901069 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Training loop: loss: 2.894182  [    0/ 4000]\n",
            "Training Accuracy: 31.4%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.896881 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Training loop: loss: 2.892513  [    0/ 4000]\n",
            "Training Accuracy: 30.9%\n",
            "Testing loop: \n",
            " Accuracy: 30.6%, Avg loss: 2.892451 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Training loop: loss: 2.888682  [    0/ 4000]\n",
            "Training Accuracy: 31.3%\n",
            "Testing loop: \n",
            " Accuracy: 31.0%, Avg loss: 2.887739 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Training loop: loss: 2.890016  [    0/ 4000]\n",
            "Training Accuracy: 31.7%\n",
            "Testing loop: \n",
            " Accuracy: 30.8%, Avg loss: 2.882722 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Training loop: loss: 2.873922  [    0/ 4000]\n",
            "Training Accuracy: 31.6%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.877503 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Training loop: loss: 2.867556  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.871997 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Training loop: loss: 2.863430  [    0/ 4000]\n",
            "Training Accuracy: 31.8%\n",
            "Testing loop: \n",
            " Accuracy: 30.0%, Avg loss: 2.866194 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Training loop: loss: 2.854280  [    0/ 4000]\n",
            "Training Accuracy: 31.3%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.860116 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Training loop: loss: 2.866218  [    0/ 4000]\n",
            "Training Accuracy: 31.2%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.853723 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Training loop: loss: 2.854155  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.8%, Avg loss: 2.847015 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Training loop: loss: 2.840580  [    0/ 4000]\n",
            "Training Accuracy: 29.7%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.839913 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Training loop: loss: 2.828150  [    0/ 4000]\n",
            "Training Accuracy: 29.9%\n",
            "Testing loop: \n",
            " Accuracy: 29.0%, Avg loss: 2.832496 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Training loop: loss: 2.826107  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.824672 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Training loop: loss: 2.817121  [    0/ 4000]\n",
            "Training Accuracy: 29.9%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.816598 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Training loop: loss: 2.815814  [    0/ 4000]\n",
            "Training Accuracy: 30.3%\n",
            "Testing loop: \n",
            " Accuracy: 27.8%, Avg loss: 2.807809 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Training loop: loss: 2.809957  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 27.6%, Avg loss: 2.798970 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Training loop: loss: 2.774550  [    0/ 4000]\n",
            "Training Accuracy: 29.3%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.789386 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Training loop: loss: 2.774417  [    0/ 4000]\n",
            "Training Accuracy: 30.2%\n",
            "Testing loop: \n",
            " Accuracy: 27.8%, Avg loss: 2.779851 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Training loop: loss: 2.770833  [    0/ 4000]\n",
            "Training Accuracy: 29.2%\n",
            "Testing loop: \n",
            " Accuracy: 28.4%, Avg loss: 2.769590 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Training loop: loss: 2.750622  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.758934 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Training loop: loss: 2.742185  [    0/ 4000]\n",
            "Training Accuracy: 30.1%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.748146 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Training loop: loss: 2.734633  [    0/ 4000]\n",
            "Training Accuracy: 29.6%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.736943 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Training loop: loss: 2.721717  [    0/ 4000]\n",
            "Training Accuracy: 29.5%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.725565 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Training loop: loss: 2.690283  [    0/ 4000]\n",
            "Training Accuracy: 29.3%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.713983 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Training loop: loss: 2.694849  [    0/ 4000]\n",
            "Training Accuracy: 30.3%\n",
            "Testing loop: \n",
            " Accuracy: 28.6%, Avg loss: 2.702421 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Training loop: loss: 2.707117  [    0/ 4000]\n",
            "Training Accuracy: 28.5%\n",
            "Testing loop: \n",
            " Accuracy: 29.8%, Avg loss: 2.690042 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Training loop: loss: 2.682637  [    0/ 4000]\n",
            "Training Accuracy: 30.4%\n",
            "Testing loop: \n",
            " Accuracy: 29.4%, Avg loss: 2.678071 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Training loop: loss: 2.666535  [    0/ 4000]\n",
            "Training Accuracy: 30.8%\n",
            "Testing loop: \n",
            " Accuracy: 28.2%, Avg loss: 2.665810 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Training loop: loss: 2.647371  [    0/ 4000]\n",
            "Training Accuracy: 30.2%\n",
            "Testing loop: \n",
            " Accuracy: 29.2%, Avg loss: 2.653140 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Training loop: loss: 2.610035  [    0/ 4000]\n",
            "Training Accuracy: 30.5%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.640705 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Training loop: loss: 2.614855  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 29.6%, Avg loss: 2.628356 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Training loop: loss: 2.601415  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.616197 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Training loop: loss: 2.616789  [    0/ 4000]\n",
            "Training Accuracy: 31.1%\n",
            "Testing loop: \n",
            " Accuracy: 30.4%, Avg loss: 2.603730 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Training loop: loss: 2.576848  [    0/ 4000]\n",
            "Training Accuracy: 31.2%\n",
            "Testing loop: \n",
            " Accuracy: 30.2%, Avg loss: 2.591341 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Training loop: loss: 2.565297  [    0/ 4000]\n",
            "Training Accuracy: 31.2%\n",
            "Testing loop: \n",
            " Accuracy: 31.8%, Avg loss: 2.579201 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Training loop: loss: 2.556011  [    0/ 4000]\n",
            "Training Accuracy: 32.8%\n",
            "Testing loop: \n",
            " Accuracy: 31.4%, Avg loss: 2.566911 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Training loop: loss: 2.518433  [    0/ 4000]\n",
            "Training Accuracy: 31.6%\n",
            "Testing loop: \n",
            " Accuracy: 33.0%, Avg loss: 2.554974 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Training loop: loss: 2.503520  [    0/ 4000]\n",
            "Training Accuracy: 33.3%\n",
            "Testing loop: \n",
            " Accuracy: 33.0%, Avg loss: 2.543151 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Training loop: loss: 2.536875  [    0/ 4000]\n",
            "Training Accuracy: 33.3%\n",
            "Testing loop: \n",
            " Accuracy: 34.0%, Avg loss: 2.532108 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Training loop: loss: 2.527123  [    0/ 4000]\n",
            "Training Accuracy: 33.4%\n",
            "Testing loop: \n",
            " Accuracy: 33.6%, Avg loss: 2.519396 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Training loop: loss: 2.481405  [    0/ 4000]\n",
            "Training Accuracy: 34.0%\n",
            "Testing loop: \n",
            " Accuracy: 34.2%, Avg loss: 2.507351 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Training loop: loss: 2.489172  [    0/ 4000]\n",
            "Training Accuracy: 35.2%\n",
            "Testing loop: \n",
            " Accuracy: 33.8%, Avg loss: 2.495707 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Training loop: loss: 2.465306  [    0/ 4000]\n",
            "Training Accuracy: 35.1%\n",
            "Testing loop: \n",
            " Accuracy: 34.4%, Avg loss: 2.481468 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Training loop: loss: 2.465713  [    0/ 4000]\n",
            "Training Accuracy: 34.8%\n",
            "Testing loop: \n",
            " Accuracy: 35.4%, Avg loss: 2.469850 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Training loop: loss: 2.422431  [    0/ 4000]\n",
            "Training Accuracy: 35.7%\n",
            "Testing loop: \n",
            " Accuracy: 35.6%, Avg loss: 2.456616 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Training loop: loss: 2.433023  [    0/ 4000]\n",
            "Training Accuracy: 36.8%\n",
            "Testing loop: \n",
            " Accuracy: 36.0%, Avg loss: 2.443623 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "Training loop: loss: 2.430540  [    0/ 4000]\n",
            "Training Accuracy: 37.4%\n",
            "Testing loop: \n",
            " Accuracy: 35.8%, Avg loss: 2.432510 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "Training loop: loss: 2.407487  [    0/ 4000]\n",
            "Training Accuracy: 37.0%\n",
            "Testing loop: \n",
            " Accuracy: 36.0%, Avg loss: 2.418199 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "Training loop: loss: 2.399487  [    0/ 4000]\n",
            "Training Accuracy: 37.8%\n",
            "Testing loop: \n",
            " Accuracy: 36.2%, Avg loss: 2.404771 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "Training loop: loss: 2.339257  [    0/ 4000]\n",
            "Training Accuracy: 38.1%\n",
            "Testing loop: \n",
            " Accuracy: 36.4%, Avg loss: 2.392178 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "Training loop: loss: 2.340054  [    0/ 4000]\n",
            "Training Accuracy: 38.5%\n",
            "Testing loop: \n",
            " Accuracy: 37.4%, Avg loss: 2.379355 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "Training loop: loss: 2.316900  [    0/ 4000]\n",
            "Training Accuracy: 38.5%\n",
            "Testing loop: \n",
            " Accuracy: 38.2%, Avg loss: 2.369026 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "Training loop: loss: 2.398202  [    0/ 4000]\n",
            "Training Accuracy: 38.9%\n",
            "Testing loop: \n",
            " Accuracy: 37.0%, Avg loss: 2.353237 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "Training loop: loss: 2.314634  [    0/ 4000]\n",
            "Training Accuracy: 38.9%\n",
            "Testing loop: \n",
            " Accuracy: 37.4%, Avg loss: 2.339704 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "Training loop: loss: 2.298706  [    0/ 4000]\n",
            "Training Accuracy: 40.1%\n",
            "Testing loop: \n",
            " Accuracy: 39.0%, Avg loss: 2.326100 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "Training loop: loss: 2.293435  [    0/ 4000]\n",
            "Training Accuracy: 39.3%\n",
            "Testing loop: \n",
            " Accuracy: 40.6%, Avg loss: 2.311970 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "Training loop: loss: 2.292430  [    0/ 4000]\n",
            "Training Accuracy: 40.2%\n",
            "Testing loop: \n",
            " Accuracy: 39.0%, Avg loss: 2.303210 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "Training loop: loss: 2.254410  [    0/ 4000]\n",
            "Training Accuracy: 40.0%\n",
            "Testing loop: \n",
            " Accuracy: 40.4%, Avg loss: 2.285803 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "Training loop: loss: 2.260133  [    0/ 4000]\n",
            "Training Accuracy: 40.6%\n",
            "Testing loop: \n",
            " Accuracy: 41.0%, Avg loss: 2.272951 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "Training loop: loss: 2.251752  [    0/ 4000]\n",
            "Training Accuracy: 40.6%\n",
            "Testing loop: \n",
            " Accuracy: 41.4%, Avg loss: 2.257946 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "Training loop: loss: 2.197559  [    0/ 4000]\n",
            "Training Accuracy: 40.9%\n",
            "Testing loop: \n",
            " Accuracy: 41.8%, Avg loss: 2.243691 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "Training loop: loss: 2.230116  [    0/ 4000]\n",
            "Training Accuracy: 41.5%\n",
            "Testing loop: \n",
            " Accuracy: 43.2%, Avg loss: 2.232793 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "Training loop: loss: 2.156959  [    0/ 4000]\n",
            "Training Accuracy: 41.5%\n",
            "Testing loop: \n",
            " Accuracy: 42.6%, Avg loss: 2.217165 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "Training loop: loss: 2.230139  [    0/ 4000]\n",
            "Training Accuracy: 42.2%\n",
            "Testing loop: \n",
            " Accuracy: 42.8%, Avg loss: 2.206619 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "Training loop: loss: 2.168643  [    0/ 4000]\n",
            "Training Accuracy: 41.5%\n",
            "Testing loop: \n",
            " Accuracy: 42.4%, Avg loss: 2.190861 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "Training loop: loss: 2.123875  [    0/ 4000]\n",
            "Training Accuracy: 42.1%\n",
            "Testing loop: \n",
            " Accuracy: 42.4%, Avg loss: 2.177420 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "Training loop: loss: 2.180677  [    0/ 4000]\n",
            "Training Accuracy: 43.1%\n",
            "Testing loop: \n",
            " Accuracy: 43.4%, Avg loss: 2.173545 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "Training loop: loss: 2.140682  [    0/ 4000]\n",
            "Training Accuracy: 43.6%\n",
            "Testing loop: \n",
            " Accuracy: 40.8%, Avg loss: 2.167517 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "Training loop: loss: 2.151993  [    0/ 4000]\n",
            "Training Accuracy: 43.5%\n",
            "Testing loop: \n",
            " Accuracy: 43.0%, Avg loss: 2.144327 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "Training loop: loss: 2.087518  [    0/ 4000]\n",
            "Training Accuracy: 44.0%\n",
            "Testing loop: \n",
            " Accuracy: 43.0%, Avg loss: 2.137483 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "Training loop: loss: 2.059123  [    0/ 4000]\n",
            "Training Accuracy: 43.4%\n",
            "Testing loop: \n",
            " Accuracy: 43.8%, Avg loss: 2.128822 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "Training loop: loss: 2.137123  [    0/ 4000]\n",
            "Training Accuracy: 44.1%\n",
            "Testing loop: \n",
            " Accuracy: 42.6%, Avg loss: 2.121905 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "Training loop: loss: 2.034664  [    0/ 4000]\n",
            "Training Accuracy: 44.3%\n",
            "Testing loop: \n",
            " Accuracy: 44.6%, Avg loss: 2.090274 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "Training loop: loss: 2.013299  [    0/ 4000]\n",
            "Training Accuracy: 45.0%\n",
            "Testing loop: \n",
            " Accuracy: 45.0%, Avg loss: 2.089015 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "Training loop: loss: 1.939112  [    0/ 4000]\n",
            "Training Accuracy: 45.4%\n",
            "Testing loop: \n",
            " Accuracy: 44.4%, Avg loss: 2.067498 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "Training loop: loss: 2.046516  [    0/ 4000]\n",
            "Training Accuracy: 46.1%\n",
            "Testing loop: \n",
            " Accuracy: 45.4%, Avg loss: 2.059045 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "Training loop: loss: 2.011682  [    0/ 4000]\n",
            "Training Accuracy: 45.4%\n",
            "Testing loop: \n",
            " Accuracy: 44.4%, Avg loss: 2.048012 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "Training loop: loss: 2.013202  [    0/ 4000]\n",
            "Training Accuracy: 46.6%\n",
            "Testing loop: \n",
            " Accuracy: 45.2%, Avg loss: 2.034796 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "Training loop: loss: 1.998275  [    0/ 4000]\n",
            "Training Accuracy: 46.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 2.030745 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "Training loop: loss: 1.962030  [    0/ 4000]\n",
            "Training Accuracy: 45.6%\n",
            "Testing loop: \n",
            " Accuracy: 45.6%, Avg loss: 2.014638 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "Training loop: loss: 1.967399  [    0/ 4000]\n",
            "Training Accuracy: 47.3%\n",
            "Testing loop: \n",
            " Accuracy: 43.0%, Avg loss: 2.040908 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "Training loop: loss: 1.938762  [    0/ 4000]\n",
            "Training Accuracy: 46.9%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.995002 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "Training loop: loss: 1.953920  [    0/ 4000]\n",
            "Training Accuracy: 47.5%\n",
            "Testing loop: \n",
            " Accuracy: 46.4%, Avg loss: 1.986288 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "Training loop: loss: 1.911316  [    0/ 4000]\n",
            "Training Accuracy: 47.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.972886 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "Training loop: loss: 1.893404  [    0/ 4000]\n",
            "Training Accuracy: 47.5%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 1.966839 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "Training loop: loss: 1.808577  [    0/ 4000]\n",
            "Training Accuracy: 47.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 1.958432 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "Training loop: loss: 1.902179  [    0/ 4000]\n",
            "Training Accuracy: 48.3%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 1.945844 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "Training loop: loss: 1.864539  [    0/ 4000]\n",
            "Training Accuracy: 48.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 1.952174 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "Training loop: loss: 1.841570  [    0/ 4000]\n",
            "Training Accuracy: 48.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.4%, Avg loss: 1.943097 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "Training loop: loss: 1.867782  [    0/ 4000]\n",
            "Training Accuracy: 48.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.920410 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "Training loop: loss: 1.908240  [    0/ 4000]\n",
            "Training Accuracy: 48.5%\n",
            "Testing loop: \n",
            " Accuracy: 41.4%, Avg loss: 2.001593 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "Training loop: loss: 1.971858  [    0/ 4000]\n",
            "Training Accuracy: 47.5%\n",
            "Testing loop: \n",
            " Accuracy: 45.4%, Avg loss: 1.927072 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "Training loop: loss: 1.910522  [    0/ 4000]\n",
            "Training Accuracy: 48.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.906359 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "Training loop: loss: 1.849862  [    0/ 4000]\n",
            "Training Accuracy: 48.2%\n",
            "Testing loop: \n",
            " Accuracy: 44.6%, Avg loss: 1.945219 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "Training loop: loss: 1.865589  [    0/ 4000]\n",
            "Training Accuracy: 47.3%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 1.929673 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "Training loop: loss: 1.762569  [    0/ 4000]\n",
            "Training Accuracy: 48.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.919927 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "Training loop: loss: 1.813930  [    0/ 4000]\n",
            "Training Accuracy: 49.3%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 1.920173 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "Training loop: loss: 1.828293  [    0/ 4000]\n",
            "Training Accuracy: 47.7%\n",
            "Testing loop: \n",
            " Accuracy: 47.2%, Avg loss: 1.895048 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "Training loop: loss: 1.837951  [    0/ 4000]\n",
            "Training Accuracy: 48.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 1.872468 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "Training loop: loss: 1.786353  [    0/ 4000]\n",
            "Training Accuracy: 49.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.4%, Avg loss: 1.880910 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "Training loop: loss: 1.847421  [    0/ 4000]\n",
            "Training Accuracy: 49.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.846335 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "Training loop: loss: 1.807889  [    0/ 4000]\n",
            "Training Accuracy: 49.9%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.843599 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "Training loop: loss: 1.736146  [    0/ 4000]\n",
            "Training Accuracy: 50.2%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 1.865487 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "Training loop: loss: 1.806063  [    0/ 4000]\n",
            "Training Accuracy: 49.4%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.837087 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "Training loop: loss: 1.700921  [    0/ 4000]\n",
            "Training Accuracy: 49.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.851810 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "Training loop: loss: 1.860030  [    0/ 4000]\n",
            "Training Accuracy: 49.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.6%, Avg loss: 1.832530 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "Training loop: loss: 1.746285  [    0/ 4000]\n",
            "Training Accuracy: 51.1%\n",
            "Testing loop: \n",
            " Accuracy: 49.8%, Avg loss: 1.830588 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "Training loop: loss: 1.754778  [    0/ 4000]\n",
            "Training Accuracy: 49.5%\n",
            "Testing loop: \n",
            " Accuracy: 42.6%, Avg loss: 1.985165 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "Training loop: loss: 1.920053  [    0/ 4000]\n",
            "Training Accuracy: 44.1%\n",
            "Testing loop: \n",
            " Accuracy: 46.8%, Avg loss: 1.858497 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "Training loop: loss: 1.734743  [    0/ 4000]\n",
            "Training Accuracy: 50.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.820201 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "Training loop: loss: 1.716832  [    0/ 4000]\n",
            "Training Accuracy: 50.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.801483 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "Training loop: loss: 1.701523  [    0/ 4000]\n",
            "Training Accuracy: 50.9%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.790598 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "Training loop: loss: 1.716167  [    0/ 4000]\n",
            "Training Accuracy: 51.8%\n",
            "Testing loop: \n",
            " Accuracy: 48.0%, Avg loss: 1.835703 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "Training loop: loss: 1.741114  [    0/ 4000]\n",
            "Training Accuracy: 48.8%\n",
            "Testing loop: \n",
            " Accuracy: 46.2%, Avg loss: 1.847484 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "Training loop: loss: 1.771662  [    0/ 4000]\n",
            "Training Accuracy: 49.4%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.843584 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "Training loop: loss: 1.733144  [    0/ 4000]\n",
            "Training Accuracy: 50.1%\n",
            "Testing loop: \n",
            " Accuracy: 48.6%, Avg loss: 1.837877 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "Training loop: loss: 1.657307  [    0/ 4000]\n",
            "Training Accuracy: 50.0%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.778115 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "Training loop: loss: 1.754765  [    0/ 4000]\n",
            "Training Accuracy: 50.9%\n",
            "Testing loop: \n",
            " Accuracy: 48.4%, Avg loss: 1.826167 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "Training loop: loss: 1.708113  [    0/ 4000]\n",
            "Training Accuracy: 51.3%\n",
            "Testing loop: \n",
            " Accuracy: 50.6%, Avg loss: 1.773201 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "Training loop: loss: 1.658947  [    0/ 4000]\n",
            "Training Accuracy: 49.5%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.801343 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "Training loop: loss: 1.749636  [    0/ 4000]\n",
            "Training Accuracy: 51.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.757706 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "Training loop: loss: 1.676572  [    0/ 4000]\n",
            "Training Accuracy: 51.3%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.779029 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "Training loop: loss: 1.658862  [    0/ 4000]\n",
            "Training Accuracy: 48.8%\n",
            "Testing loop: \n",
            " Accuracy: 47.4%, Avg loss: 1.823787 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "Training loop: loss: 1.715893  [    0/ 4000]\n",
            "Training Accuracy: 49.2%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 1.816905 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "Training loop: loss: 1.744349  [    0/ 4000]\n",
            "Training Accuracy: 50.0%\n",
            "Testing loop: \n",
            " Accuracy: 49.4%, Avg loss: 1.794242 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "Training loop: loss: 1.704202  [    0/ 4000]\n",
            "Training Accuracy: 50.1%\n",
            "Testing loop: \n",
            " Accuracy: 51.6%, Avg loss: 1.756238 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "Training loop: loss: 1.710745  [    0/ 4000]\n",
            "Training Accuracy: 52.3%\n",
            "Testing loop: \n",
            " Accuracy: 50.8%, Avg loss: 1.755836 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "Training loop: loss: 1.614588  [    0/ 4000]\n",
            "Training Accuracy: 49.6%\n",
            "Testing loop: \n",
            " Accuracy: 47.0%, Avg loss: 1.845579 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "Training loop: loss: 1.783609  [    0/ 4000]\n",
            "Training Accuracy: 48.4%\n",
            "Testing loop: \n",
            " Accuracy: 46.6%, Avg loss: 1.869936 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "Training loop: loss: 1.751395  [    0/ 4000]\n",
            "Training Accuracy: 49.4%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.772292 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "Training loop: loss: 1.616313  [    0/ 4000]\n",
            "Training Accuracy: 51.5%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.824808 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "Training loop: loss: 1.621560  [    0/ 4000]\n",
            "Training Accuracy: 51.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.732379 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "Training loop: loss: 1.650329  [    0/ 4000]\n",
            "Training Accuracy: 52.8%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.733555 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "Training loop: loss: 1.605013  [    0/ 4000]\n",
            "Training Accuracy: 53.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.6%, Avg loss: 1.738592 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "Training loop: loss: 1.574061  [    0/ 4000]\n",
            "Training Accuracy: 52.1%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.744018 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "Training loop: loss: 1.603160  [    0/ 4000]\n",
            "Training Accuracy: 51.8%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.719406 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "Training loop: loss: 1.677126  [    0/ 4000]\n",
            "Training Accuracy: 52.3%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.723804 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "Training loop: loss: 1.666148  [    0/ 4000]\n",
            "Training Accuracy: 52.7%\n",
            "Testing loop: \n",
            " Accuracy: 51.6%, Avg loss: 1.715421 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "Training loop: loss: 1.527945  [    0/ 4000]\n",
            "Training Accuracy: 53.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.8%, Avg loss: 1.713990 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "Training loop: loss: 1.562390  [    0/ 4000]\n",
            "Training Accuracy: 52.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.713730 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "Training loop: loss: 1.546793  [    0/ 4000]\n",
            "Training Accuracy: 51.9%\n",
            "Testing loop: \n",
            " Accuracy: 50.0%, Avg loss: 1.750513 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "Training loop: loss: 1.703436  [    0/ 4000]\n",
            "Training Accuracy: 50.7%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.716177 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "Training loop: loss: 1.587039  [    0/ 4000]\n",
            "Training Accuracy: 53.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.693646 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "Training loop: loss: 1.567025  [    0/ 4000]\n",
            "Training Accuracy: 53.4%\n",
            "Testing loop: \n",
            " Accuracy: 48.8%, Avg loss: 1.740859 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "Training loop: loss: 1.693458  [    0/ 4000]\n",
            "Training Accuracy: 53.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.0%, Avg loss: 1.730460 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "Training loop: loss: 1.675993  [    0/ 4000]\n",
            "Training Accuracy: 52.5%\n",
            "Testing loop: \n",
            " Accuracy: 50.2%, Avg loss: 1.748015 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "Training loop: loss: 1.611056  [    0/ 4000]\n",
            "Training Accuracy: 52.5%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.698859 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "Training loop: loss: 1.582296  [    0/ 4000]\n",
            "Training Accuracy: 53.5%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.708167 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "Training loop: loss: 1.603215  [    0/ 4000]\n",
            "Training Accuracy: 53.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.2%, Avg loss: 1.718772 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "Training loop: loss: 1.598037  [    0/ 4000]\n",
            "Training Accuracy: 52.3%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.713449 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "Training loop: loss: 1.604471  [    0/ 4000]\n",
            "Training Accuracy: 53.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.4%, Avg loss: 1.698938 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "Training loop: loss: 1.635437  [    0/ 4000]\n",
            "Training Accuracy: 52.5%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.706003 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "Training loop: loss: 1.489993  [    0/ 4000]\n",
            "Training Accuracy: 54.8%\n",
            "Testing loop: \n",
            " Accuracy: 52.0%, Avg loss: 1.691814 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "Training loop: loss: 1.477196  [    0/ 4000]\n",
            "Training Accuracy: 54.2%\n",
            "Testing loop: \n",
            " Accuracy: 51.6%, Avg loss: 1.692805 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "Training loop: loss: 1.594815  [    0/ 4000]\n",
            "Training Accuracy: 53.8%\n",
            "Testing loop: \n",
            " Accuracy: 49.0%, Avg loss: 1.756618 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "Training loop: loss: 1.613404  [    0/ 4000]\n",
            "Training Accuracy: 51.5%\n",
            "Testing loop: \n",
            " Accuracy: 47.8%, Avg loss: 1.759220 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "Training loop: loss: 1.600196  [    0/ 4000]\n",
            "Training Accuracy: 53.6%\n",
            "Testing loop: \n",
            " Accuracy: 52.4%, Avg loss: 1.700321 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "Training loop: loss: 1.512424  [    0/ 4000]\n",
            "Training Accuracy: 53.3%\n",
            "Testing loop: \n",
            " Accuracy: 51.6%, Avg loss: 1.670824 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "Training loop: loss: 1.567276  [    0/ 4000]\n",
            "Training Accuracy: 55.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.2%, Avg loss: 1.698296 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "Training loop: loss: 1.554245  [    0/ 4000]\n",
            "Training Accuracy: 54.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.658759 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "Training loop: loss: 1.600226  [    0/ 4000]\n",
            "Training Accuracy: 54.4%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.668596 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "Training loop: loss: 1.488075  [    0/ 4000]\n",
            "Training Accuracy: 54.3%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.658845 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "Training loop: loss: 1.465079  [    0/ 4000]\n",
            "Training Accuracy: 54.2%\n",
            "Testing loop: \n",
            " Accuracy: 50.4%, Avg loss: 1.747151 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "Training loop: loss: 1.563075  [    0/ 4000]\n",
            "Training Accuracy: 54.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.693263 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "Training loop: loss: 1.696034  [    0/ 4000]\n",
            "Training Accuracy: 55.4%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.648865 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "Training loop: loss: 1.533231  [    0/ 4000]\n",
            "Training Accuracy: 55.1%\n",
            "Testing loop: \n",
            " Accuracy: 52.2%, Avg loss: 1.677272 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "Training loop: loss: 1.555414  [    0/ 4000]\n",
            "Training Accuracy: 54.8%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.666941 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "Training loop: loss: 1.479877  [    0/ 4000]\n",
            "Training Accuracy: 53.8%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.693681 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "Training loop: loss: 1.569364  [    0/ 4000]\n",
            "Training Accuracy: 52.9%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.683032 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "Training loop: loss: 1.560239  [    0/ 4000]\n",
            "Training Accuracy: 54.5%\n",
            "Testing loop: \n",
            " Accuracy: 51.8%, Avg loss: 1.690146 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "Training loop: loss: 1.479905  [    0/ 4000]\n",
            "Training Accuracy: 54.9%\n",
            "Testing loop: \n",
            " Accuracy: 51.2%, Avg loss: 1.703682 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "Training loop: loss: 1.558361  [    0/ 4000]\n",
            "Training Accuracy: 54.4%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.658934 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "Training loop: loss: 1.624434  [    0/ 4000]\n",
            "Training Accuracy: 55.0%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.679446 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "Training loop: loss: 1.550373  [    0/ 4000]\n",
            "Training Accuracy: 55.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.718301 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "Training loop: loss: 1.497895  [    0/ 4000]\n",
            "Training Accuracy: 55.5%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.676183 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "Training loop: loss: 1.614901  [    0/ 4000]\n",
            "Training Accuracy: 54.6%\n",
            "Testing loop: \n",
            " Accuracy: 49.2%, Avg loss: 1.722853 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "Training loop: loss: 1.530700  [    0/ 4000]\n",
            "Training Accuracy: 53.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.633684 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "Training loop: loss: 1.357767  [    0/ 4000]\n",
            "Training Accuracy: 55.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.628908 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "Training loop: loss: 1.450239  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.640761 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "Training loop: loss: 1.481588  [    0/ 4000]\n",
            "Training Accuracy: 55.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.655868 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "Training loop: loss: 1.431665  [    0/ 4000]\n",
            "Training Accuracy: 54.0%\n",
            "Testing loop: \n",
            " Accuracy: 52.6%, Avg loss: 1.671267 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "Training loop: loss: 1.519071  [    0/ 4000]\n",
            "Training Accuracy: 55.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.636368 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "Training loop: loss: 1.499015  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.621353 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "Training loop: loss: 1.529673  [    0/ 4000]\n",
            "Training Accuracy: 55.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.634000 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "Training loop: loss: 1.578031  [    0/ 4000]\n",
            "Training Accuracy: 55.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.651986 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "Training loop: loss: 1.552411  [    0/ 4000]\n",
            "Training Accuracy: 56.3%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.614963 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "Training loop: loss: 1.515489  [    0/ 4000]\n",
            "Training Accuracy: 56.6%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.624555 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "Training loop: loss: 1.465936  [    0/ 4000]\n",
            "Training Accuracy: 56.2%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.663802 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "Training loop: loss: 1.474541  [    0/ 4000]\n",
            "Training Accuracy: 56.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.615559 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "Training loop: loss: 1.477053  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.602118 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "Training loop: loss: 1.547777  [    0/ 4000]\n",
            "Training Accuracy: 56.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.609831 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "Training loop: loss: 1.373138  [    0/ 4000]\n",
            "Training Accuracy: 56.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.8%, Avg loss: 1.610144 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "Training loop: loss: 1.352097  [    0/ 4000]\n",
            "Training Accuracy: 57.2%\n",
            "Testing loop: \n",
            " Accuracy: 52.2%, Avg loss: 1.679508 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "Training loop: loss: 1.446895  [    0/ 4000]\n",
            "Training Accuracy: 56.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.8%, Avg loss: 1.608771 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "Training loop: loss: 1.450627  [    0/ 4000]\n",
            "Training Accuracy: 57.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.598164 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "Training loop: loss: 1.478612  [    0/ 4000]\n",
            "Training Accuracy: 56.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.618160 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "Training loop: loss: 1.496119  [    0/ 4000]\n",
            "Training Accuracy: 57.0%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.642917 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "Training loop: loss: 1.496973  [    0/ 4000]\n",
            "Training Accuracy: 57.0%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.643559 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "Training loop: loss: 1.434398  [    0/ 4000]\n",
            "Training Accuracy: 57.1%\n",
            "Testing loop: \n",
            " Accuracy: 53.2%, Avg loss: 1.671073 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "Training loop: loss: 1.641140  [    0/ 4000]\n",
            "Training Accuracy: 58.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.596153 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "Training loop: loss: 1.463125  [    0/ 4000]\n",
            "Training Accuracy: 56.2%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.615125 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "Training loop: loss: 1.444439  [    0/ 4000]\n",
            "Training Accuracy: 56.0%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.598949 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "Training loop: loss: 1.444969  [    0/ 4000]\n",
            "Training Accuracy: 58.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.591419 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "Training loop: loss: 1.420702  [    0/ 4000]\n",
            "Training Accuracy: 58.1%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.589078 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "Training loop: loss: 1.328895  [    0/ 4000]\n",
            "Training Accuracy: 57.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.8%, Avg loss: 1.599086 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "Training loop: loss: 1.398289  [    0/ 4000]\n",
            "Training Accuracy: 58.4%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.599827 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "Training loop: loss: 1.487921  [    0/ 4000]\n",
            "Training Accuracy: 58.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.589068 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "Training loop: loss: 1.416380  [    0/ 4000]\n",
            "Training Accuracy: 58.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.590077 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "Training loop: loss: 1.406106  [    0/ 4000]\n",
            "Training Accuracy: 57.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.593854 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "Training loop: loss: 1.313890  [    0/ 4000]\n",
            "Training Accuracy: 58.8%\n",
            "Testing loop: \n",
            " Accuracy: 51.4%, Avg loss: 1.639227 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "Training loop: loss: 1.304089  [    0/ 4000]\n",
            "Training Accuracy: 58.2%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.601732 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "Training loop: loss: 1.450893  [    0/ 4000]\n",
            "Training Accuracy: 58.7%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.621371 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "Training loop: loss: 1.413045  [    0/ 4000]\n",
            "Training Accuracy: 57.2%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.589996 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "Training loop: loss: 1.448987  [    0/ 4000]\n",
            "Training Accuracy: 57.9%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.601988 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "Training loop: loss: 1.481415  [    0/ 4000]\n",
            "Training Accuracy: 56.4%\n",
            "Testing loop: \n",
            " Accuracy: 53.4%, Avg loss: 1.627524 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "Training loop: loss: 1.565965  [    0/ 4000]\n",
            "Training Accuracy: 58.3%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.579457 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "Training loop: loss: 1.358159  [    0/ 4000]\n",
            "Training Accuracy: 59.2%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.570547 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "Training loop: loss: 1.434431  [    0/ 4000]\n",
            "Training Accuracy: 59.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.588180 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "Training loop: loss: 1.405794  [    0/ 4000]\n",
            "Training Accuracy: 58.3%\n",
            "Testing loop: \n",
            " Accuracy: 53.0%, Avg loss: 1.640141 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "Training loop: loss: 1.492519  [    0/ 4000]\n",
            "Training Accuracy: 58.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.6%, Avg loss: 1.594942 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "Training loop: loss: 1.403251  [    0/ 4000]\n",
            "Training Accuracy: 58.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.574451 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "Training loop: loss: 1.358520  [    0/ 4000]\n",
            "Training Accuracy: 59.1%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.568593 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "Training loop: loss: 1.429522  [    0/ 4000]\n",
            "Training Accuracy: 58.7%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.575937 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "Training loop: loss: 1.478392  [    0/ 4000]\n",
            "Training Accuracy: 59.0%\n",
            "Testing loop: \n",
            " Accuracy: 56.6%, Avg loss: 1.573705 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "Training loop: loss: 1.425213  [    0/ 4000]\n",
            "Training Accuracy: 59.5%\n",
            "Testing loop: \n",
            " Accuracy: 56.0%, Avg loss: 1.563345 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "Training loop: loss: 1.381908  [    0/ 4000]\n",
            "Training Accuracy: 59.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.8%, Avg loss: 1.576604 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "Training loop: loss: 1.482112  [    0/ 4000]\n",
            "Training Accuracy: 58.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.571532 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "Training loop: loss: 1.469790  [    0/ 4000]\n",
            "Training Accuracy: 58.8%\n",
            "Testing loop: \n",
            " Accuracy: 55.8%, Avg loss: 1.563573 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "Training loop: loss: 1.347783  [    0/ 4000]\n",
            "Training Accuracy: 59.8%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.577576 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "Training loop: loss: 1.380466  [    0/ 4000]\n",
            "Training Accuracy: 59.4%\n",
            "Testing loop: \n",
            " Accuracy: 56.4%, Avg loss: 1.558797 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "Training loop: loss: 1.289637  [    0/ 4000]\n",
            "Training Accuracy: 58.6%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.568144 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "Training loop: loss: 1.348172  [    0/ 4000]\n",
            "Training Accuracy: 58.9%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.584856 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "Training loop: loss: 1.397363  [    0/ 4000]\n",
            "Training Accuracy: 59.6%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.598768 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "Training loop: loss: 1.443105  [    0/ 4000]\n",
            "Training Accuracy: 58.9%\n",
            "Testing loop: \n",
            " Accuracy: 56.2%, Avg loss: 1.561841 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "Training loop: loss: 1.357258  [    0/ 4000]\n",
            "Training Accuracy: 59.9%\n",
            "Testing loop: \n",
            " Accuracy: 55.4%, Avg loss: 1.556355 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "Training loop: loss: 1.365069  [    0/ 4000]\n",
            "Training Accuracy: 59.6%\n",
            "Testing loop: \n",
            " Accuracy: 53.8%, Avg loss: 1.592853 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "Training loop: loss: 1.397473  [    0/ 4000]\n",
            "Training Accuracy: 58.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.4%, Avg loss: 1.568746 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "Training loop: loss: 1.427521  [    0/ 4000]\n",
            "Training Accuracy: 58.5%\n",
            "Testing loop: \n",
            " Accuracy: 54.2%, Avg loss: 1.593778 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "Training loop: loss: 1.370384  [    0/ 4000]\n",
            "Training Accuracy: 60.1%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.565877 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "Training loop: loss: 1.543420  [    0/ 4000]\n",
            "Training Accuracy: 59.9%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.573039 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "Training loop: loss: 1.388012  [    0/ 4000]\n",
            "Training Accuracy: 59.7%\n",
            "Testing loop: \n",
            " Accuracy: 54.8%, Avg loss: 1.572542 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "Training loop: loss: 1.325389  [    0/ 4000]\n",
            "Training Accuracy: 60.5%\n",
            "Testing loop: \n",
            " Accuracy: 55.0%, Avg loss: 1.560370 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "Training loop: loss: 1.473630  [    0/ 4000]\n",
            "Training Accuracy: 60.6%\n",
            "Testing loop: \n",
            " Accuracy: 54.0%, Avg loss: 1.570606 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "Training loop: loss: 1.352327  [    0/ 4000]\n",
            "Training Accuracy: 60.9%\n",
            "Testing loop: \n",
            " Accuracy: 53.6%, Avg loss: 1.556557 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "Training loop: loss: 1.324443  [    0/ 4000]\n",
            "Training Accuracy: 60.0%\n",
            "Testing loop: \n",
            " Accuracy: 55.2%, Avg loss: 1.555883 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "Training loop: loss: 1.317042  [    0/ 4000]\n",
            "Training Accuracy: 59.3%\n",
            "Testing loop: \n",
            " Accuracy: 55.6%, Avg loss: 1.552551 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe7cf843210>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 500\n",
        "epochs = 300\n",
        "momentum = 0\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "model = TinyQuickDrawStudentClassifier().to(device)\n",
        "model.requires_grad_(True)\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), \n",
        "                        lr = learning_rate, \n",
        "                        momentum = momentum, \n",
        "                        weight_decay = weight_decay, \n",
        "                        dampening= dampening)\n",
        "# Loss Func\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Dataloaders\n",
        "# Created a bigger training dataset for better model training\n",
        "train_data_dir = \"tiny_quick_draw/bigger_train\"\n",
        "train_data = ImageFolder(train_data_dir, transform=Compose([ToTensor()]))\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "example_train_accs = []\n",
        "example_test_accs = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    training_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    testing_acc, _ = test_loop(test_dataloader, model, loss_fn)\n",
        "    example_train_accs.append(training_acc)\n",
        "    example_test_accs.append(testing_acc)\n",
        "print(\"Done!\")\n",
        "\n",
        "epoch_list = [i for i in range(epochs)]\n",
        "plt.plot(epoch_list, example_train_accs, color ='tab:green', label='training accuracy')\n",
        "plt.plot(epoch_list, example_test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.legend()"
      ],
      "id": "2a1521d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714562c6"
      },
      "source": [
        "# Fine-Tuning Hyperparameters"
      ],
      "id": "714562c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f385fec6"
      },
      "source": [
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get some practice with this."
      ],
      "id": "f385fec6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02d3656b"
      },
      "source": [
        "## Exercise 2.1.3"
      ],
      "id": "02d3656b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924fc4ff"
      },
      "source": [
        "Below, you should experiment with different values of the various hyperparameters, including layer size, batch size, learning rate, and number of training epochs. You should also experiment with SGD optimization hyperparameters including momentum, weight decay and more. Plot at least 3 graphs to illustrate how does the change of one hyperparameter (choose any except for epochs) affect the training and testing accuracy and testing loss. To show these plots, you can add more code blocks in this section. We provide a boilerplate code for one (epochs) to give an idea of what we are looking for."
      ],
      "id": "924fc4ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1617
        },
        "id": "3e50ab30",
        "outputId": "5b089528-7fc4-45f2-bca9-9d69e722fad0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently running epoch 1\n",
            "Currently running epoch 1\n",
            "Currently running epoch 2\n",
            "Currently running epoch 3\n",
            "Currently running epoch 4\n",
            "Currently running epoch 5\n",
            "Currently running epoch 1\n",
            "Currently running epoch 2\n",
            "Currently running epoch 3\n",
            "Currently running epoch 4\n",
            "Currently running epoch 5\n",
            "Currently running epoch 6\n",
            "Currently running epoch 7\n",
            "Currently running epoch 8\n",
            "Currently running epoch 9\n",
            "Currently running epoch 10\n",
            "Currently running epoch 1\n",
            "Currently running epoch 2\n",
            "Currently running epoch 3\n",
            "Currently running epoch 4\n",
            "Currently running epoch 5\n",
            "Currently running epoch 6\n",
            "Currently running epoch 7\n",
            "Currently running epoch 8\n",
            "Currently running epoch 9\n",
            "Currently running epoch 10\n",
            "Currently running epoch 11\n",
            "Currently running epoch 12\n",
            "Currently running epoch 13\n",
            "Currently running epoch 14\n",
            "Currently running epoch 15\n",
            "Currently running epoch 1\n",
            "Currently running epoch 2\n",
            "Currently running epoch 3\n",
            "Currently running epoch 4\n",
            "Currently running epoch 5\n",
            "Currently running epoch 6\n",
            "Currently running epoch 7\n",
            "Currently running epoch 8\n",
            "Currently running epoch 9\n",
            "Currently running epoch 10\n",
            "Currently running epoch 11\n",
            "Currently running epoch 12\n",
            "Currently running epoch 13\n",
            "Currently running epoch 14\n",
            "Currently running epoch 15\n",
            "Currently running epoch 16\n",
            "Currently running epoch 17\n",
            "Currently running epoch 18\n",
            "Currently running epoch 19\n",
            "Currently running epoch 20\n",
            "Currently running epoch 1\n",
            "Currently running epoch 2\n",
            "Currently running epoch 3\n",
            "Currently running epoch 4\n",
            "Currently running epoch 5\n",
            "Currently running epoch 6\n",
            "Currently running epoch 7\n",
            "Currently running epoch 8\n",
            "Currently running epoch 9\n",
            "Currently running epoch 10\n",
            "Currently running epoch 11\n",
            "Currently running epoch 12\n",
            "Currently running epoch 13\n",
            "Currently running epoch 14\n",
            "Currently running epoch 15\n",
            "Currently running epoch 16\n",
            "Currently running epoch 17\n",
            "Currently running epoch 18\n",
            "Currently running epoch 19\n",
            "Currently running epoch 20\n",
            "Currently running epoch 21\n",
            "Currently running epoch 22\n",
            "Currently running epoch 23\n",
            "Currently running epoch 24\n",
            "Currently running epoch 25\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in epochs: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = learning_rate, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(e):\n",
        "        print(f\"Currently running epoch {t+1}\")\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(epochs,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(epochs,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(epochs,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "3e50ab30"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including weight decay and momentum in variable epochs"
      ],
      "metadata": {
        "id": "iKpd1YWy5mfS"
      },
      "id": "iKpd1YWy5mfS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "gOkSLo-Fy1Ot",
        "outputId": "c6a8a465-25eb-42cc-b6e8-ea4b753ef76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in epochs: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = learning_rate, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(e):\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(epochs,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(epochs,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(epochs,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "gOkSLo-Fy1Ot"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Varying the momentum and decay for the same epoch changes we see that the improvement happens earlier."
      ],
      "metadata": {
        "id": "dnnON_gnI-yK"
      },
      "id": "dnnON_gnI-yK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpR9YjTU-_h"
      },
      "source": [
        "### Varying batch Sizes and visualizing how increasing batch sizes makes changes in training, testing, validation accuracies."
      ],
      "id": "iYpR9YjTU-_h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "b0433fdb",
        "outputId": "630bc109-b0d7-41a0-e5bb-5ded2371dff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running for Batch set as 2\n",
            "Currently Running for Batch set as 8\n",
            "Currently Running for Batch set as 16\n",
            "Currently Running for Batch set as 32\n",
            "Currently Running for Batch set as 64\n",
            "Currently Running for Batch set as 100\n",
            "Currently Running for Batch set as 500\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = [2, 8 , 16, 32, 64, 100, 500]\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in batch_size: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    print(f\"Currently Running for Batch set as {e}\")\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = learning_rate, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=e)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=e)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(100):\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(batch_size,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(batch_size,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(batch_size,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "b0433fdb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Batch size is less we are able to get a good accuracy on the training set because it learns like identity function, and when we take the whole of 500 at at time the accuracy reduces."
      ],
      "metadata": {
        "id": "1h6MYVuQJLFq"
      },
      "id": "1h6MYVuQJLFq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXYObaj8yaoU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2282
        },
        "outputId": "fd8a3710-3754-486b-bdf7-f9fc27762aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running for Batch set as 2\n",
            "---Currently running epoch 1\n",
            "---Currently running epoch 2\n",
            "---Currently running epoch 3\n",
            "---Currently running epoch 4\n",
            "---Currently running epoch 5\n",
            "---Currently running epoch 6\n",
            "---Currently running epoch 7\n",
            "---Currently running epoch 8\n",
            "---Currently running epoch 9\n",
            "---Currently running epoch 10\n",
            "---Currently running epoch 11\n",
            "---Currently running epoch 12\n",
            "---Currently running epoch 13\n",
            "---Currently running epoch 14\n",
            "---Currently running epoch 15\n",
            "---Currently running epoch 16\n",
            "---Currently running epoch 17\n",
            "---Currently running epoch 18\n",
            "---Currently running epoch 19\n",
            "---Currently running epoch 20\n",
            "---Currently running epoch 21\n",
            "---Currently running epoch 22\n",
            "---Currently running epoch 23\n",
            "---Currently running epoch 24\n",
            "---Currently running epoch 25\n",
            "---Currently running epoch 26\n",
            "---Currently running epoch 27\n",
            "---Currently running epoch 28\n",
            "---Currently running epoch 29\n",
            "---Currently running epoch 30\n",
            "---Currently running epoch 31\n",
            "---Currently running epoch 32\n",
            "---Currently running epoch 33\n",
            "---Currently running epoch 34\n",
            "---Currently running epoch 35\n",
            "---Currently running epoch 36\n",
            "---Currently running epoch 37\n",
            "---Currently running epoch 38\n",
            "---Currently running epoch 39\n",
            "---Currently running epoch 40\n",
            "---Currently running epoch 41\n",
            "---Currently running epoch 42\n",
            "---Currently running epoch 43\n",
            "---Currently running epoch 44\n",
            "---Currently running epoch 45\n",
            "---Currently running epoch 46\n",
            "---Currently running epoch 47\n",
            "---Currently running epoch 48\n",
            "---Currently running epoch 49\n",
            "---Currently running epoch 50\n",
            "---Currently running epoch 51\n",
            "---Currently running epoch 52\n",
            "---Currently running epoch 53\n",
            "---Currently running epoch 54\n",
            "---Currently running epoch 55\n",
            "---Currently running epoch 56\n",
            "---Currently running epoch 57\n",
            "---Currently running epoch 58\n",
            "---Currently running epoch 59\n",
            "---Currently running epoch 60\n",
            "---Currently running epoch 61\n",
            "---Currently running epoch 62\n",
            "---Currently running epoch 63\n",
            "---Currently running epoch 64\n",
            "---Currently running epoch 65\n",
            "---Currently running epoch 66\n",
            "---Currently running epoch 67\n",
            "---Currently running epoch 68\n",
            "---Currently running epoch 69\n",
            "---Currently running epoch 70\n",
            "---Currently running epoch 71\n",
            "---Currently running epoch 72\n",
            "---Currently running epoch 73\n",
            "---Currently running epoch 74\n",
            "---Currently running epoch 75\n",
            "---Currently running epoch 76\n",
            "---Currently running epoch 77\n",
            "---Currently running epoch 78\n",
            "---Currently running epoch 79\n",
            "---Currently running epoch 80\n",
            "---Currently running epoch 81\n",
            "---Currently running epoch 82\n",
            "---Currently running epoch 83\n",
            "---Currently running epoch 84\n",
            "---Currently running epoch 85\n",
            "---Currently running epoch 86\n",
            "---Currently running epoch 87\n",
            "---Currently running epoch 88\n",
            "---Currently running epoch 89\n",
            "---Currently running epoch 90\n",
            "---Currently running epoch 91\n",
            "---Currently running epoch 92\n",
            "---Currently running epoch 93\n",
            "---Currently running epoch 94\n",
            "---Currently running epoch 95\n",
            "---Currently running epoch 96\n",
            "---Currently running epoch 97\n",
            "---Currently running epoch 98\n",
            "---Currently running epoch 99\n",
            "---Currently running epoch 100\n",
            "Currently Running for Batch set as 8\n",
            "---Currently running epoch 1\n",
            "---Currently running epoch 2\n",
            "---Currently running epoch 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e5df90cc0fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"---Currently running epoch {t+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtraining_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtesting_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfinal_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b1c896431489>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, print_log)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprint_log\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mhas_sparse_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_sparse_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 foreach=group['foreach'])\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    204\u001b[0m          \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnesterov\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m          \u001b[0mhas_sparse_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_sparse_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m          maximize=maximize)\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m def _single_tensor_sgd(params: List[Tensor],\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = [16, 32, 64, 100, 500]\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in batch_size: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    print(f\"Currently Running for Batch set as {e}\")\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = learning_rate, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=e)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=e)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(50):\n",
        "        print(f\"---Currently running epoch {t+1}\")\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(batch_size,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(batch_size,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(batch_size,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "fXYObaj8yaoU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Batch Size and momentum and weight decay"
      ],
      "metadata": {
        "id": "klznfNLr5Qxt"
      },
      "id": "klznfNLr5Qxt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Learning Rate"
      ],
      "metadata": {
        "id": "4BehpyT05OK6"
      },
      "id": "4BehpyT05OK6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAf7Nn874kaQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "a47aac2b-3b69-4585-ff69-81fa30fad097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running for Learning rate set as 0.1\n",
            "Currently Running for Learning rate set as 0.01\n",
            "Currently Running for Learning rate set as 0.001\n",
            "Currently Running for Learning rate set as 0.5\n",
            "Currently Running for Learning rate set as 1\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = [0.1, 0.01, 0.001, 0.5, 1]\n",
        "batch_size = 50\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in learning_rate: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    print(f\"Currently Running for Learning rate set as {e}\")\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = e, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(50):\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(learning_rate,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(learning_rate,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(learning_rate,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "IAf7Nn874kaQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Learning Rate with weight decay"
      ],
      "metadata": {
        "id": "S3ybKTx45Ff_"
      },
      "id": "S3ybKTx45Ff_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvIQL49IR5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f6a4c064-b5b1-4df0-e0c1-5e4f543b9200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running for Learning rate set as 0.001\n",
            "Currently Running for Learning rate set as 0.01\n",
            "Currently Running for Learning rate set as 0.1\n",
            "Currently Running for Learning rate set as 0.5\n",
            "Currently Running for Learning rate set as 1\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = [0.001, 0.01, 0.1, 0.5, 1]\n",
        "batch_size = 50\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in learning_rate: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    print(f\"Currently Running for Learning rate set as {e}\")\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = e, \n",
        "                            momentum = momentum, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(50):\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(learning_rate,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(learning_rate,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(learning_rate,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "id": "JvIQL49IR5de"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the algorithm gets stuck after particular learning rate."
      ],
      "metadata": {
        "id": "QHmzkYX9Q8VY"
      },
      "id": "QHmzkYX9Q8VY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable momentum"
      ],
      "metadata": {
        "id": "0g99EdGW5155"
      },
      "id": "0g99EdGW5155"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 50\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = [0.03,0.1, 0.3, 0.5, 0.8, 0.9,1]\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in momentum: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    print(f\"Currently Running for momentum set as {e}\")\n",
        "    model = TinyQuickDrawStudentClassifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr = learning_rate, \n",
        "                            momentum = e, \n",
        "                            weight_decay = weight_decay, \n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(50):\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss) \n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(momentum,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(momentum,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(momentum,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "s6wZBwiH5zos",
        "outputId": "e1d31d33-1078-44ac-c553-bc596b8d53c8"
      },
      "id": "s6wZBwiH5zos",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running for momentum set as 0.03\n",
            "Currently Running for momentum set as 0.1\n",
            "Currently Running for momentum set as 0.3\n",
            "Currently Running for momentum set as 0.5\n",
            "Currently Running for momentum set as 0.8\n",
            "Currently Running for momentum set as 0.9\n",
            "Currently Running for momentum set as 1\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum works well when it is 0.5 else we see it reduces the accuracy on both side of the 0.5 ."
      ],
      "metadata": {
        "id": "bL7OZ_W-RHhy"
      },
      "id": "bL7OZ_W-RHhy"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2c687004",
        "2PTZyL_0Nfnu",
        "szlFdoNCP7za",
        "PZ3RNcOCRdXO",
        "vcIf-G9LOlND",
        "iGg1qIq4Nm9A",
        "PoPRJaFPNk0B"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca18b354f0974e3f9455744e0007f07a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1785f8f3f8444dbda75b7007891ad113",
              "IPY_MODEL_09842abe94dd4b208ad3770950e5170f",
              "IPY_MODEL_5b29eff153d841f3b3814bfe410cffe9"
            ],
            "layout": "IPY_MODEL_826f6e617627401faeae7001417d00a0"
          }
        },
        "1785f8f3f8444dbda75b7007891ad113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_087374c2f407486d8cb62ec6fd4531a6",
            "placeholder": "​",
            "style": "IPY_MODEL_3901cd5a42dd4a86a5372dff14095380",
            "value": "100%"
          }
        },
        "09842abe94dd4b208ad3770950e5170f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a6c02212854b9bbef2feb58ba6f269",
            "max": 2965111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cf0dc3a20cc4f12b89e00f13d5ed55e",
            "value": 2965111
          }
        },
        "5b29eff153d841f3b3814bfe410cffe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7668ad40d6924e4aa43b269f1fc35b1e",
            "placeholder": "​",
            "style": "IPY_MODEL_314b4052325344d1af87888f5eb37e98",
            "value": " 2965111/2965111 [00:01&lt;00:00, 2219750.62it/s]"
          }
        },
        "826f6e617627401faeae7001417d00a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "087374c2f407486d8cb62ec6fd4531a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3901cd5a42dd4a86a5372dff14095380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a6c02212854b9bbef2feb58ba6f269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf0dc3a20cc4f12b89e00f13d5ed55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7668ad40d6924e4aa43b269f1fc35b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314b4052325344d1af87888f5eb37e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}